{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Run DRN\n",
    "- PIT Histogram for 24h, 72h, 120h\n",
    "- Rank Histogram of raw data"
   ],
   "id": "2a41077302fdadb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:42:39.857755Z",
     "start_time": "2025-05-08T03:42:35.943483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load data first\n",
    "%cd /home/ltchen/gnnpp\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from models.drn import DRN\n",
    "from models.model_utils import EmbedStations\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from scipy.stats import norm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.data import load_dataframes, summary_statistics\n",
    "from utils.drn_utils import *\n",
    "from models.loss import NormalCRPS, crps_no_avg\n"
   ],
   "id": "259579d6231826a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# All stations",
   "id": "3c58f481ce1383d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:42:39.870711Z",
     "start_time": "2025-05-08T03:42:39.865751Z"
    }
   },
   "cell_type": "code",
   "source": "DIRECTORY = os.getcwd()\n",
   "id": "7741420991c4a5c3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DRNs (MSE & CRPS) for all stations",
   "id": "a1bf4765e3a030b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MSE",
   "id": "37faab48eb142e67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T22:57:57.189119Z",
     "start_time": "2025-05-03T22:57:57.169808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# leave out\n",
    "# nn mse loss with lightning\n",
    "# Deterministic NN with one hidden layer using MSE as loss and embeddings\n",
    "\n",
    "class MyDRN(L.LightningModule):\n",
    "    def __init__(self, hidden_size, embedding_dim, in_feat, optimizer_class, optimizer_params):\n",
    "        super(MyDRN, self).__init__()\n",
    "        self.embedding = EmbedStations(num_stations_max=120, embedding_dim=embedding_dim)\n",
    "        self.linear = torch.nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear_t2m = torch.nn.Linear(in_features=hidden_size, out_features=1)  # output t2m value\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.shape) # (8, 65)\n",
    "        x = self.embedding(x)\n",
    "        #print(f\"After embedding: {x.shape}\") # (8, 84)\n",
    "        x = self.linear(x)\n",
    "        #print(x.shape) # (8, 64)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.linear_t2m(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())  # why y.flatten()?\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        self.log(\"validation_loss\", loss.item(), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):  # unterschied zwischen predict und test_step?\n",
    "        x, y = batch  # wieso hat test_step auch y?\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        self.log(\"test_loss\", loss.item(), on_epoch=True, prog_bar=True)\n",
    "        return loss\n"
   ],
   "id": "107cd80da8b042da",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CRPS",
   "id": "1cd7820c845949fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:42:44.269838Z",
     "start_time": "2025-05-08T03:42:44.244962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nn crps loss with lightning\n",
    "class CRPSDRN(L.LightningModule):\n",
    "    def __init__(self, embedding_dim, in_channels, hidden_channels, optimizer_class, optimizer_params):\n",
    "        super(CRPSDRN, self).__init__()\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = len(hidden_channels)\n",
    "\n",
    "        self.embedding = EmbedStations(num_stations_max=120, embedding_dim=embedding_dim)\n",
    "        self.linear = nn.ModuleList()\n",
    "        for hidden_size in self.hidden_channels:\n",
    "            self.linear.append(nn.Linear(in_features=in_channels, out_features=hidden_size))\n",
    "            in_channels = hidden_size\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.last_linear_mu = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.last_linear_sigma = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        self.loss_fn = NormalCRPS()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embedding(x)  #(8, 65)\n",
    "        for layer in self.linear:\n",
    "            x = layer(x)\n",
    "            # print(x.shape)\n",
    "            x = self.relu(x)\n",
    "            # print(x.shape)\n",
    "        # mu = self.last_linear_mu(x)\n",
    "        # x = self.relu(x)  #(8, 64)\n",
    "        mu = self.last_linear_mu(x)\n",
    "        sigma = self.softplus(self.last_linear_sigma(x))\n",
    "        res = torch.cat([mu, sigma], dim=1)\n",
    "        return res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        self.log(\"validation_loss\", loss.item(), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):  # unterschied zwischen predict und test_step?\n",
    "        x, y = batch  # wieso hat test_step auch y?\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        self.log(\"test_loss\", loss.item(), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat\n"
   ],
   "id": "925031646b5d3ffe",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 24h R2F",
   "id": "80f71d5b69b39b55"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dataframes for train, valid, test",
   "id": "d2df53cbcd0652e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:42:53.814241Z",
     "start_time": "2025-05-08T03:42:48.275496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataloader and preprocessing\n",
    "MODEL = \"24h\"\n",
    "dataframes = load_dataframes(leadtime=MODEL)  # train mode => for training nn? Wie wird das im Paper beschrieben?\n",
    "dataframes = summary_statistics(\n",
    "    dataframes)  # wie sehen die daten von summary statistics aus? => wenn das nur die Daten von einer Station sind, dann über Zeitpunkte\n",
    "dataframes.pop(\"stations\")\n",
    "\n",
    "# test\n",
    "for X, y in dataframes.values():  # wofuer?\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train, valid_test = normalize_features(\n",
    "    training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    ")"
   ],
   "id": "e6d863460e3de5e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for valid\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "[INFO] Normalizing features...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:42:53.988827Z",
     "start_time": "2025-05-08T03:42:53.820430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = drop_nans(train)\n",
    "(test_rf, test_f) = valid_test\n",
    "test_rf = drop_nans(test_rf)\n",
    "test_f = drop_nans(test_f)\n",
    "\n",
    "leadtime = MODEL\n",
    "# SAVEPATH = os.path.join(PATH, f\"drn_{MODEL}/models\")\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/drn_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/drn_{leadtime}\")"
   ],
   "id": "c17df2492291cf05",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T00:28:02.014216Z",
     "start_time": "2025-05-04T00:28:02.005904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(dataframes['test_rf'][0].station_id.nunique())\n",
    "print(test_rf[0].station_id.nunique())\n",
    "print(test_rf[0].shape)\n",
    "print(test_rf[1].shape)"
   ],
   "id": "5290498c0f612ec6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "109\n",
      "(76903, 65)\n",
      "(76903, 3)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T00:10:44.629715Z",
     "start_time": "2025-05-04T00:10:44.619640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(dataframes['train'][0].station_id.nunique())\n",
    "print(train[0].station_id.nunique())"
   ],
   "id": "188dbf0536cb5ef4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T14:40:18.042661Z",
     "start_time": "2025-03-05T14:40:18.034485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check for nans - leave out\n",
    "nans = train[1][\"t2m\"].isna().reset_index(drop=True)\n",
    "print(train[0].index)\n",
    "print(train[1].index)\n",
    "#nans.index\n",
    "#train[0][~nans]\n",
    "#res = (train[0][~nans], train[1][~nans])\n",
    "#res"
   ],
   "id": "bc2332f570572189",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=4627216, step=1)\n",
      "RangeIndex(start=0, stop=420656, step=1)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T15:17:26.271842Z",
     "start_time": "2025-03-03T15:17:26.265961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# difference between valid and test - one station only - leave out\n",
    "# train\n",
    "for i in train:\n",
    "    print(i.shape)\n",
    "    print(i.columns)\n",
    "\n",
    "# valid_test\n",
    "for (i, j) in valid_test:\n",
    "    print(i.shape)\n",
    "    print(i.columns)"
   ],
   "id": "6c927fe1e7b6ae17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398866, 65)\n",
      "Index(['station_id', 'model_orography', 'station_altitude', 'station_latitude',\n",
      "       'station_longitude', 'cape_mean', 'cape_std', 'sd_mean', 'sd_std',\n",
      "       'stl1_mean', 'stl1_std', 'swvl1_mean', 'swvl1_std', 't2m_mean',\n",
      "       't2m_std', 'tcc_mean', 'tcc_std', 'tcw_mean', 'tcw_std', 'tcwv_mean',\n",
      "       'tcwv_std', 'u10_mean', 'u10_std', 'u100_mean', 'u100_std', 'v10_mean',\n",
      "       'v10_std', 'v100_mean', 'v100_std', 'vis_mean', 'vis_std', 'cp6_mean',\n",
      "       'cp6_std', 'mn2t6_mean', 'mn2t6_std', 'mx2t6_mean', 'mx2t6_std',\n",
      "       'p10fg6_mean', 'p10fg6_std', 'slhf6_mean', 'slhf6_std', 'sshf6_mean',\n",
      "       'sshf6_std', 'ssr6_mean', 'ssr6_std', 'ssrd6_mean', 'ssrd6_std',\n",
      "       'str6_mean', 'str6_std', 'strd6_mean', 'strd6_std', 'tp6_mean',\n",
      "       'tp6_std', 'z_mean', 'z_std', 'q_mean', 'q_std', 'u_mean', 'u_std',\n",
      "       'v_mean', 'v_std', 't_mean', 't_std', 'cos_doy', 'sin_doy'],\n",
      "      dtype='object')\n",
      "(398866, 3)\n",
      "Index(['time', 'station_id', 't2m'], dtype='object')\n",
      "(89304, 65)\n",
      "Index(['station_id', 'model_orography', 'station_altitude', 'station_latitude',\n",
      "       'station_longitude', 'cape_mean', 'cape_std', 'sd_mean', 'sd_std',\n",
      "       'stl1_mean', 'stl1_std', 'swvl1_mean', 'swvl1_std', 't2m_mean',\n",
      "       't2m_std', 'tcc_mean', 'tcc_std', 'tcw_mean', 'tcw_std', 'tcwv_mean',\n",
      "       'tcwv_std', 'u10_mean', 'u10_std', 'u100_mean', 'u100_std', 'v10_mean',\n",
      "       'v10_std', 'v100_mean', 'v100_std', 'vis_mean', 'vis_std', 'cp6_mean',\n",
      "       'cp6_std', 'mn2t6_mean', 'mn2t6_std', 'mx2t6_mean', 'mx2t6_std',\n",
      "       'p10fg6_mean', 'p10fg6_std', 'slhf6_mean', 'slhf6_std', 'sshf6_mean',\n",
      "       'sshf6_std', 'ssr6_mean', 'ssr6_std', 'ssrd6_mean', 'ssrd6_std',\n",
      "       'str6_mean', 'str6_std', 'strd6_mean', 'strd6_std', 'tp6_mean',\n",
      "       'tp6_std', 'z_mean', 'z_std', 'q_mean', 'q_std', 'u_mean', 'u_std',\n",
      "       'v_mean', 'v_std', 't_mean', 't_std', 'cos_doy', 'sin_doy'],\n",
      "      dtype='object')\n",
      "(89060, 65)\n",
      "Index(['station_id', 'model_orography', 'station_altitude', 'station_latitude',\n",
      "       'station_longitude', 'cape_mean', 'cape_std', 'sd_mean', 'sd_std',\n",
      "       'stl1_mean', 'stl1_std', 'swvl1_mean', 'swvl1_std', 't2m_mean',\n",
      "       't2m_std', 'tcc_mean', 'tcc_std', 'tcw_mean', 'tcw_std', 'tcwv_mean',\n",
      "       'tcwv_std', 'u10_mean', 'u10_std', 'u100_mean', 'u100_std', 'v10_mean',\n",
      "       'v10_std', 'v100_mean', 'v100_std', 'vis_mean', 'vis_std', 'cp6_mean',\n",
      "       'cp6_std', 'mn2t6_mean', 'mn2t6_std', 'mx2t6_mean', 'mx2t6_std',\n",
      "       'p10fg6_mean', 'p10fg6_std', 'slhf6_mean', 'slhf6_std', 'sshf6_mean',\n",
      "       'sshf6_std', 'ssr6_mean', 'ssr6_std', 'ssrd6_mean', 'ssrd6_std',\n",
      "       'str6_mean', 'str6_std', 'strd6_mean', 'strd6_std', 'tp6_mean',\n",
      "       'tp6_std', 'z_mean', 'z_std', 'q_mean', 'q_std', 'u_mean', 'u_std',\n",
      "       'v_mean', 'v_std', 't_mean', 't_std', 'cos_doy', 'sin_doy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:43:58.163901Z",
     "start_time": "2025-05-08T03:43:58.157928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import json file to config\n",
    "CONFIG_FOLDER = os.path.join(DIRECTORY, f\"trained_models/drn_{MODEL}\")\n",
    "JSONPATH = os.path.join(CONFIG_FOLDER, \"params.json\")\n",
    "\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "print(args_dict)\n",
    "config = args_dict\n",
    "batch_size = config['batch_size']\n",
    "hidden_channels = config['hidden_channels']\n",
    "lr = config['lr']\n",
    "max_epochs = config['max_epochs']\n",
    "embed_dim = 20\n",
    "in_channels = train[0].shape[1] + embed_dim - 1"
   ],
   "id": "5cb2ba4038d3881a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/drn_24h/params.json\n",
      "{'leadtime': '24h', 'batch_size': 4096, 'lr': 0.01, 'hidden_channels': [256], 'max_epochs': 26, 'only_summary': 'True'}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train DRN",
   "id": "da78fb47506148bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:09:51.459100Z",
     "start_time": "2025-05-04T23:04:26.538631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(6, 10):\n",
    "    TRAINNAME = f\"drn_{leadtime}_train_run{i}\"\n",
    "    with wandb.init(\n",
    "            project=\"drn_reproduction\",\n",
    "            id=TRAINNAME,\n",
    "            config=config,\n",
    "            tags=[\"final\"],\n",
    "            resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "        y_scaler = StandardScaler(with_std=False)  # wieso scalen wir überhaupt? => robuster?\n",
    "        y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "        #batch_size =2048\n",
    "        #hidden_size=128\n",
    "        #lr=0.0002\n",
    "        #max_epochs=31\n",
    "\n",
    "        embed_dim = 20\n",
    "        in_channels = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "        train_dataset = TensorDataset(torch.Tensor(train[0].to_numpy()),\n",
    "                                      torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]])))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        mydrn = CRPSDRN(\n",
    "            embedding_dim=embed_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=lr),\n",
    "        )\n",
    "\n",
    "        wandb_logger = WandbLogger(project=\"all_station_crps\")\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=10,\n",
    "            accelerator=\"gpu\",\n",
    "            enable_progress_bar=True,\n",
    "            enable_model_summary=True,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=checkpoint_callback,\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=mydrn, train_dataloaders=train_loader)\n",
    "\n",
    "        final_loss = trainer.logged_metrics[\"train_loss_step\"]\n",
    "        print(\"Final MSE Loss:\", final_loss)"
   ],
   "id": "2f67ca662e764b80",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_010426-drn_120h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run6' target=\"_blank\">drn_120h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/drn_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 43.5 K | train\n",
      "2 | relu              | ReLU          | 0      | train\n",
      "3 | softplus          | Softplus      | 0      | train\n",
      "4 | last_linear_mu    | Linear        | 513    | train\n",
      "5 | last_linear_sigma | Linear        | 513    | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "46.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.9 K    Total params\n",
      "0.188     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:03<00:00,  9.77it/s, v_num=run6, train_loss_step=1.100, train_loss_epoch=1.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:03<00:00,  9.74it/s, v_num=run6, train_loss_step=1.100, train_loss_epoch=1.100]\n",
      "Final MSE Loss: tensor(1.1022)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>1.09984</td></tr><tr><td>train_loss_step</td><td>1.10217</td></tr><tr><td>trainer/global_step</td><td>759</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drn_120h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010426-drn_120h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_010547-drn_120h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run7' target=\"_blank\">drn_120h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/drn_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 43.5 K | train\n",
      "2 | relu              | ReLU          | 0      | train\n",
      "3 | softplus          | Softplus      | 0      | train\n",
      "4 | last_linear_mu    | Linear        | 513    | train\n",
      "5 | last_linear_sigma | Linear        | 513    | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "46.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.9 K    Total params\n",
      "0.188     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:04<00:00,  9.34it/s, v_num=run7, train_loss_step=1.110, train_loss_epoch=1.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:04<00:00,  9.31it/s, v_num=run7, train_loss_step=1.110, train_loss_epoch=1.100]\n",
      "Final MSE Loss: tensor(1.1092)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>1.10024</td></tr><tr><td>train_loss_step</td><td>1.10925</td></tr><tr><td>trainer/global_step</td><td>759</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drn_120h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010547-drn_120h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_010707-drn_120h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run8' target=\"_blank\">drn_120h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/drn_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 43.5 K | train\n",
      "2 | relu              | ReLU          | 0      | train\n",
      "3 | softplus          | Softplus      | 0      | train\n",
      "4 | last_linear_mu    | Linear        | 513    | train\n",
      "5 | last_linear_sigma | Linear        | 513    | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "46.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.9 K    Total params\n",
      "0.188     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:03<00:00, 10.18it/s, v_num=run8, train_loss_step=1.090, train_loss_epoch=1.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:03<00:00, 10.17it/s, v_num=run8, train_loss_step=1.090, train_loss_epoch=1.100]\n",
      "Final MSE Loss: tensor(1.0869)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>1.10187</td></tr><tr><td>train_loss_step</td><td>1.08695</td></tr><tr><td>trainer/global_step</td><td>759</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drn_120h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010707-drn_120h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_010829-drn_120h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run9' target=\"_blank\">drn_120h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/drn_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 43.5 K | train\n",
      "2 | relu              | ReLU          | 0      | train\n",
      "3 | softplus          | Softplus      | 0      | train\n",
      "4 | last_linear_mu    | Linear        | 513    | train\n",
      "5 | last_linear_sigma | Linear        | 513    | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "46.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "46.9 K    Total params\n",
      "0.188     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:04<00:00,  9.34it/s, v_num=run9, train_loss_step=1.080, train_loss_epoch=1.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 38/38 [00:04<00:00,  9.30it/s, v_num=run9, train_loss_step=1.080, train_loss_epoch=1.100]\n",
      "Final MSE Loss: tensor(1.0807)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>1.09829</td></tr><tr><td>train_loss_step</td><td>1.08069</td></tr><tr><td>trainer/global_step</td><td>759</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drn_120h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction/runs/drn_120h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/drn_reproduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010829-drn_120h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate",
   "id": "f6678ec29db9084f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:44:04.512599Z",
     "start_time": "2025-05-08T03:44:04.466595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_scaler = StandardScaler(with_std=False)  # wieso scalen wir überhaupt? => robuster?\n",
    "y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "test_rf_dataset = TensorDataset(torch.Tensor(test_rf[0].to_numpy()),\n",
    "                                torch.Tensor(y_scaler.transform(test_rf[1][[\"t2m\"]])))\n",
    "test_rf_loader = DataLoader(test_rf_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_f_dataset = TensorDataset(torch.Tensor(test_f[0].to_numpy()), torch.Tensor(y_scaler.transform(test_f[1][[\"t2m\"]])))\n",
    "test_f_loader = DataLoader(test_f_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = (test_rf_loader, test_f_loader)"
   ],
   "id": "f3bb6e468d9e9a33",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T03:51:05.636085Z",
     "start_time": "2025-05-08T03:51:04.322393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# eval (aber wo ist validate?)\n",
    "data_list = [\"rf\", \"f\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            mydrn = CRPSDRN.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                embedding_dim=embed_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels=hidden_channels,\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=lr),\n",
    "            )\n",
    "            mydrn.eval()\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", enable_progress_bar=True, enable_model_summary=False)\n",
    "            preds = trainer.predict(model=mydrn, dataloaders=[tl]) #R2F\n",
    "            preds = torch.cat(preds, dim=0) # 0\n",
    "            # print(len(preds))\n",
    "            # Reverse transform of the y_scaler (only on the mean)\n",
    "            preds[:, 0] = torch.Tensor(y_scaler.inverse_transform(preds[:, 0].view(-1, 1))).flatten() # 273\n",
    "            preds_list.append(preds)\n",
    "            print(preds_list)\n",
    "            # print(f\"preds shape: {preds.shape}\")\n",
    "\n",
    "    # targets = torch.Tensor(targets.t2m.values)\n",
    "    if data == \"f\":\n",
    "        targets = test_f[1]\n",
    "    else:\n",
    "        targets = test_rf[1]\n",
    "    targets = torch.tensor(targets.t2m.values) #  - 273.15\n",
    "    print(\"targets\")\n",
    "    print(targets)\n",
    "    # print(\"t2m values:\")\n",
    "    # print(f\"targets shape: {targets.shape}\")\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "    # print(\"final_preds\")\n",
    "    # print(final_preds)\n",
    "\n",
    "    res = mydrn.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    '''\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    # print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_drn_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")\n",
    "    '''"
   ],
   "id": "ffbe839e13e80b27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from drn_24h_train_run0.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 19/19 [00:00<00:00, 32.31it/s]\n",
      "[tensor([[280.1857,   0.8083],\n",
      "        [280.0982,   0.6068],\n",
      "        [280.4253,   0.6386],\n",
      "        ...,\n",
      "        [278.6718,   3.2550],\n",
      "        [275.6284,   2.4455],\n",
      "        [271.8437,   2.3964]])]\n",
      "[INFO] Loading model from drn_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:  53%|█████▎    | 10/19 [00:00<00:00, 24.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:897\u001B[0m, in \u001B[0;36mTrainer._predict_impl\u001B[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001B[0m\n\u001B[1;32m    894\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    895\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn, ckpt_path, model_provided\u001B[38;5;241m=\u001B[39mmodel_provided, model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    896\u001B[0m )\n\u001B[0;32m--> 897\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    899\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1020\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting:\n\u001B[0;32m-> 1020\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1021\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:178\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 178\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/prediction_loop.py:121\u001B[0m, in \u001B[0;36m_PredictionLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    120\u001B[0m     dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 121\u001B[0m     batch, batch_idx, dataloader_idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:133\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;66;03m# the iterator is empty\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py:60\u001B[0m, in \u001B[0;36m_DataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 60\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001B[0m, in \u001B[0;36mCombinedLoader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator, _Sequential):\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001B[0m, in \u001B[0;36m_Sequential.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 142\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterators\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;66;03m# try the next iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    672\u001B[0m index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 19\u001B[0m\n\u001B[1;32m     16\u001B[0m mydrn\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     18\u001B[0m trainer \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mTrainer(log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m, enable_progress_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, enable_model_summary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 19\u001B[0m preds \u001B[38;5;241m=\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmydrn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mtl\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#R2F\u001B[39;00m\n\u001B[1;32m     20\u001B[0m preds \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(preds, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;66;03m# 0\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# print(len(preds))\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Reverse transform of the y_scaler (only on the mean)\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:858\u001B[0m, in \u001B[0;36mTrainer.predict\u001B[0;34m(self, model, dataloaders, datamodule, return_predictions, ckpt_path)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    857\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredicting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 858\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_predict_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_predictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 64\u001B[0m     \u001B[43mexit\u001B[49m(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PIT Histogram",
   "id": "b249969f31082989"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:11:13.617905Z",
     "start_time": "2025-05-04T23:11:13.568185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT\n",
    "DATASET = \"rf\"\n",
    "predictions = pd.read_csv(os.path.join(RESULTPATH, f\"{DATASET}_drn_{leadtime}_results.csv\"))\n",
    "# f_drn_24h_results.csv\n",
    "predictions = predictions.dropna(axis=0)\n",
    "\n",
    "y = torch.tensor(predictions[\"t2m\"].to_numpy())\n",
    "preds = torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy())\n",
    "\n",
    "print(SAVEPATH)"
   ],
   "id": "ec4c13a340724eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp/leas_trained_models/drn_120h/models\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:11:15.350649Z",
     "start_time": "2025-05-04T23:11:15.326106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = predictions[\"t2m\"].to_numpy()\n",
    "print(y)\n",
    "print(y.shape)\n",
    "mu = predictions[\"mu\"].to_numpy()\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "sigma = predictions[\"sigma\"].to_numpy()\n",
    "print(sigma)\n",
    "print(sigma.shape)\n",
    "\n",
    "normalCRPS = NormalCRPS()\n",
    "\n",
    "err = normalCRPS.crps(mu_sigma=torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy()),\n",
    "                      y=torch.tensor(predictions[\"t2m\"].to_numpy())).item()\n",
    "err"
   ],
   "id": "bcd31b76a8475b2a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[279.65 280.05 279.65 ... 279.65 278.15 276.25]\n",
      "(76917,)\n",
      "[278.30285645 278.03045654 278.72848511 ... 275.7442627  272.75747681\n",
      " 270.65548706]\n",
      "(76917,)\n",
      "[2.03647971 1.79880738 1.70109701 ... 3.42186356 3.54197216 4.01046705]\n",
      "(76917,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1900603909772431"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:11:17.307244Z",
     "start_time": "2025-05-04T23:11:17.163103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT for 24h\n",
    "probs = norm.cdf(y.flatten(), loc=mu.flatten(), scale=sigma.flatten())  # scale is standard deviation\n",
    "n, bins, patches = plt.hist(probs, bins=15, density=True)\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= (max(col))\n",
    "\n",
    "plt.ylim(0, 1.5)  # Layout\n",
    "plt.hlines(xmin=0, xmax=1, y=1, colors=\"black\", linestyles=\"--\")"
   ],
   "id": "949de41536803c62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f4e3829d8a0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 72h R2F\n",
    " not used anymore - directly changed MODEL = \"24h\" in the above code"
   ],
   "id": "30675838ab6da53b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dataframes for train and valie",
   "id": "16c57bd02d652b91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:28:44.708469Z",
     "start_time": "2025-03-14T16:28:38.598145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataloader and preprocessing\n",
    "MODEL = \"72h\"\n",
    "dataframes = load_dataframes(mode=\"train\",\n",
    "                             leadtime=MODEL)  # train mode => for training nn? Wie wird das im Paper beschrieben?\n",
    "dataframes = summary_statistics(\n",
    "    dataframes)  # wie sehen die daten von summary statistics aus? => wenn das nur die Daten von einer Station sind, dann über Zeitpunkte\n",
    "dataframes.pop(\"stations\")\n",
    "\n",
    "# test\n",
    "for X, y in dataframes.values():  # wofuer?\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train, valid_test = normalize_features(\n",
    "    training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    ")\n",
    "\n",
    "train = drop_nans(train)\n",
    "(test_rf, test_f) = valid_test\n",
    "test_rf = drop_nans(test_rf)\n",
    "test_f = drop_nans(test_f)\n",
    "\n",
    "SAVEPATH = os.path.join(PATH, f\"drn_{MODEL}/models\")"
   ],
   "id": "4175a84e0a03878c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "[INFO] Normalizing features...\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:29:05.218943Z",
     "start_time": "2025-03-14T16:29:05.210012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONFIG_FOLDER = os.path.join(DIRECTORY, f\"trained_models/drn_{MODEL}\")\n",
    "JSONPATH = os.path.join(CONFIG_FOLDER, \"params.json\")\n",
    "\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "print(args_dict)\n",
    "config = args_dict"
   ],
   "id": "8d319e588cfaa368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/drn_72h/params.json\n",
      "{'leadtime': '72h', 'batch_size': 8192, 'lr': 0.001, 'hidden_channels': [512], 'max_epochs': 27, 'only_summary': 'True'}\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train",
   "id": "f635cca89eef4633"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:32:06.573305Z",
     "start_time": "2025-03-14T16:29:50.255235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with wandb.init(\n",
    "        project=\"reproduction\",\n",
    "        id=f\"training_run_{MODEL}_crps\",\n",
    "        config=config,\n",
    "        tags=[\"exploration\"],\n",
    "):\n",
    "    config = wandb.config\n",
    "    y_scaler = StandardScaler(with_std=False)  # wieso scalen wir überhaupt? => robuster?\n",
    "    y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "    #batch_size =2048\n",
    "    #hidden_size=128\n",
    "    #lr=0.0002\n",
    "    #max_epochs=31\n",
    "\n",
    "    embed_dim = 20\n",
    "    in_channels = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "    train_dataset = TensorDataset(torch.Tensor(train[0].to_numpy()),\n",
    "                                  torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]])))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    mydrn = CRPSDRN(\n",
    "        embedding_dim=embed_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=config.hidden_channels,\n",
    "\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config.lr),\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"all_station_crps\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=f\"run_{MODEL}\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config.max_epochs,\n",
    "        log_every_n_steps=10,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=mydrn, train_dataloaders=train_loader)\n",
    "\n",
    "    final_loss = trainer.logged_metrics[\"train_loss_step\"]\n",
    "    print(\"Final MSE Loss:\", final_loss)"
   ],
   "id": "da54bfdf06d9c210",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250314_172950-training_run_72h_crps</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/reproduction/runs/training_run_72h_crps' target=\"_blank\">training_run_72h_crps</a></strong> to <a href='https://wandb.ai/leachen_thesis/reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/reproduction' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/reproduction/runs/training_run_72h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction/runs/training_run_72h_crps</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/drn_72h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 43.5 K | train\n",
      "2 | relu              | ReLU          | 0      | train\n",
      "3 | softplus          | Softplus      | 0      | train\n",
      "4 | last_linear_mu    | Linear        | 513    | train\n",
      "5 | last_linear_sigma | Linear        | 513    | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "47.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "47.0 K    Total params\n",
      "0.188     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/49 [00:00<?, ?it/s] torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   2%|▏         | 1/49 [00:00<00:03, 13.34it/s, v_num=crps, train_loss_step=4.900]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   4%|▍         | 2/49 [00:00<00:03, 15.32it/s, v_num=crps, train_loss_step=4.690]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   6%|▌         | 3/49 [00:00<00:03, 14.26it/s, v_num=crps, train_loss_step=4.560]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   8%|▊         | 4/49 [00:00<00:03, 14.19it/s, v_num=crps, train_loss_step=4.320]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  10%|█         | 5/49 [00:00<00:05,  8.56it/s, v_num=crps, train_loss_step=4.230]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  12%|█▏        | 6/49 [00:00<00:04,  9.14it/s, v_num=crps, train_loss_step=4.020]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  14%|█▍        | 7/49 [00:00<00:04,  9.84it/s, v_num=crps, train_loss_step=3.850]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  16%|█▋        | 8/49 [00:00<00:03, 10.44it/s, v_num=crps, train_loss_step=3.660]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  18%|█▊        | 9/49 [00:00<00:03, 10.97it/s, v_num=crps, train_loss_step=3.530]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  20%|██        | 10/49 [00:01<00:04,  9.19it/s, v_num=crps, train_loss_step=3.370]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  22%|██▏       | 11/49 [00:01<00:03,  9.59it/s, v_num=crps, train_loss_step=3.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  24%|██▍       | 12/49 [00:01<00:03,  9.68it/s, v_num=crps, train_loss_step=2.990]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  27%|██▋       | 13/49 [00:01<00:03, 10.04it/s, v_num=crps, train_loss_step=2.850]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  29%|██▊       | 14/49 [00:01<00:03, 10.25it/s, v_num=crps, train_loss_step=2.650]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  31%|███       | 15/49 [00:01<00:03, 10.36it/s, v_num=crps, train_loss_step=2.500]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  33%|███▎      | 16/49 [00:01<00:03,  9.29it/s, v_num=crps, train_loss_step=2.330]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  35%|███▍      | 17/49 [00:01<00:03,  9.58it/s, v_num=crps, train_loss_step=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  37%|███▋      | 18/49 [00:01<00:03,  9.84it/s, v_num=crps, train_loss_step=2.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  39%|███▉      | 19/49 [00:01<00:02, 10.08it/s, v_num=crps, train_loss_step=2.030]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  41%|████      | 20/49 [00:01<00:02, 10.32it/s, v_num=crps, train_loss_step=1.930]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  43%|████▎     | 21/49 [00:02<00:02,  9.58it/s, v_num=crps, train_loss_step=1.830]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  45%|████▍     | 22/49 [00:02<00:02,  9.79it/s, v_num=crps, train_loss_step=1.780]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  47%|████▋     | 23/49 [00:02<00:02,  9.93it/s, v_num=crps, train_loss_step=1.720]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  49%|████▉     | 24/49 [00:02<00:02, 10.13it/s, v_num=crps, train_loss_step=1.710]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  51%|█████     | 25/49 [00:02<00:02, 10.16it/s, v_num=crps, train_loss_step=1.660]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  53%|█████▎    | 26/49 [00:02<00:02,  9.38it/s, v_num=crps, train_loss_step=1.610]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  55%|█████▌    | 27/49 [00:02<00:02,  9.49it/s, v_num=crps, train_loss_step=1.620]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  57%|█████▋    | 28/49 [00:02<00:02,  9.53it/s, v_num=crps, train_loss_step=1.610]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  59%|█████▉    | 29/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=1.600]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  61%|██████    | 30/49 [00:03<00:01,  9.83it/s, v_num=crps, train_loss_step=1.590]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  63%|██████▎   | 31/49 [00:03<00:01,  9.89it/s, v_num=crps, train_loss_step=1.590]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  65%|██████▌   | 32/49 [00:03<00:01,  9.45it/s, v_num=crps, train_loss_step=1.540]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  67%|██████▋   | 33/49 [00:03<00:01,  9.59it/s, v_num=crps, train_loss_step=1.570]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  69%|██████▉   | 34/49 [00:03<00:01,  9.67it/s, v_num=crps, train_loss_step=1.550]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  71%|███████▏  | 35/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=1.540]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  73%|███████▎  | 36/49 [00:03<00:01,  9.93it/s, v_num=crps, train_loss_step=1.490]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  76%|███████▌  | 37/49 [00:03<00:01,  9.53it/s, v_num=crps, train_loss_step=1.480]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  78%|███████▊  | 38/49 [00:03<00:01,  9.64it/s, v_num=crps, train_loss_step=1.490]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  80%|███████▉  | 39/49 [00:04<00:01,  9.74it/s, v_num=crps, train_loss_step=1.430]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  82%|████████▏ | 40/49 [00:04<00:00,  9.83it/s, v_num=crps, train_loss_step=1.430]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  84%|████████▎ | 41/49 [00:04<00:00,  9.94it/s, v_num=crps, train_loss_step=1.390]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  86%|████████▌ | 42/49 [00:04<00:00, 10.01it/s, v_num=crps, train_loss_step=1.380]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  88%|████████▊ | 43/49 [00:04<00:00,  9.62it/s, v_num=crps, train_loss_step=1.370]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  90%|████████▉ | 44/49 [00:04<00:00,  9.72it/s, v_num=crps, train_loss_step=1.330]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  92%|█████████▏| 45/49 [00:04<00:00,  9.75it/s, v_num=crps, train_loss_step=1.300]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  94%|█████████▍| 46/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=1.310]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  96%|█████████▌| 47/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=1.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  98%|█████████▊| 48/49 [00:05<00:00,  9.55it/s, v_num=crps, train_loss_step=1.270]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 1:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=2.240]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   2%|▏         | 1/49 [00:00<00:03, 12.07it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   4%|▍         | 2/49 [00:00<00:03, 14.34it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   6%|▌         | 3/49 [00:00<00:03, 14.08it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   8%|▊         | 4/49 [00:00<00:03, 13.20it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  10%|█         | 5/49 [00:00<00:04,  8.85it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  12%|█▏        | 6/49 [00:00<00:04,  9.45it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  14%|█▍        | 7/49 [00:00<00:04, 10.15it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  16%|█▋        | 8/49 [00:00<00:03, 10.27it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  18%|█▊        | 9/49 [00:00<00:03, 10.79it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  20%|██        | 10/49 [00:01<00:04,  9.20it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  22%|██▏       | 11/49 [00:01<00:03,  9.63it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  24%|██▍       | 12/49 [00:01<00:03, 10.04it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  27%|██▋       | 13/49 [00:01<00:03, 10.41it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  29%|██▊       | 14/49 [00:01<00:03, 10.75it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  31%|███       | 15/49 [00:01<00:03, 11.02it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  33%|███▎      | 16/49 [00:01<00:03,  9.85it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  35%|███▍      | 17/49 [00:01<00:03, 10.13it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  37%|███▋      | 18/49 [00:01<00:02, 10.38it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  39%|███▉      | 19/49 [00:01<00:02, 10.49it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  41%|████      | 20/49 [00:01<00:02, 10.72it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  43%|████▎     | 21/49 [00:02<00:02,  9.92it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  45%|████▍     | 22/49 [00:02<00:02, 10.13it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  47%|████▋     | 23/49 [00:02<00:02, 10.29it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  49%|████▉     | 24/49 [00:02<00:02, 10.48it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  51%|█████     | 25/49 [00:02<00:02, 10.66it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  53%|█████▎    | 26/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=1.070, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  55%|█████▌    | 27/49 [00:02<00:02, 10.17it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  57%|█████▋    | 28/49 [00:02<00:02, 10.34it/s, v_num=crps, train_loss_step=1.060, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  59%|█████▉    | 29/49 [00:02<00:01, 10.50it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  61%|██████    | 30/49 [00:02<00:01, 10.66it/s, v_num=crps, train_loss_step=1.070, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  63%|██████▎   | 31/49 [00:03<00:01, 10.10it/s, v_num=crps, train_loss_step=1.060, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  65%|██████▌   | 32/49 [00:03<00:01, 10.24it/s, v_num=crps, train_loss_step=1.060, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  67%|██████▋   | 33/49 [00:03<00:01, 10.38it/s, v_num=crps, train_loss_step=1.050, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  69%|██████▉   | 34/49 [00:03<00:01, 10.51it/s, v_num=crps, train_loss_step=1.050, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  71%|███████▏  | 35/49 [00:03<00:01, 10.64it/s, v_num=crps, train_loss_step=1.060, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  73%|███████▎  | 36/49 [00:03<00:01, 10.76it/s, v_num=crps, train_loss_step=1.060, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  76%|███████▌  | 37/49 [00:03<00:01, 10.29it/s, v_num=crps, train_loss_step=1.050, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  78%|███████▊  | 38/49 [00:03<00:01, 10.34it/s, v_num=crps, train_loss_step=1.050, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  80%|███████▉  | 39/49 [00:03<00:00, 10.46it/s, v_num=crps, train_loss_step=1.050, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  82%|████████▏ | 40/49 [00:03<00:00, 10.58it/s, v_num=crps, train_loss_step=1.040, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  84%|████████▎ | 41/49 [00:03<00:00, 10.69it/s, v_num=crps, train_loss_step=1.040, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  86%|████████▌ | 42/49 [00:04<00:00, 10.27it/s, v_num=crps, train_loss_step=1.040, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  88%|████████▊ | 43/49 [00:04<00:00, 10.38it/s, v_num=crps, train_loss_step=1.030, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  90%|████████▉ | 44/49 [00:04<00:00, 10.48it/s, v_num=crps, train_loss_step=1.040, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  92%|█████████▏| 45/49 [00:04<00:00, 10.58it/s, v_num=crps, train_loss_step=1.030, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  94%|█████████▍| 46/49 [00:04<00:00, 10.68it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  96%|█████████▌| 47/49 [00:04<00:00, 10.78it/s, v_num=crps, train_loss_step=1.020, train_loss_epoch=2.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  98%|█████████▊| 48/49 [00:04<00:00, 10.40it/s, v_num=crps, train_loss_step=1.020, train_loss_epoch=2.240]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 2:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.020, train_loss_epoch=1.100]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   2%|▏         | 1/49 [00:00<00:03, 12.51it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   4%|▍         | 2/49 [00:00<00:03, 12.16it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   6%|▌         | 3/49 [00:00<00:03, 13.68it/s, v_num=crps, train_loss_step=1.000, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   8%|▊         | 4/49 [00:00<00:05,  8.45it/s, v_num=crps, train_loss_step=1.000, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  10%|█         | 5/49 [00:00<00:04,  9.47it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  12%|█▏        | 6/49 [00:00<00:04, 10.24it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  14%|█▍        | 7/49 [00:00<00:03, 10.89it/s, v_num=crps, train_loss_step=1.000, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  16%|█▋        | 8/49 [00:00<00:03, 11.08it/s, v_num=crps, train_loss_step=0.981, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  18%|█▊        | 9/49 [00:00<00:03, 11.17it/s, v_num=crps, train_loss_step=0.994, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  20%|██        | 10/49 [00:01<00:04,  9.44it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  22%|██▏       | 11/49 [00:01<00:03,  9.87it/s, v_num=crps, train_loss_step=1.000, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  24%|██▍       | 12/49 [00:01<00:03, 10.27it/s, v_num=crps, train_loss_step=0.997, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  27%|██▋       | 13/49 [00:01<00:03, 10.49it/s, v_num=crps, train_loss_step=0.984, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  29%|██▊       | 14/49 [00:01<00:03, 10.82it/s, v_num=crps, train_loss_step=1.010, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  31%|███       | 15/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=0.986, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  33%|███▎      | 16/49 [00:01<00:03,  9.99it/s, v_num=crps, train_loss_step=0.993, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  35%|███▍      | 17/49 [00:01<00:03, 10.26it/s, v_num=crps, train_loss_step=0.983, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  37%|███▋      | 18/49 [00:01<00:02, 10.53it/s, v_num=crps, train_loss_step=0.988, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  39%|███▉      | 19/49 [00:01<00:02, 10.77it/s, v_num=crps, train_loss_step=1.000, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  41%|████      | 20/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=0.992, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  43%|████▎     | 21/49 [00:02<00:02, 10.11it/s, v_num=crps, train_loss_step=0.971, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  45%|████▍     | 22/49 [00:02<00:02, 10.31it/s, v_num=crps, train_loss_step=0.979, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  47%|████▋     | 23/49 [00:02<00:02, 10.51it/s, v_num=crps, train_loss_step=0.989, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  49%|████▉     | 24/49 [00:02<00:02, 10.69it/s, v_num=crps, train_loss_step=0.972, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  51%|█████     | 25/49 [00:02<00:02, 10.88it/s, v_num=crps, train_loss_step=0.964, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  53%|█████▎    | 26/49 [00:02<00:02, 10.20it/s, v_num=crps, train_loss_step=0.975, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  55%|█████▌    | 27/49 [00:02<00:02, 10.37it/s, v_num=crps, train_loss_step=0.993, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  57%|█████▋    | 28/49 [00:02<00:01, 10.53it/s, v_num=crps, train_loss_step=0.968, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  59%|█████▉    | 29/49 [00:02<00:01, 10.68it/s, v_num=crps, train_loss_step=0.963, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  61%|██████    | 30/49 [00:02<00:01, 10.83it/s, v_num=crps, train_loss_step=0.970, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  63%|██████▎   | 31/49 [00:03<00:01, 10.25it/s, v_num=crps, train_loss_step=0.950, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  65%|██████▌   | 32/49 [00:03<00:01, 10.39it/s, v_num=crps, train_loss_step=0.958, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  67%|██████▋   | 33/49 [00:03<00:01, 10.53it/s, v_num=crps, train_loss_step=0.953, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  69%|██████▉   | 34/49 [00:03<00:01, 10.67it/s, v_num=crps, train_loss_step=0.965, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  71%|███████▏  | 35/49 [00:03<00:01, 10.79it/s, v_num=crps, train_loss_step=0.957, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  73%|███████▎  | 36/49 [00:03<00:01, 10.29it/s, v_num=crps, train_loss_step=0.963, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  76%|███████▌  | 37/49 [00:03<00:01, 10.41it/s, v_num=crps, train_loss_step=0.954, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  78%|███████▊  | 38/49 [00:03<00:01, 10.53it/s, v_num=crps, train_loss_step=0.962, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  80%|███████▉  | 39/49 [00:03<00:00, 10.64it/s, v_num=crps, train_loss_step=0.963, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  82%|████████▏ | 40/49 [00:03<00:00, 10.69it/s, v_num=crps, train_loss_step=0.962, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  84%|████████▎ | 41/49 [00:04<00:00, 10.19it/s, v_num=crps, train_loss_step=0.949, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  86%|████████▌ | 42/49 [00:04<00:00, 10.30it/s, v_num=crps, train_loss_step=0.946, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  88%|████████▊ | 43/49 [00:04<00:00, 10.40it/s, v_num=crps, train_loss_step=0.948, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  90%|████████▉ | 44/49 [00:04<00:00, 10.51it/s, v_num=crps, train_loss_step=0.953, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  92%|█████████▏| 45/49 [00:04<00:00, 10.61it/s, v_num=crps, train_loss_step=0.941, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  94%|█████████▍| 46/49 [00:04<00:00, 10.70it/s, v_num=crps, train_loss_step=0.948, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  96%|█████████▌| 47/49 [00:04<00:00, 10.32it/s, v_num=crps, train_loss_step=0.945, train_loss_epoch=1.100]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  98%|█████████▊| 48/49 [00:04<00:00, 10.42it/s, v_num=crps, train_loss_step=0.942, train_loss_epoch=1.100]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 3:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.948, train_loss_epoch=0.976]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   2%|▏         | 1/49 [00:00<00:03, 12.50it/s, v_num=crps, train_loss_step=0.931, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   4%|▍         | 2/49 [00:00<00:03, 14.57it/s, v_num=crps, train_loss_step=0.936, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   6%|▌         | 3/49 [00:00<00:06,  7.20it/s, v_num=crps, train_loss_step=0.942, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   8%|▊         | 4/49 [00:00<00:05,  8.49it/s, v_num=crps, train_loss_step=0.931, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  10%|█         | 5/49 [00:00<00:04,  9.52it/s, v_num=crps, train_loss_step=0.922, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  12%|█▏        | 6/49 [00:00<00:04, 10.35it/s, v_num=crps, train_loss_step=0.930, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  14%|█▍        | 7/49 [00:00<00:03, 11.07it/s, v_num=crps, train_loss_step=0.952, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  16%|█▋        | 8/49 [00:00<00:03, 11.66it/s, v_num=crps, train_loss_step=0.946, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  18%|█▊        | 9/49 [00:00<00:04,  9.29it/s, v_num=crps, train_loss_step=0.940, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  20%|██        | 10/49 [00:01<00:04,  9.68it/s, v_num=crps, train_loss_step=0.939, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  22%|██▏       | 11/49 [00:01<00:03,  9.92it/s, v_num=crps, train_loss_step=0.952, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  24%|██▍       | 12/49 [00:01<00:03, 10.31it/s, v_num=crps, train_loss_step=0.934, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  27%|██▋       | 13/49 [00:01<00:03, 10.63it/s, v_num=crps, train_loss_step=0.928, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  29%|██▊       | 14/49 [00:01<00:03,  9.40it/s, v_num=crps, train_loss_step=0.940, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  31%|███       | 15/49 [00:01<00:03,  9.72it/s, v_num=crps, train_loss_step=0.928, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  33%|███▎      | 16/49 [00:01<00:03, 10.01it/s, v_num=crps, train_loss_step=0.934, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  35%|███▍      | 17/49 [00:01<00:03, 10.29it/s, v_num=crps, train_loss_step=0.925, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  37%|███▋      | 18/49 [00:01<00:02, 10.53it/s, v_num=crps, train_loss_step=0.936, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  39%|███▉      | 19/49 [00:01<00:02, 10.77it/s, v_num=crps, train_loss_step=0.938, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  41%|████      | 20/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=0.923, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  43%|████▎     | 21/49 [00:02<00:02, 10.13it/s, v_num=crps, train_loss_step=0.941, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  45%|████▍     | 22/49 [00:02<00:02, 10.34it/s, v_num=crps, train_loss_step=0.941, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  47%|████▋     | 23/49 [00:02<00:02, 10.54it/s, v_num=crps, train_loss_step=0.934, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  49%|████▉     | 24/49 [00:02<00:02, 10.74it/s, v_num=crps, train_loss_step=0.915, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  51%|█████     | 25/49 [00:02<00:02,  9.98it/s, v_num=crps, train_loss_step=0.929, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  53%|█████▎    | 26/49 [00:02<00:02, 10.04it/s, v_num=crps, train_loss_step=0.918, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  55%|█████▌    | 27/49 [00:02<00:02, 10.14it/s, v_num=crps, train_loss_step=0.929, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  57%|█████▋    | 28/49 [00:02<00:02, 10.14it/s, v_num=crps, train_loss_step=0.932, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  59%|█████▉    | 29/49 [00:02<00:01, 10.16it/s, v_num=crps, train_loss_step=0.939, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  61%|██████    | 30/49 [00:03<00:01,  9.66it/s, v_num=crps, train_loss_step=0.933, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  63%|██████▎   | 31/49 [00:03<00:01,  9.82it/s, v_num=crps, train_loss_step=0.932, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  65%|██████▌   | 32/49 [00:03<00:01,  9.97it/s, v_num=crps, train_loss_step=0.927, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  67%|██████▋   | 33/49 [00:03<00:01, 10.04it/s, v_num=crps, train_loss_step=0.925, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  69%|██████▉   | 34/49 [00:03<00:01, 10.17it/s, v_num=crps, train_loss_step=0.928, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  71%|███████▏  | 35/49 [00:03<00:01, 10.23it/s, v_num=crps, train_loss_step=0.917, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  73%|███████▎  | 36/49 [00:03<00:01,  9.75it/s, v_num=crps, train_loss_step=0.936, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  76%|███████▌  | 37/49 [00:03<00:01,  9.84it/s, v_num=crps, train_loss_step=0.921, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  78%|███████▊  | 38/49 [00:03<00:01,  9.88it/s, v_num=crps, train_loss_step=0.922, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  80%|███████▉  | 39/49 [00:03<00:01,  9.90it/s, v_num=crps, train_loss_step=0.916, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  82%|████████▏ | 40/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=0.903, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  84%|████████▎ | 41/49 [00:04<00:00,  9.63it/s, v_num=crps, train_loss_step=0.931, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  86%|████████▌ | 42/49 [00:04<00:00,  9.74it/s, v_num=crps, train_loss_step=0.912, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  88%|████████▊ | 43/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=0.925, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  90%|████████▉ | 44/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=0.916, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  92%|█████████▏| 45/49 [00:04<00:00, 10.01it/s, v_num=crps, train_loss_step=0.935, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  94%|█████████▍| 46/49 [00:04<00:00,  9.64it/s, v_num=crps, train_loss_step=0.907, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  96%|█████████▌| 47/49 [00:04<00:00,  9.75it/s, v_num=crps, train_loss_step=0.910, train_loss_epoch=0.976]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  98%|█████████▊| 48/49 [00:04<00:00,  9.79it/s, v_num=crps, train_loss_step=0.922, train_loss_epoch=0.976]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 4:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.915, train_loss_epoch=0.929]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   2%|▏         | 1/49 [00:00<00:03, 13.16it/s, v_num=crps, train_loss_step=0.909, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   4%|▍         | 2/49 [00:00<00:03, 15.06it/s, v_num=crps, train_loss_step=0.924, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   6%|▌         | 3/49 [00:00<00:05,  7.71it/s, v_num=crps, train_loss_step=0.928, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   8%|▊         | 4/49 [00:00<00:05,  8.95it/s, v_num=crps, train_loss_step=0.941, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  10%|█         | 5/49 [00:00<00:04,  9.68it/s, v_num=crps, train_loss_step=0.921, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  12%|█▏        | 6/49 [00:00<00:04, 10.14it/s, v_num=crps, train_loss_step=0.908, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  14%|█▍        | 7/49 [00:00<00:03, 10.85it/s, v_num=crps, train_loss_step=0.918, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  16%|█▋        | 8/49 [00:00<00:04,  8.88it/s, v_num=crps, train_loss_step=0.900, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  18%|█▊        | 9/49 [00:00<00:04,  9.28it/s, v_num=crps, train_loss_step=0.920, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  20%|██        | 10/49 [00:01<00:03,  9.75it/s, v_num=crps, train_loss_step=0.906, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  22%|██▏       | 11/49 [00:01<00:03, 10.15it/s, v_num=crps, train_loss_step=0.916, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  24%|██▍       | 12/49 [00:01<00:03, 10.53it/s, v_num=crps, train_loss_step=0.913, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  27%|██▋       | 13/49 [00:01<00:03, 10.87it/s, v_num=crps, train_loss_step=0.909, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  29%|██▊       | 14/49 [00:01<00:03,  9.56it/s, v_num=crps, train_loss_step=0.917, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  31%|███       | 15/49 [00:01<00:03,  9.87it/s, v_num=crps, train_loss_step=0.905, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  33%|███▎      | 16/49 [00:01<00:03, 10.15it/s, v_num=crps, train_loss_step=0.898, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  35%|███▍      | 17/49 [00:01<00:03, 10.43it/s, v_num=crps, train_loss_step=0.916, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  37%|███▋      | 18/49 [00:01<00:02, 10.70it/s, v_num=crps, train_loss_step=0.906, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  39%|███▉      | 19/49 [00:01<00:03,  9.80it/s, v_num=crps, train_loss_step=0.914, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  41%|████      | 20/49 [00:01<00:02, 10.04it/s, v_num=crps, train_loss_step=0.903, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  43%|████▎     | 21/49 [00:02<00:02, 10.27it/s, v_num=crps, train_loss_step=0.907, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  45%|████▍     | 22/49 [00:02<00:02, 10.47it/s, v_num=crps, train_loss_step=0.911, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  47%|████▋     | 23/49 [00:02<00:02, 10.67it/s, v_num=crps, train_loss_step=0.901, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  49%|████▉     | 24/49 [00:02<00:02, 10.66it/s, v_num=crps, train_loss_step=0.887, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  51%|█████     | 25/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=0.905, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  53%|█████▎    | 26/49 [00:02<00:02, 10.00it/s, v_num=crps, train_loss_step=0.893, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  55%|█████▌    | 27/49 [00:02<00:02, 10.08it/s, v_num=crps, train_loss_step=0.914, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  57%|█████▋    | 28/49 [00:02<00:02, 10.13it/s, v_num=crps, train_loss_step=0.912, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  59%|█████▉    | 29/49 [00:02<00:01, 10.22it/s, v_num=crps, train_loss_step=0.899, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  61%|██████    | 30/49 [00:03<00:01,  9.69it/s, v_num=crps, train_loss_step=0.907, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  63%|██████▎   | 31/49 [00:03<00:01,  9.84it/s, v_num=crps, train_loss_step=0.898, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  65%|██████▌   | 32/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=0.905, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  67%|██████▋   | 33/49 [00:03<00:01, 10.12it/s, v_num=crps, train_loss_step=0.889, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  69%|██████▉   | 34/49 [00:03<00:01, 10.25it/s, v_num=crps, train_loss_step=0.895, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  71%|███████▏  | 35/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=0.916, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  73%|███████▎  | 36/49 [00:03<00:01,  9.88it/s, v_num=crps, train_loss_step=0.902, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  76%|███████▌  | 37/49 [00:03<00:01, 10.01it/s, v_num=crps, train_loss_step=0.895, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  78%|███████▊  | 38/49 [00:03<00:01, 10.13it/s, v_num=crps, train_loss_step=0.914, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  80%|███████▉  | 39/49 [00:03<00:00, 10.22it/s, v_num=crps, train_loss_step=0.903, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  82%|████████▏ | 40/49 [00:03<00:00, 10.31it/s, v_num=crps, train_loss_step=0.900, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  84%|████████▎ | 41/49 [00:04<00:00,  9.92it/s, v_num=crps, train_loss_step=0.902, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  86%|████████▌ | 42/49 [00:04<00:00, 10.03it/s, v_num=crps, train_loss_step=0.896, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  88%|████████▊ | 43/49 [00:04<00:00, 10.13it/s, v_num=crps, train_loss_step=0.891, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  90%|████████▉ | 44/49 [00:04<00:00, 10.22it/s, v_num=crps, train_loss_step=0.891, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  92%|█████████▏| 45/49 [00:04<00:00, 10.23it/s, v_num=crps, train_loss_step=0.896, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  94%|█████████▍| 46/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=0.900, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  96%|█████████▌| 47/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=0.912, train_loss_epoch=0.929]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  98%|█████████▊| 48/49 [00:04<00:00, 10.05it/s, v_num=crps, train_loss_step=0.890, train_loss_epoch=0.929]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 5:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.906]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   2%|▏         | 1/49 [00:00<00:03, 13.83it/s, v_num=crps, train_loss_step=0.898, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   4%|▍         | 2/49 [00:00<00:03, 15.56it/s, v_num=crps, train_loss_step=0.905, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   6%|▌         | 3/49 [00:00<00:05,  7.92it/s, v_num=crps, train_loss_step=0.916, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   8%|▊         | 4/49 [00:00<00:04,  9.25it/s, v_num=crps, train_loss_step=0.903, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  10%|█         | 5/49 [00:00<00:04, 10.18it/s, v_num=crps, train_loss_step=0.897, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  12%|█▏        | 6/49 [00:00<00:04, 10.19it/s, v_num=crps, train_loss_step=0.887, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  14%|█▍        | 7/49 [00:00<00:03, 10.89it/s, v_num=crps, train_loss_step=0.906, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  16%|█▋        | 8/49 [00:00<00:04,  8.96it/s, v_num=crps, train_loss_step=0.890, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  18%|█▊        | 9/49 [00:00<00:04,  9.51it/s, v_num=crps, train_loss_step=0.901, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  20%|██        | 10/49 [00:01<00:03,  9.97it/s, v_num=crps, train_loss_step=0.901, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  22%|██▏       | 11/49 [00:01<00:03, 10.03it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  24%|██▍       | 12/49 [00:01<00:03, 10.25it/s, v_num=crps, train_loss_step=0.905, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  27%|██▋       | 13/49 [00:01<00:03,  9.08it/s, v_num=crps, train_loss_step=0.880, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  29%|██▊       | 14/49 [00:01<00:03,  9.41it/s, v_num=crps, train_loss_step=0.897, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  31%|███       | 15/49 [00:01<00:03,  9.72it/s, v_num=crps, train_loss_step=0.899, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  33%|███▎      | 16/49 [00:01<00:03, 10.00it/s, v_num=crps, train_loss_step=0.892, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  35%|███▍      | 17/49 [00:01<00:03, 10.26it/s, v_num=crps, train_loss_step=0.884, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  37%|███▋      | 18/49 [00:01<00:03,  9.27it/s, v_num=crps, train_loss_step=0.890, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  39%|███▉      | 19/49 [00:01<00:03,  9.51it/s, v_num=crps, train_loss_step=0.880, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  41%|████      | 20/49 [00:02<00:02,  9.73it/s, v_num=crps, train_loss_step=0.898, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  43%|████▎     | 21/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=0.901, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  45%|████▍     | 22/49 [00:02<00:02, 10.17it/s, v_num=crps, train_loss_step=0.895, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  47%|████▋     | 23/49 [00:02<00:02, 10.37it/s, v_num=crps, train_loss_step=0.903, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  49%|████▉     | 24/49 [00:02<00:02,  9.71it/s, v_num=crps, train_loss_step=0.883, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  51%|█████     | 25/49 [00:02<00:02,  9.89it/s, v_num=crps, train_loss_step=0.892, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  53%|█████▎    | 26/49 [00:02<00:02, 10.07it/s, v_num=crps, train_loss_step=0.887, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  55%|█████▌    | 27/49 [00:02<00:02, 10.23it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  57%|█████▋    | 28/49 [00:02<00:02, 10.40it/s, v_num=crps, train_loss_step=0.886, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  59%|█████▉    | 29/49 [00:02<00:02,  9.82it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  61%|██████    | 30/49 [00:03<00:01,  9.97it/s, v_num=crps, train_loss_step=0.890, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  63%|██████▎   | 31/49 [00:03<00:01, 10.12it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  65%|██████▌   | 32/49 [00:03<00:01, 10.26it/s, v_num=crps, train_loss_step=0.894, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  67%|██████▋   | 33/49 [00:03<00:01, 10.35it/s, v_num=crps, train_loss_step=0.887, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  69%|██████▉   | 34/49 [00:03<00:01, 10.39it/s, v_num=crps, train_loss_step=0.886, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  71%|███████▏  | 35/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=0.887, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  73%|███████▎  | 36/49 [00:03<00:01,  9.91it/s, v_num=crps, train_loss_step=0.895, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  76%|███████▌  | 37/49 [00:03<00:01,  9.94it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  78%|███████▊  | 38/49 [00:03<00:01, 10.03it/s, v_num=crps, train_loss_step=0.889, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  80%|███████▉  | 39/49 [00:03<00:00, 10.13it/s, v_num=crps, train_loss_step=0.890, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  82%|████████▏ | 40/49 [00:04<00:00,  9.74it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  84%|████████▎ | 41/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  86%|████████▌ | 42/49 [00:04<00:00,  9.92it/s, v_num=crps, train_loss_step=0.886, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  88%|████████▊ | 43/49 [00:04<00:00, 10.03it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  90%|████████▉ | 44/49 [00:04<00:00, 10.09it/s, v_num=crps, train_loss_step=0.896, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  92%|█████████▏| 45/49 [00:04<00:00,  9.67it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  94%|█████████▍| 46/49 [00:04<00:00,  9.76it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  96%|█████████▌| 47/49 [00:04<00:00,  9.82it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.906]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  98%|█████████▊| 48/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.906]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 6:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.891]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   2%|▏         | 1/49 [00:00<00:03, 12.99it/s, v_num=crps, train_loss_step=0.901, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   4%|▍         | 2/49 [00:00<00:08,  5.74it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   6%|▌         | 3/49 [00:00<00:06,  7.38it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   8%|▊         | 4/49 [00:00<00:05,  8.27it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  10%|█         | 5/49 [00:00<00:04,  9.30it/s, v_num=crps, train_loss_step=0.883, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  12%|█▏        | 6/49 [00:00<00:04, 10.08it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  14%|█▍        | 7/49 [00:00<00:05,  8.20it/s, v_num=crps, train_loss_step=0.890, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  16%|█▋        | 8/49 [00:00<00:04,  8.82it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  18%|█▊        | 9/49 [00:00<00:04,  9.35it/s, v_num=crps, train_loss_step=0.889, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  20%|██        | 10/49 [00:01<00:03,  9.84it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  22%|██▏       | 11/49 [00:01<00:03, 10.28it/s, v_num=crps, train_loss_step=0.874, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  24%|██▍       | 12/49 [00:01<00:03, 10.65it/s, v_num=crps, train_loss_step=0.858, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  27%|██▋       | 13/49 [00:01<00:03,  9.40it/s, v_num=crps, train_loss_step=0.889, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  29%|██▊       | 14/49 [00:01<00:03,  9.58it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  31%|███       | 15/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  33%|███▎      | 16/49 [00:01<00:03,  9.88it/s, v_num=crps, train_loss_step=0.878, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  35%|███▍      | 17/49 [00:01<00:03,  9.95it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  37%|███▋      | 18/49 [00:01<00:03,  9.15it/s, v_num=crps, train_loss_step=0.899, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  39%|███▉      | 19/49 [00:02<00:03,  9.39it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  41%|████      | 20/49 [00:02<00:03,  9.62it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  43%|████▎     | 21/49 [00:02<00:02,  9.84it/s, v_num=crps, train_loss_step=0.886, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  45%|████▍     | 22/49 [00:02<00:02, 10.06it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  47%|████▋     | 23/49 [00:02<00:02,  9.42it/s, v_num=crps, train_loss_step=0.876, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  49%|████▉     | 24/49 [00:02<00:02,  9.62it/s, v_num=crps, train_loss_step=0.892, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  51%|█████     | 25/49 [00:02<00:02,  9.81it/s, v_num=crps, train_loss_step=0.884, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  53%|█████▎    | 26/49 [00:02<00:02,  9.99it/s, v_num=crps, train_loss_step=0.879, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  55%|█████▌    | 27/49 [00:02<00:02, 10.16it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  57%|█████▋    | 28/49 [00:02<00:02,  9.57it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  59%|█████▉    | 29/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=0.891, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  61%|██████    | 30/49 [00:03<00:01,  9.85it/s, v_num=crps, train_loss_step=0.883, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  63%|██████▎   | 31/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  65%|██████▌   | 32/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=0.874, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  67%|██████▋   | 33/49 [00:03<00:01, 10.17it/s, v_num=crps, train_loss_step=0.889, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  69%|██████▉   | 34/49 [00:03<00:01,  9.68it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  71%|███████▏  | 35/49 [00:03<00:01,  9.82it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  73%|███████▎  | 36/49 [00:03<00:01,  9.94it/s, v_num=crps, train_loss_step=0.887, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  76%|███████▌  | 37/49 [00:03<00:01,  9.97it/s, v_num=crps, train_loss_step=0.895, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  78%|███████▊  | 38/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  80%|███████▉  | 39/49 [00:04<00:01,  9.70it/s, v_num=crps, train_loss_step=0.875, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  82%|████████▏ | 40/49 [00:04<00:00,  9.74it/s, v_num=crps, train_loss_step=0.875, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  84%|████████▎ | 41/49 [00:04<00:00,  9.78it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  86%|████████▌ | 42/49 [00:04<00:00,  9.85it/s, v_num=crps, train_loss_step=0.880, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  88%|████████▊ | 43/49 [00:04<00:00,  9.88it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  90%|████████▉ | 44/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=0.874, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  92%|█████████▏| 45/49 [00:04<00:00,  9.65it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  94%|█████████▍| 46/49 [00:04<00:00,  9.75it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  96%|█████████▌| 47/49 [00:04<00:00,  9.85it/s, v_num=crps, train_loss_step=0.889, train_loss_epoch=0.891]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  98%|█████████▊| 48/49 [00:04<00:00,  9.95it/s, v_num=crps, train_loss_step=0.880, train_loss_epoch=0.891]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 7:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.884, train_loss_epoch=0.880]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   2%|▏         | 1/49 [00:00<00:13,  3.54it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   4%|▍         | 2/49 [00:00<00:07,  5.94it/s, v_num=crps, train_loss_step=0.873, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   6%|▌         | 3/49 [00:00<00:05,  7.67it/s, v_num=crps, train_loss_step=0.876, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   8%|▊         | 4/49 [00:00<00:05,  8.98it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  10%|█         | 5/49 [00:00<00:04, 10.00it/s, v_num=crps, train_loss_step=0.880, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  12%|█▏        | 6/49 [00:00<00:05,  7.95it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  14%|█▍        | 7/49 [00:00<00:04,  8.63it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  16%|█▋        | 8/49 [00:00<00:04,  9.09it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  18%|█▊        | 9/49 [00:00<00:04,  9.58it/s, v_num=crps, train_loss_step=0.871, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  20%|██        | 10/49 [00:01<00:03,  9.89it/s, v_num=crps, train_loss_step=0.873, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  22%|██▏       | 11/49 [00:01<00:03, 10.32it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  24%|██▍       | 12/49 [00:01<00:04,  9.10it/s, v_num=crps, train_loss_step=0.871, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  27%|██▋       | 13/49 [00:01<00:03,  9.47it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  29%|██▊       | 14/49 [00:01<00:03,  9.81it/s, v_num=crps, train_loss_step=0.858, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  31%|███       | 15/49 [00:01<00:03, 10.13it/s, v_num=crps, train_loss_step=0.868, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  33%|███▎      | 16/49 [00:01<00:03, 10.38it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  35%|███▍      | 17/49 [00:01<00:03,  9.45it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  37%|███▋      | 18/49 [00:01<00:03,  9.71it/s, v_num=crps, train_loss_step=0.878, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  39%|███▉      | 19/49 [00:01<00:03,  9.87it/s, v_num=crps, train_loss_step=0.885, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  41%|████      | 20/49 [00:01<00:02, 10.11it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  43%|████▎     | 21/49 [00:02<00:02, 10.34it/s, v_num=crps, train_loss_step=0.881, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  45%|████▍     | 22/49 [00:02<00:02,  9.61it/s, v_num=crps, train_loss_step=0.869, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  47%|████▋     | 23/49 [00:02<00:02,  9.81it/s, v_num=crps, train_loss_step=0.875, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  49%|████▉     | 24/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=0.873, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  51%|█████     | 25/49 [00:02<00:02, 10.12it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  53%|█████▎    | 26/49 [00:02<00:02, 10.30it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  55%|█████▌    | 27/49 [00:02<00:02, 10.36it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  57%|█████▋    | 28/49 [00:02<00:02,  9.72it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  59%|█████▉    | 29/49 [00:02<00:02,  9.87it/s, v_num=crps, train_loss_step=0.888, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  61%|██████    | 30/49 [00:02<00:01, 10.03it/s, v_num=crps, train_loss_step=0.869, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  63%|██████▎   | 31/49 [00:03<00:01, 10.18it/s, v_num=crps, train_loss_step=0.868, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  65%|██████▌   | 32/49 [00:03<00:01, 10.32it/s, v_num=crps, train_loss_step=0.882, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  67%|██████▋   | 33/49 [00:03<00:01,  9.82it/s, v_num=crps, train_loss_step=0.871, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  69%|██████▉   | 34/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  71%|███████▏  | 35/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  73%|███████▎  | 36/49 [00:03<00:01, 10.22it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  76%|███████▌  | 37/49 [00:03<00:01, 10.34it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  78%|███████▊  | 38/49 [00:03<00:01,  9.92it/s, v_num=crps, train_loss_step=0.874, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  80%|███████▉  | 39/49 [00:03<00:00, 10.04it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  82%|████████▏ | 40/49 [00:03<00:00, 10.15it/s, v_num=crps, train_loss_step=0.878, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  84%|████████▎ | 41/49 [00:03<00:00, 10.26it/s, v_num=crps, train_loss_step=0.868, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  86%|████████▌ | 42/49 [00:04<00:00, 10.37it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  88%|████████▊ | 43/49 [00:04<00:00,  9.99it/s, v_num=crps, train_loss_step=0.869, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  90%|████████▉ | 44/49 [00:04<00:00, 10.08it/s, v_num=crps, train_loss_step=0.883, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  92%|█████████▏| 45/49 [00:04<00:00, 10.11it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  94%|█████████▍| 46/49 [00:04<00:00, 10.17it/s, v_num=crps, train_loss_step=0.876, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  96%|█████████▌| 47/49 [00:04<00:00, 10.25it/s, v_num=crps, train_loss_step=0.871, train_loss_epoch=0.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  98%|█████████▊| 48/49 [00:04<00:00, 10.34it/s, v_num=crps, train_loss_step=0.874, train_loss_epoch=0.880]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 8:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.872]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   2%|▏         | 1/49 [00:00<00:03, 12.41it/s, v_num=crps, train_loss_step=0.876, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   4%|▍         | 2/49 [00:00<00:03, 14.42it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   6%|▌         | 3/49 [00:00<00:03, 14.23it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   8%|▊         | 4/49 [00:00<00:02, 15.07it/s, v_num=crps, train_loss_step=0.880, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  10%|█         | 5/49 [00:00<00:02, 15.56it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  12%|█▏        | 6/49 [00:00<00:04, 10.47it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  14%|█▍        | 7/49 [00:00<00:03, 11.16it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  16%|█▋        | 8/49 [00:00<00:03, 11.73it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  18%|█▊        | 9/49 [00:00<00:03, 12.21it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  20%|██        | 10/49 [00:00<00:03, 12.55it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  22%|██▏       | 11/49 [00:01<00:03, 10.45it/s, v_num=crps, train_loss_step=0.879, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  24%|██▍       | 12/49 [00:01<00:03, 10.83it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  27%|██▋       | 13/49 [00:01<00:03, 11.19it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  29%|██▊       | 14/49 [00:01<00:03, 11.51it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  31%|███       | 15/49 [00:01<00:02, 11.81it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  33%|███▎      | 16/49 [00:01<00:03, 10.49it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  35%|███▍      | 17/49 [00:01<00:02, 10.69it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  37%|███▋      | 18/49 [00:01<00:02, 10.91it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  39%|███▉      | 19/49 [00:01<00:02, 11.15it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  41%|████      | 20/49 [00:01<00:02, 11.14it/s, v_num=crps, train_loss_step=0.884, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  43%|████▎     | 21/49 [00:01<00:02, 11.23it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  45%|████▍     | 22/49 [00:02<00:02, 10.36it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  47%|████▋     | 23/49 [00:02<00:02, 10.39it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  49%|████▉     | 24/49 [00:02<00:02, 10.58it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  51%|█████     | 25/49 [00:02<00:02, 10.77it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  53%|█████▎    | 26/49 [00:02<00:02, 10.94it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  55%|█████▌    | 27/49 [00:02<00:02, 10.27it/s, v_num=crps, train_loss_step=0.871, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  57%|█████▋    | 28/49 [00:02<00:02, 10.43it/s, v_num=crps, train_loss_step=0.879, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  59%|█████▉    | 29/49 [00:02<00:01, 10.59it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  61%|██████    | 30/49 [00:02<00:01, 10.74it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  63%|██████▎   | 31/49 [00:02<00:01, 10.88it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  65%|██████▌   | 32/49 [00:03<00:01, 10.32it/s, v_num=crps, train_loss_step=0.868, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  67%|██████▋   | 33/49 [00:03<00:01, 10.45it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  69%|██████▉   | 34/49 [00:03<00:01, 10.59it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  71%|███████▏  | 35/49 [00:03<00:01, 10.72it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  73%|███████▎  | 36/49 [00:03<00:01, 10.84it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  76%|███████▌  | 37/49 [00:03<00:01, 10.96it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  78%|███████▊  | 38/49 [00:03<00:01, 10.48it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  80%|███████▉  | 39/49 [00:03<00:00, 10.60it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  82%|████████▏ | 40/49 [00:03<00:00, 10.72it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  84%|████████▎ | 41/49 [00:03<00:00, 10.82it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  86%|████████▌ | 42/49 [00:03<00:00, 10.87it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  88%|████████▊ | 43/49 [00:04<00:00, 10.45it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  90%|████████▉ | 44/49 [00:04<00:00, 10.51it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  92%|█████████▏| 45/49 [00:04<00:00, 10.61it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  94%|█████████▍| 46/49 [00:04<00:00, 10.71it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  96%|█████████▌| 47/49 [00:04<00:00, 10.81it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.872]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  98%|█████████▊| 48/49 [00:04<00:00, 10.44it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.872]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 9:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.865]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   2%|▏         | 1/49 [00:00<00:03, 13.89it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   4%|▍         | 2/49 [00:00<00:02, 15.77it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   6%|▌         | 3/49 [00:00<00:02, 16.56it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   8%|▊         | 4/49 [00:00<00:02, 17.09it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  10%|█         | 5/49 [00:00<00:04, 10.40it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  12%|█▏        | 6/49 [00:00<00:03, 11.24it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  14%|█▍        | 7/49 [00:00<00:03, 11.95it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  16%|█▋        | 8/49 [00:00<00:03, 12.52it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  18%|█▊        | 9/49 [00:00<00:03, 13.01it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  20%|██        | 10/49 [00:00<00:03, 10.61it/s, v_num=crps, train_loss_step=0.869, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  22%|██▏       | 11/49 [00:00<00:03, 11.06it/s, v_num=crps, train_loss_step=0.848, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  24%|██▍       | 12/49 [00:01<00:03, 11.46it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  27%|██▋       | 13/49 [00:01<00:03, 11.64it/s, v_num=crps, train_loss_step=0.872, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  29%|██▊       | 14/49 [00:01<00:03, 11.60it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  31%|███       | 15/49 [00:01<00:03, 10.27it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  33%|███▎      | 16/49 [00:01<00:03, 10.57it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  35%|███▍      | 17/49 [00:01<00:02, 10.84it/s, v_num=crps, train_loss_step=0.876, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  37%|███▋      | 18/49 [00:01<00:02, 11.08it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  39%|███▉      | 19/49 [00:01<00:02, 11.32it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  41%|████      | 20/49 [00:01<00:02, 11.53it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  43%|████▎     | 21/49 [00:01<00:02, 10.54it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  45%|████▍     | 22/49 [00:02<00:02, 10.74it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  47%|████▋     | 23/49 [00:02<00:02, 10.94it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  49%|████▉     | 24/49 [00:02<00:02, 11.12it/s, v_num=crps, train_loss_step=0.868, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  51%|█████     | 25/49 [00:02<00:02, 11.30it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  53%|█████▎    | 26/49 [00:02<00:02, 10.55it/s, v_num=crps, train_loss_step=0.848, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  55%|█████▌    | 27/49 [00:02<00:02, 10.71it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  57%|█████▋    | 28/49 [00:02<00:01, 10.88it/s, v_num=crps, train_loss_step=0.874, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  59%|█████▉    | 29/49 [00:02<00:01, 11.03it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  61%|██████    | 30/49 [00:02<00:01, 11.16it/s, v_num=crps, train_loss_step=0.869, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  63%|██████▎   | 31/49 [00:02<00:01, 10.45it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  65%|██████▌   | 32/49 [00:03<00:01, 10.60it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  67%|██████▋   | 33/49 [00:03<00:01, 10.74it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  69%|██████▉   | 34/49 [00:03<00:01, 10.87it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  71%|███████▏  | 35/49 [00:03<00:01, 10.98it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  73%|███████▎  | 36/49 [00:03<00:01, 11.11it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  76%|███████▌  | 37/49 [00:03<00:01, 10.59it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  78%|███████▊  | 38/49 [00:03<00:01, 10.70it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  80%|███████▉  | 39/49 [00:03<00:00, 10.82it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  82%|████████▏ | 40/49 [00:03<00:00, 10.93it/s, v_num=crps, train_loss_step=0.858, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  84%|████████▎ | 41/49 [00:03<00:00, 11.03it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  86%|████████▌ | 42/49 [00:03<00:00, 10.57it/s, v_num=crps, train_loss_step=0.858, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  88%|████████▊ | 43/49 [00:04<00:00, 10.68it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  90%|████████▉ | 44/49 [00:04<00:00, 10.78it/s, v_num=crps, train_loss_step=0.848, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  92%|█████████▏| 45/49 [00:04<00:00, 10.88it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  94%|█████████▍| 46/49 [00:04<00:00, 10.97it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  96%|█████████▌| 47/49 [00:04<00:00, 10.57it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.865]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  98%|█████████▊| 48/49 [00:04<00:00, 10.66it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.865]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 10:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.877, train_loss_epoch=0.858]        torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   2%|▏         | 1/49 [00:00<00:03, 13.36it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   4%|▍         | 2/49 [00:00<00:03, 15.01it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   6%|▌         | 3/49 [00:00<00:03, 15.27it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   8%|▊         | 4/49 [00:00<00:05,  8.85it/s, v_num=crps, train_loss_step=0.870, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  10%|█         | 5/49 [00:00<00:04,  9.89it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  12%|█▏        | 6/49 [00:00<00:04, 10.70it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  14%|█▍        | 7/49 [00:00<00:03, 11.36it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  16%|█▋        | 8/49 [00:00<00:03, 11.38it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  18%|█▊        | 9/49 [00:00<00:04,  9.00it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  20%|██        | 10/49 [00:01<00:04,  9.48it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  22%|██▏       | 11/49 [00:01<00:03,  9.88it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  24%|██▍       | 12/49 [00:01<00:03, 10.26it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  27%|██▋       | 13/49 [00:01<00:03, 10.62it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  29%|██▊       | 14/49 [00:01<00:03, 10.95it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  31%|███       | 15/49 [00:01<00:03,  9.82it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  33%|███▎      | 16/49 [00:01<00:03, 10.10it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  35%|███▍      | 17/49 [00:01<00:03, 10.38it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  37%|███▋      | 18/49 [00:01<00:02, 10.64it/s, v_num=crps, train_loss_step=0.863, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  39%|███▉      | 19/49 [00:01<00:02, 10.89it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  41%|████      | 20/49 [00:02<00:02,  9.80it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  43%|████▎     | 21/49 [00:02<00:02,  9.93it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  45%|████▍     | 22/49 [00:02<00:02, 10.15it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  47%|████▋     | 23/49 [00:02<00:02, 10.35it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  49%|████▉     | 24/49 [00:02<00:02, 10.54it/s, v_num=crps, train_loss_step=0.866, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  51%|█████     | 25/49 [00:02<00:02, 10.72it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  53%|█████▎    | 26/49 [00:02<00:02, 10.04it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  55%|█████▌    | 27/49 [00:02<00:02, 10.21it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  57%|█████▋    | 28/49 [00:02<00:02, 10.36it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  59%|█████▉    | 29/49 [00:02<00:01, 10.46it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  61%|██████    | 30/49 [00:02<00:01, 10.55it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  63%|██████▎   | 31/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  65%|██████▌   | 32/49 [00:03<00:01, 10.03it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  67%|██████▋   | 33/49 [00:03<00:01, 10.15it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  69%|██████▉   | 34/49 [00:03<00:01, 10.24it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  71%|███████▏  | 35/49 [00:03<00:01, 10.30it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  73%|███████▎  | 36/49 [00:03<00:01,  9.81it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  76%|███████▌  | 37/49 [00:03<00:01,  9.94it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  78%|███████▊  | 38/49 [00:03<00:01, 10.06it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  80%|███████▉  | 39/49 [00:03<00:00, 10.18it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  82%|████████▏ | 40/49 [00:03<00:00, 10.29it/s, v_num=crps, train_loss_step=0.864, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  84%|████████▎ | 41/49 [00:03<00:00, 10.36it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  86%|████████▌ | 42/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  88%|████████▊ | 43/49 [00:04<00:00, 10.08it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  90%|████████▉ | 44/49 [00:04<00:00, 10.18it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  92%|█████████▏| 45/49 [00:04<00:00, 10.28it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  94%|█████████▍| 46/49 [00:04<00:00, 10.34it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  96%|█████████▌| 47/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.858]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  98%|█████████▊| 48/49 [00:04<00:00, 10.06it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.858]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 11:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.853]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   2%|▏         | 1/49 [00:00<00:04, 10.67it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   4%|▍         | 2/49 [00:00<00:03, 13.30it/s, v_num=crps, train_loss_step=0.848, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   6%|▌         | 3/49 [00:00<00:06,  7.35it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   8%|▊         | 4/49 [00:00<00:05,  8.53it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  10%|█         | 5/49 [00:00<00:04,  9.02it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  12%|█▏        | 6/49 [00:00<00:04,  9.77it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  14%|█▍        | 7/49 [00:00<00:04, 10.23it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  16%|█▋        | 8/49 [00:00<00:03, 10.80it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  18%|█▊        | 9/49 [00:01<00:04,  8.81it/s, v_num=crps, train_loss_step=0.848, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  20%|██        | 10/49 [00:01<00:04,  9.22it/s, v_num=crps, train_loss_step=0.869, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  22%|██▏       | 11/49 [00:01<00:04,  9.40it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  24%|██▍       | 12/49 [00:01<00:03,  9.67it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  27%|██▋       | 13/49 [00:01<00:03, 10.03it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  29%|██▊       | 14/49 [00:01<00:03,  8.95it/s, v_num=crps, train_loss_step=0.865, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  31%|███       | 15/49 [00:01<00:03,  9.26it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  33%|███▎      | 16/49 [00:01<00:03,  9.38it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  35%|███▍      | 17/49 [00:01<00:03,  9.64it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  37%|███▋      | 18/49 [00:01<00:03,  9.71it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  39%|███▉      | 19/49 [00:02<00:03,  8.89it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  41%|████      | 20/49 [00:02<00:03,  9.13it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  43%|████▎     | 21/49 [00:02<00:03,  9.22it/s, v_num=crps, train_loss_step=0.867, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  45%|████▍     | 22/49 [00:02<00:02,  9.29it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  47%|████▋     | 23/49 [00:02<00:02,  9.38it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  49%|████▉     | 24/49 [00:02<00:02,  8.77it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  51%|█████     | 25/49 [00:02<00:02,  8.93it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  53%|█████▎    | 26/49 [00:02<00:02,  9.00it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  55%|█████▌    | 27/49 [00:02<00:02,  9.08it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  57%|█████▋    | 28/49 [00:03<00:02,  9.22it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  59%|█████▉    | 29/49 [00:03<00:02,  9.27it/s, v_num=crps, train_loss_step=0.842, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  61%|██████    | 30/49 [00:03<00:02,  8.85it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  63%|██████▎   | 31/49 [00:03<00:02,  8.99it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  65%|██████▌   | 32/49 [00:03<00:01,  9.07it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  67%|██████▋   | 33/49 [00:03<00:01,  9.14it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  69%|██████▉   | 34/49 [00:03<00:01,  9.28it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  71%|███████▏  | 35/49 [00:03<00:01,  8.84it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  73%|███████▎  | 36/49 [00:04<00:01,  8.93it/s, v_num=crps, train_loss_step=0.862, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  76%|███████▌  | 37/49 [00:04<00:01,  9.06it/s, v_num=crps, train_loss_step=0.860, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  78%|███████▊  | 38/49 [00:04<00:01,  9.12it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  80%|███████▉  | 39/49 [00:04<00:01,  9.16it/s, v_num=crps, train_loss_step=0.858, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  82%|████████▏ | 40/49 [00:04<00:01,  8.81it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  84%|████████▎ | 41/49 [00:04<00:00,  8.89it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  86%|████████▌ | 42/49 [00:04<00:00,  8.97it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  88%|████████▊ | 43/49 [00:04<00:00,  9.06it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  90%|████████▉ | 44/49 [00:04<00:00,  9.13it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  92%|█████████▏| 45/49 [00:04<00:00,  9.22it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  94%|█████████▍| 46/49 [00:05<00:00,  8.91it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  96%|█████████▌| 47/49 [00:05<00:00,  9.01it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.853]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  98%|█████████▊| 48/49 [00:05<00:00,  9.11it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.853]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 12:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.849]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   2%|▏         | 1/49 [00:00<00:04, 10.95it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   4%|▍         | 2/49 [00:00<00:08,  5.31it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   6%|▌         | 3/49 [00:00<00:07,  6.47it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   8%|▊         | 4/49 [00:00<00:05,  7.63it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  10%|█         | 5/49 [00:00<00:05,  8.62it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  12%|█▏        | 6/49 [00:00<00:04,  9.44it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  14%|█▍        | 7/49 [00:00<00:04, 10.11it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  16%|█▋        | 8/49 [00:00<00:04,  8.25it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  18%|█▊        | 9/49 [00:01<00:04,  8.60it/s, v_num=crps, train_loss_step=0.856, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  20%|██        | 10/49 [00:01<00:04,  9.08it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  22%|██▏       | 11/49 [00:01<00:03,  9.52it/s, v_num=crps, train_loss_step=0.842, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  24%|██▍       | 12/49 [00:01<00:03,  9.62it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  27%|██▋       | 13/49 [00:01<00:04,  8.63it/s, v_num=crps, train_loss_step=0.861, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  29%|██▊       | 14/49 [00:01<00:03,  8.97it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  31%|███       | 15/49 [00:01<00:03,  9.29it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  33%|███▎      | 16/49 [00:01<00:03,  9.58it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  35%|███▍      | 17/49 [00:01<00:03,  9.87it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  37%|███▋      | 18/49 [00:01<00:03,  9.07it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  39%|███▉      | 19/49 [00:02<00:03,  9.31it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  41%|████      | 20/49 [00:02<00:03,  9.55it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  43%|████▎     | 21/49 [00:02<00:02,  9.77it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  45%|████▍     | 22/49 [00:02<00:02,  9.98it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  47%|████▋     | 23/49 [00:02<00:02, 10.16it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  49%|████▉     | 24/49 [00:02<00:02,  9.52it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  51%|█████     | 25/49 [00:02<00:02,  9.71it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  53%|█████▎    | 26/49 [00:02<00:02,  9.88it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  55%|█████▌    | 27/49 [00:02<00:02, 10.05it/s, v_num=crps, train_loss_step=0.842, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  57%|█████▋    | 28/49 [00:02<00:02, 10.22it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  59%|█████▉    | 29/49 [00:02<00:02,  9.68it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  61%|██████    | 30/49 [00:03<00:01,  9.83it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  63%|██████▎   | 31/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  65%|██████▌   | 32/49 [00:03<00:01, 10.12it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  67%|██████▋   | 33/49 [00:03<00:01, 10.26it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  69%|██████▉   | 34/49 [00:03<00:01,  9.79it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  71%|███████▏  | 35/49 [00:03<00:01,  9.92it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  73%|███████▎  | 36/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  76%|███████▌  | 37/49 [00:03<00:01, 10.01it/s, v_num=crps, train_loss_step=0.859, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  78%|███████▊  | 38/49 [00:03<00:01, 10.08it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  80%|███████▉  | 39/49 [00:03<00:00, 10.20it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  82%|████████▏ | 40/49 [00:04<00:00,  9.81it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  84%|████████▎ | 41/49 [00:04<00:00,  9.92it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  86%|████████▌ | 42/49 [00:04<00:00, 10.03it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  88%|████████▊ | 43/49 [00:04<00:00, 10.13it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  90%|████████▉ | 44/49 [00:04<00:00, 10.24it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  92%|█████████▏| 45/49 [00:04<00:00,  9.89it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  94%|█████████▍| 46/49 [00:04<00:00,  9.99it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  96%|█████████▌| 47/49 [00:04<00:00, 10.09it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.849]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  98%|█████████▊| 48/49 [00:04<00:00, 10.18it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.849]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 13:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.844]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   2%|▏         | 1/49 [00:00<00:03, 12.13it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   4%|▍         | 2/49 [00:00<00:07,  5.97it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   6%|▌         | 3/49 [00:00<00:05,  7.69it/s, v_num=crps, train_loss_step=0.850, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   8%|▊         | 4/49 [00:00<00:04,  9.02it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  10%|█         | 5/49 [00:00<00:04,  9.96it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  12%|█▏        | 6/49 [00:00<00:04, 10.46it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  14%|█▍        | 7/49 [00:00<00:05,  8.16it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  16%|█▋        | 8/49 [00:00<00:04,  8.44it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  18%|█▊        | 9/49 [00:01<00:04,  8.97it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  20%|██        | 10/49 [00:01<00:04,  9.45it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  22%|██▏       | 11/49 [00:01<00:03,  9.87it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  24%|██▍       | 12/49 [00:01<00:04,  8.75it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  27%|██▋       | 13/49 [00:01<00:03,  9.11it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  29%|██▊       | 14/49 [00:01<00:03,  9.45it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  31%|███       | 15/49 [00:01<00:03,  9.77it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  33%|███▎      | 16/49 [00:01<00:03,  9.94it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  35%|███▍      | 17/49 [00:01<00:03,  9.10it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  37%|███▋      | 18/49 [00:01<00:03,  9.35it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  39%|███▉      | 19/49 [00:01<00:03,  9.60it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  41%|████      | 20/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=0.853, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  43%|████▎     | 21/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  45%|████▍     | 22/49 [00:02<00:02, 10.09it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  47%|████▋     | 23/49 [00:02<00:02,  9.40it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  49%|████▉     | 24/49 [00:02<00:02,  9.58it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  51%|█████     | 25/49 [00:02<00:02,  9.76it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  53%|█████▎    | 26/49 [00:02<00:02,  9.92it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  55%|█████▌    | 27/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=0.848, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  57%|█████▋    | 28/49 [00:02<00:02,  9.42it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  59%|█████▉    | 29/49 [00:03<00:02,  9.56it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  61%|██████    | 30/49 [00:03<00:01,  9.62it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  63%|██████▎   | 31/49 [00:03<00:01,  9.71it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  65%|██████▌   | 32/49 [00:03<00:01,  9.86it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  67%|██████▋   | 33/49 [00:03<00:01,  9.41it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  69%|██████▉   | 34/49 [00:03<00:01,  9.50it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  71%|███████▏  | 35/49 [00:03<00:01,  9.62it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  73%|███████▎  | 36/49 [00:03<00:01,  9.70it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  76%|███████▌  | 37/49 [00:03<00:01,  9.82it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  78%|███████▊  | 38/49 [00:04<00:01,  9.45it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  80%|███████▉  | 39/49 [00:04<00:01,  9.57it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  82%|████████▏ | 40/49 [00:04<00:00,  9.68it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  84%|████████▎ | 41/49 [00:04<00:00,  9.79it/s, v_num=crps, train_loss_step=0.846, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  86%|████████▌ | 42/49 [00:04<00:00,  9.90it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  88%|████████▊ | 43/49 [00:04<00:00, 10.00it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  90%|████████▉ | 44/49 [00:04<00:00,  9.67it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  92%|█████████▏| 45/49 [00:04<00:00,  9.77it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  94%|█████████▍| 46/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  96%|█████████▌| 47/49 [00:04<00:00,  9.90it/s, v_num=crps, train_loss_step=0.857, train_loss_epoch=0.844]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  98%|█████████▊| 48/49 [00:04<00:00,  9.97it/s, v_num=crps, train_loss_step=0.849, train_loss_epoch=0.844]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 14:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.840]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   2%|▏         | 1/49 [00:00<00:03, 13.24it/s, v_num=crps, train_loss_step=0.854, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   4%|▍         | 2/49 [00:00<00:03, 15.33it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   6%|▌         | 3/49 [00:00<00:02, 15.86it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   8%|▊         | 4/49 [00:00<00:02, 16.24it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  10%|█         | 5/49 [00:00<00:02, 16.53it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  12%|█▏        | 6/49 [00:00<00:04,  9.84it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  14%|█▍        | 7/49 [00:00<00:03, 10.53it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  16%|█▋        | 8/49 [00:00<00:03, 11.11it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  18%|█▊        | 9/49 [00:00<00:03, 11.62it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  20%|██        | 10/49 [00:00<00:03, 12.06it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  22%|██▏       | 11/49 [00:01<00:03, 10.14it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  24%|██▍       | 12/49 [00:01<00:03, 10.54it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  27%|██▋       | 13/49 [00:01<00:03, 10.73it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  29%|██▊       | 14/49 [00:01<00:03, 11.05it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  31%|███       | 15/49 [00:01<00:02, 11.34it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  33%|███▎      | 16/49 [00:01<00:03, 10.12it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  35%|███▍      | 17/49 [00:01<00:03, 10.38it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  37%|███▋      | 18/49 [00:01<00:02, 10.64it/s, v_num=crps, train_loss_step=0.852, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  39%|███▉      | 19/49 [00:01<00:02, 10.77it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  41%|████      | 20/49 [00:01<00:02, 10.99it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  43%|████▎     | 21/49 [00:01<00:02, 11.02it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  45%|████▍     | 22/49 [00:02<00:02, 10.20it/s, v_num=crps, train_loss_step=0.846, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  47%|████▋     | 23/49 [00:02<00:02, 10.40it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  49%|████▉     | 24/49 [00:02<00:02, 10.56it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  51%|█████     | 25/49 [00:02<00:02, 10.62it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  53%|█████▎    | 26/49 [00:02<00:02, 10.58it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  55%|█████▌    | 27/49 [00:02<00:02,  9.88it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  57%|█████▋    | 28/49 [00:02<00:02,  9.92it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  59%|█████▉    | 29/49 [00:02<00:02,  9.93it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  61%|██████    | 30/49 [00:02<00:01, 10.08it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  63%|██████▎   | 31/49 [00:03<00:01, 10.22it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  65%|██████▌   | 32/49 [00:03<00:01,  9.67it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  67%|██████▋   | 33/49 [00:03<00:01,  9.72it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  69%|██████▉   | 34/49 [00:03<00:01,  9.79it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  71%|███████▏  | 35/49 [00:03<00:01,  9.82it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  73%|███████▎  | 36/49 [00:03<00:01,  9.94it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  76%|███████▌  | 37/49 [00:03<00:01, 10.07it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  78%|███████▊  | 38/49 [00:03<00:01,  9.67it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  80%|███████▉  | 39/49 [00:03<00:01,  9.79it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  82%|████████▏ | 40/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=0.855, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  84%|████████▎ | 41/49 [00:04<00:00, 10.02it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  86%|████████▌ | 42/49 [00:04<00:00, 10.03it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  88%|████████▊ | 43/49 [00:04<00:00,  9.65it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  90%|████████▉ | 44/49 [00:04<00:00,  9.68it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  92%|█████████▏| 45/49 [00:04<00:00,  9.79it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  94%|█████████▍| 46/49 [00:04<00:00,  9.88it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  96%|█████████▌| 47/49 [00:04<00:00,  9.97it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  98%|█████████▊| 48/49 [00:05<00:00,  9.59it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.840]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 15:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.837]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   2%|▏         | 1/49 [00:00<00:04,  9.69it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   4%|▍         | 2/49 [00:00<00:04, 10.33it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   6%|▌         | 3/49 [00:00<00:04, 11.15it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   8%|▊         | 4/49 [00:00<00:03, 12.37it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  10%|█         | 5/49 [00:00<00:05,  8.65it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  12%|█▏        | 6/49 [00:00<00:04,  9.47it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  14%|█▍        | 7/49 [00:00<00:04, 10.17it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  16%|█▋        | 8/49 [00:00<00:03, 10.55it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  18%|█▊        | 9/49 [00:00<00:03, 11.08it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  20%|██        | 10/49 [00:00<00:03, 11.54it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  22%|██▏       | 11/49 [00:01<00:03,  9.59it/s, v_num=crps, train_loss_step=0.842, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  24%|██▍       | 12/49 [00:01<00:03,  9.89it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  27%|██▋       | 13/49 [00:01<00:03, 10.24it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  29%|██▊       | 14/49 [00:01<00:03, 10.51it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  31%|███       | 15/49 [00:01<00:03, 10.67it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  33%|███▎      | 16/49 [00:01<00:03,  9.55it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  35%|███▍      | 17/49 [00:01<00:03,  9.73it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  37%|███▋      | 18/49 [00:01<00:03,  9.80it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  39%|███▉      | 19/49 [00:01<00:02, 10.04it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  41%|████      | 20/49 [00:01<00:02, 10.24it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  43%|████▎     | 21/49 [00:02<00:02, 10.33it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  45%|████▍     | 22/49 [00:02<00:02,  9.53it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  47%|████▋     | 23/49 [00:02<00:02,  9.66it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  49%|████▉     | 24/49 [00:02<00:02,  9.85it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  51%|█████     | 25/49 [00:02<00:02, 10.03it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  53%|█████▎    | 26/49 [00:02<00:02, 10.19it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  55%|█████▌    | 27/49 [00:02<00:02,  9.56it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  57%|█████▋    | 28/49 [00:02<00:02,  9.72it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  59%|█████▉    | 29/49 [00:02<00:02,  9.88it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  61%|██████    | 30/49 [00:02<00:01, 10.03it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  63%|██████▎   | 31/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  65%|██████▌   | 32/49 [00:03<00:01,  9.49it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  67%|██████▋   | 33/49 [00:03<00:01,  9.63it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  69%|██████▉   | 34/49 [00:03<00:01,  9.76it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  71%|███████▏  | 35/49 [00:03<00:01,  9.89it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  73%|███████▎  | 36/49 [00:03<00:01,  9.88it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  76%|███████▌  | 37/49 [00:03<00:01,  9.90it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  78%|███████▊  | 38/49 [00:03<00:01,  9.52it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  80%|███████▉  | 39/49 [00:04<00:01,  9.64it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  82%|████████▏ | 40/49 [00:04<00:00,  9.76it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  84%|████████▎ | 41/49 [00:04<00:00,  9.87it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  86%|████████▌ | 42/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  88%|████████▊ | 43/49 [00:04<00:00,  9.57it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  90%|████████▉ | 44/49 [00:04<00:00,  9.67it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  92%|█████████▏| 45/49 [00:04<00:00,  9.73it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  94%|█████████▍| 46/49 [00:04<00:00,  9.80it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  96%|█████████▌| 47/49 [00:04<00:00,  9.82it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.837]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  98%|█████████▊| 48/49 [00:05<00:00,  9.48it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.837]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 16:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.834]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   2%|▏         | 1/49 [00:00<00:04, 10.54it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   4%|▍         | 2/49 [00:00<00:03, 11.94it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   6%|▌         | 3/49 [00:00<00:03, 12.29it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   8%|▊         | 4/49 [00:00<00:03, 13.27it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  10%|█         | 5/49 [00:00<00:04,  9.00it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  12%|█▏        | 6/49 [00:00<00:04,  9.82it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  14%|█▍        | 7/49 [00:00<00:04, 10.21it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  16%|█▋        | 8/49 [00:00<00:03, 10.53it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  18%|█▊        | 9/49 [00:00<00:03, 11.00it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  20%|██        | 10/49 [00:01<00:04,  9.09it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  22%|██▏       | 11/49 [00:01<00:03,  9.52it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  24%|██▍       | 12/49 [00:01<00:03,  9.62it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  27%|██▋       | 13/49 [00:01<00:03,  9.99it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  29%|██▊       | 14/49 [00:01<00:03, 10.31it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  31%|███       | 15/49 [00:01<00:03, 10.62it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  33%|███▎      | 16/49 [00:01<00:03,  9.48it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  35%|███▍      | 17/49 [00:01<00:03,  9.75it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  37%|███▋      | 18/49 [00:01<00:03, 10.01it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  39%|███▉      | 19/49 [00:01<00:02, 10.25it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  41%|████      | 20/49 [00:01<00:02, 10.48it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  43%|████▎     | 21/49 [00:02<00:02,  9.71it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  45%|████▍     | 22/49 [00:02<00:02,  9.92it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  47%|████▋     | 23/49 [00:02<00:02, 10.10it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  49%|████▉     | 24/49 [00:02<00:02, 10.30it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  51%|█████     | 25/49 [00:02<00:02, 10.48it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  53%|█████▎    | 26/49 [00:02<00:02,  9.85it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  55%|█████▌    | 27/49 [00:02<00:02, 10.02it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  57%|█████▋    | 28/49 [00:02<00:02, 10.19it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  59%|█████▉    | 29/49 [00:02<00:01, 10.28it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  61%|██████    | 30/49 [00:02<00:01, 10.43it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  63%|██████▎   | 31/49 [00:02<00:01, 10.57it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  65%|██████▌   | 32/49 [00:03<00:01, 10.04it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  67%|██████▋   | 33/49 [00:03<00:01, 10.17it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  69%|██████▉   | 34/49 [00:03<00:01, 10.31it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  71%|███████▏  | 35/49 [00:03<00:01, 10.44it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  73%|███████▎  | 36/49 [00:03<00:01, 10.56it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  76%|███████▌  | 37/49 [00:03<00:01, 10.10it/s, v_num=crps, train_loss_step=0.844, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  78%|███████▊  | 38/49 [00:03<00:01, 10.22it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  80%|███████▉  | 39/49 [00:03<00:00, 10.34it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  82%|████████▏ | 40/49 [00:03<00:00, 10.45it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  84%|████████▎ | 41/49 [00:03<00:00, 10.52it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  86%|████████▌ | 42/49 [00:04<00:00, 10.11it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  88%|████████▊ | 43/49 [00:04<00:00, 10.22it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  90%|████████▉ | 44/49 [00:04<00:00, 10.32it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  92%|█████████▏| 45/49 [00:04<00:00, 10.42it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  94%|█████████▍| 46/49 [00:04<00:00, 10.52it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  96%|█████████▌| 47/49 [00:04<00:00, 10.61it/s, v_num=crps, train_loss_step=0.843, train_loss_epoch=0.834]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  98%|█████████▊| 48/49 [00:04<00:00, 10.25it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.834]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 17:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.831]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   2%|▏         | 1/49 [00:00<00:03, 13.24it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   4%|▍         | 2/49 [00:00<00:03, 14.91it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   6%|▌         | 3/49 [00:00<00:03, 13.62it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   8%|▊         | 4/49 [00:00<00:05,  8.28it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  10%|█         | 5/49 [00:00<00:04,  9.28it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  12%|█▏        | 6/49 [00:00<00:04, 10.11it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  14%|█▍        | 7/49 [00:00<00:03, 10.78it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  16%|█▋        | 8/49 [00:00<00:03, 11.07it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  18%|█▊        | 9/49 [00:00<00:03, 11.54it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  20%|██        | 10/49 [00:01<00:04,  9.42it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  22%|██▏       | 11/49 [00:01<00:03,  9.81it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  24%|██▍       | 12/49 [00:01<00:03, 10.03it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  27%|██▋       | 13/49 [00:01<00:03, 10.36it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  29%|██▊       | 14/49 [00:01<00:03, 10.66it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  31%|███       | 15/49 [00:01<00:03,  9.49it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  33%|███▎      | 16/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  35%|███▍      | 17/49 [00:01<00:03,  9.93it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  37%|███▋      | 18/49 [00:01<00:03, 10.07it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  39%|███▉      | 19/49 [00:01<00:02, 10.24it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  41%|████      | 20/49 [00:02<00:03,  9.41it/s, v_num=crps, train_loss_step=0.851, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  43%|████▎     | 21/49 [00:02<00:02,  9.56it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  45%|████▍     | 22/49 [00:02<00:02,  9.76it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  47%|████▋     | 23/49 [00:02<00:02,  9.87it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  49%|████▉     | 24/49 [00:02<00:02,  9.99it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  51%|█████     | 25/49 [00:02<00:02, 10.17it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  53%|█████▎    | 26/49 [00:02<00:02,  9.46it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  55%|█████▌    | 27/49 [00:02<00:02,  9.59it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  57%|█████▋    | 28/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  59%|█████▉    | 29/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  61%|██████    | 30/49 [00:03<00:01,  9.90it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  63%|██████▎   | 31/49 [00:03<00:01,  9.41it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  65%|██████▌   | 32/49 [00:03<00:01,  9.55it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  67%|██████▋   | 33/49 [00:03<00:01,  9.68it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  69%|██████▉   | 34/49 [00:03<00:01,  9.71it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  71%|███████▏  | 35/49 [00:03<00:01,  9.79it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  73%|███████▎  | 36/49 [00:03<00:01,  9.33it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  76%|███████▌  | 37/49 [00:03<00:01,  9.45it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  78%|███████▊  | 38/49 [00:03<00:01,  9.57it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  80%|███████▉  | 39/49 [00:04<00:01,  9.69it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  82%|████████▏ | 40/49 [00:04<00:00,  9.80it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  84%|████████▎ | 41/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  86%|████████▌ | 42/49 [00:04<00:00,  9.50it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  88%|████████▊ | 43/49 [00:04<00:00,  9.59it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  90%|████████▉ | 44/49 [00:04<00:00,  9.69it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  92%|█████████▏| 45/49 [00:04<00:00,  9.79it/s, v_num=crps, train_loss_step=0.847, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  94%|█████████▍| 46/49 [00:04<00:00,  9.89it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  96%|█████████▌| 47/49 [00:04<00:00,  9.58it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.831]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  98%|█████████▊| 48/49 [00:04<00:00,  9.67it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.831]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 18:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   2%|▏         | 1/49 [00:00<00:04, 11.12it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   4%|▍         | 2/49 [00:00<00:03, 12.83it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   6%|▌         | 3/49 [00:00<00:03, 12.12it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   8%|▊         | 4/49 [00:00<00:05,  7.93it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  10%|█         | 5/49 [00:00<00:04,  8.92it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  12%|█▏        | 6/49 [00:00<00:04,  9.76it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  14%|█▍        | 7/49 [00:00<00:04, 10.45it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  16%|█▋        | 8/49 [00:00<00:03, 11.05it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  18%|█▊        | 9/49 [00:00<00:04,  9.16it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  20%|██        | 10/49 [00:01<00:04,  9.40it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  22%|██▏       | 11/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  24%|██▍       | 12/49 [00:01<00:03, 10.07it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  27%|██▋       | 13/49 [00:01<00:03, 10.29it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  29%|██▊       | 14/49 [00:01<00:03, 10.36it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  31%|███       | 15/49 [00:01<00:03,  9.35it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  33%|███▎      | 16/49 [00:01<00:03,  9.64it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  35%|███▍      | 17/49 [00:01<00:03,  9.91it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  37%|███▋      | 18/49 [00:01<00:03, 10.12it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  39%|███▉      | 19/49 [00:01<00:02, 10.36it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  41%|████      | 20/49 [00:02<00:03,  9.57it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  43%|████▎     | 21/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  45%|████▍     | 22/49 [00:02<00:02,  9.88it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  47%|████▋     | 23/49 [00:02<00:02,  9.97it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  49%|████▉     | 24/49 [00:02<00:02, 10.16it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  51%|█████     | 25/49 [00:02<00:02,  9.52it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  53%|█████▎    | 26/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  55%|█████▌    | 27/49 [00:02<00:02,  9.86it/s, v_num=crps, train_loss_step=0.840, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  57%|█████▋    | 28/49 [00:02<00:02,  9.96it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  59%|█████▉    | 29/49 [00:02<00:01, 10.04it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  61%|██████    | 30/49 [00:03<00:02,  9.40it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  63%|██████▎   | 31/49 [00:03<00:01,  9.54it/s, v_num=crps, train_loss_step=0.839, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  65%|██████▌   | 32/49 [00:03<00:01,  9.64it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  67%|██████▋   | 33/49 [00:03<00:01,  9.70it/s, v_num=crps, train_loss_step=0.841, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  69%|██████▉   | 34/49 [00:03<00:01,  9.78it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  71%|███████▏  | 35/49 [00:03<00:01,  9.78it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  73%|███████▎  | 36/49 [00:03<00:01,  9.34it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  76%|███████▌  | 37/49 [00:03<00:01,  9.47it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  78%|███████▊  | 38/49 [00:03<00:01,  9.59it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  80%|███████▉  | 39/49 [00:04<00:01,  9.71it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  82%|████████▏ | 40/49 [00:04<00:00,  9.82it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  84%|████████▎ | 41/49 [00:04<00:00,  9.46it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  86%|████████▌ | 42/49 [00:04<00:00,  9.51it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  88%|████████▊ | 43/49 [00:04<00:00,  9.58it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  90%|████████▉ | 44/49 [00:04<00:00,  9.69it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  92%|█████████▏| 45/49 [00:04<00:00,  9.78it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  94%|█████████▍| 46/49 [00:04<00:00,  9.88it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  96%|█████████▌| 47/49 [00:04<00:00,  9.58it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.828]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  98%|█████████▊| 48/49 [00:04<00:00,  9.67it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.828]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 19:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.825]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   2%|▏         | 1/49 [00:00<00:04, 10.90it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   4%|▍         | 2/49 [00:00<00:04, 10.83it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   6%|▌         | 3/49 [00:00<00:06,  6.77it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   8%|▊         | 4/49 [00:00<00:05,  8.01it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  10%|█         | 5/49 [00:00<00:04,  9.03it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  12%|█▏        | 6/49 [00:00<00:04,  9.84it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  14%|█▍        | 7/49 [00:00<00:03, 10.53it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  16%|█▋        | 8/49 [00:00<00:04,  8.38it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  18%|█▊        | 9/49 [00:01<00:04,  8.91it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  20%|██        | 10/49 [00:01<00:04,  9.39it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  22%|██▏       | 11/49 [00:01<00:03,  9.83it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  24%|██▍       | 12/49 [00:01<00:03, 10.08it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  27%|██▋       | 13/49 [00:01<00:03, 10.43it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  29%|██▊       | 14/49 [00:01<00:03,  9.33it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  31%|███       | 15/49 [00:01<00:03,  9.63it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  33%|███▎      | 16/49 [00:01<00:03,  9.92it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  35%|███▍      | 17/49 [00:01<00:03, 10.18it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  37%|███▋      | 18/49 [00:01<00:02, 10.43it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  39%|███▉      | 19/49 [00:01<00:03,  9.59it/s, v_num=crps, train_loss_step=0.842, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  41%|████      | 20/49 [00:02<00:02,  9.82it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  43%|████▎     | 21/49 [00:02<00:02, 10.02it/s, v_num=crps, train_loss_step=0.838, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  45%|████▍     | 22/49 [00:02<00:02, 10.19it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  47%|████▋     | 23/49 [00:02<00:02, 10.31it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  49%|████▉     | 24/49 [00:02<00:02,  9.61it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  51%|█████     | 25/49 [00:02<00:02,  9.71it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  53%|█████▎    | 26/49 [00:02<00:02,  9.89it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  55%|█████▌    | 27/49 [00:02<00:02, 10.06it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  57%|█████▋    | 28/49 [00:02<00:02, 10.22it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  59%|█████▉    | 29/49 [00:02<00:01, 10.38it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  61%|██████    | 30/49 [00:03<00:01,  9.85it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  63%|██████▎   | 31/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  65%|██████▌   | 32/49 [00:03<00:01, 10.13it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  67%|██████▋   | 33/49 [00:03<00:01, 10.27it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  69%|██████▉   | 34/49 [00:03<00:01, 10.39it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  71%|███████▏  | 35/49 [00:03<00:01,  9.93it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  73%|███████▎  | 36/49 [00:03<00:01, 10.06it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  76%|███████▌  | 37/49 [00:03<00:01, 10.18it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  78%|███████▊  | 38/49 [00:03<00:01, 10.30it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  80%|███████▉  | 39/49 [00:03<00:00, 10.42it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  82%|████████▏ | 40/49 [00:03<00:00, 10.01it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  84%|████████▎ | 41/49 [00:04<00:00, 10.12it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  86%|████████▌ | 42/49 [00:04<00:00, 10.23it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  88%|████████▊ | 43/49 [00:04<00:00, 10.33it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  90%|████████▉ | 44/49 [00:04<00:00, 10.44it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  92%|█████████▏| 45/49 [00:04<00:00, 10.07it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  94%|█████████▍| 46/49 [00:04<00:00, 10.16it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  96%|█████████▌| 47/49 [00:04<00:00, 10.24it/s, v_num=crps, train_loss_step=0.835, train_loss_epoch=0.825]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  98%|█████████▊| 48/49 [00:04<00:00, 10.33it/s, v_num=crps, train_loss_step=0.832, train_loss_epoch=0.825]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 20:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.823]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:   2%|▏         | 1/49 [00:00<00:05,  9.19it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:   4%|▍         | 2/49 [00:00<00:09,  5.12it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:   6%|▌         | 3/49 [00:00<00:07,  6.44it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:   8%|▊         | 4/49 [00:00<00:06,  7.37it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  10%|█         | 5/49 [00:00<00:05,  7.97it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  12%|█▏        | 6/49 [00:00<00:05,  8.33it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  14%|█▍        | 7/49 [00:00<00:04,  9.00it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  16%|█▋        | 8/49 [00:01<00:05,  7.75it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  18%|█▊        | 9/49 [00:01<00:04,  8.28it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  20%|██        | 10/49 [00:01<00:04,  8.76it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  22%|██▏       | 11/49 [00:01<00:04,  9.19it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  24%|██▍       | 12/49 [00:01<00:03,  9.59it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  27%|██▋       | 13/49 [00:01<00:04,  8.61it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  29%|██▊       | 14/49 [00:01<00:03,  8.95it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  31%|███       | 15/49 [00:01<00:03,  9.26it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  33%|███▎      | 16/49 [00:01<00:03,  9.55it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  35%|███▍      | 17/49 [00:01<00:03,  9.82it/s, v_num=crps, train_loss_step=0.845, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  37%|███▋      | 18/49 [00:01<00:03, 10.05it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  39%|███▉      | 19/49 [00:02<00:03,  9.17it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  41%|████      | 20/49 [00:02<00:03,  9.30it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  43%|████▎     | 21/49 [00:02<00:02,  9.42it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  45%|████▍     | 22/49 [00:02<00:02,  9.63it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  47%|████▋     | 23/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  49%|████▉     | 24/49 [00:02<00:02,  9.24it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  51%|█████     | 25/49 [00:02<00:02,  9.43it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  53%|█████▎    | 26/49 [00:02<00:02,  9.60it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  55%|█████▌    | 27/49 [00:02<00:02,  9.77it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  57%|█████▋    | 28/49 [00:02<00:02,  9.93it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  59%|█████▉    | 29/49 [00:03<00:02,  9.43it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  61%|██████    | 30/49 [00:03<00:01,  9.58it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  63%|██████▎   | 31/49 [00:03<00:01,  9.72it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  65%|██████▌   | 32/49 [00:03<00:01,  9.75it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  67%|██████▋   | 33/49 [00:03<00:01,  9.89it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  69%|██████▉   | 34/49 [00:03<00:01,  9.91it/s, v_num=crps, train_loss_step=0.831, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  71%|███████▏  | 35/49 [00:03<00:01,  9.51it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  73%|███████▎  | 36/49 [00:03<00:01,  9.64it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  76%|███████▌  | 37/49 [00:03<00:01,  9.76it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  78%|███████▊  | 38/49 [00:03<00:01,  9.88it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  80%|███████▉  | 39/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  82%|████████▏ | 40/49 [00:04<00:00,  9.62it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  84%|████████▎ | 41/49 [00:04<00:00,  9.73it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  86%|████████▌ | 42/49 [00:04<00:00,  9.83it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  88%|████████▊ | 43/49 [00:04<00:00,  9.89it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  90%|████████▉ | 44/49 [00:04<00:00,  9.93it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  92%|█████████▏| 45/49 [00:04<00:00,  9.59it/s, v_num=crps, train_loss_step=0.834, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  94%|█████████▍| 46/49 [00:04<00:00,  9.68it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  96%|█████████▌| 47/49 [00:04<00:00,  9.69it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.823]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 20:  98%|█████████▊| 48/49 [00:04<00:00,  9.70it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.823]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 21:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.820]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:   2%|▏         | 1/49 [00:00<00:04, 10.42it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:   4%|▍         | 2/49 [00:00<00:08,  5.43it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:   6%|▌         | 3/49 [00:00<00:06,  7.08it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:   8%|▊         | 4/49 [00:00<00:05,  8.37it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  10%|█         | 5/49 [00:00<00:04,  9.36it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  12%|█▏        | 6/49 [00:00<00:04, 10.19it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  14%|█▍        | 7/49 [00:00<00:05,  8.27it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  16%|█▋        | 8/49 [00:00<00:04,  8.88it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  18%|█▊        | 9/49 [00:00<00:04,  9.41it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  20%|██        | 10/49 [00:01<00:03,  9.86it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  22%|██▏       | 11/49 [00:01<00:03, 10.16it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  24%|██▍       | 12/49 [00:01<00:03, 10.57it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  27%|██▋       | 13/49 [00:01<00:03,  9.35it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  29%|██▊       | 14/49 [00:01<00:03,  9.56it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  31%|███       | 15/49 [00:01<00:03,  9.71it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  33%|███▎      | 16/49 [00:01<00:03,  9.89it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  35%|███▍      | 17/49 [00:01<00:03, 10.14it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  37%|███▋      | 18/49 [00:01<00:03,  9.32it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  39%|███▉      | 19/49 [00:02<00:03,  9.45it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  41%|████      | 20/49 [00:02<00:03,  9.66it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  43%|████▎     | 21/49 [00:02<00:02,  9.84it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  45%|████▍     | 22/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  47%|████▋     | 23/49 [00:02<00:02,  9.27it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  49%|████▉     | 24/49 [00:02<00:02,  9.40it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  51%|█████     | 25/49 [00:02<00:02,  9.58it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  53%|█████▎    | 26/49 [00:02<00:02,  9.76it/s, v_num=crps, train_loss_step=0.802, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  55%|█████▌    | 27/49 [00:02<00:02,  9.92it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  57%|█████▋    | 28/49 [00:02<00:02, 10.08it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  59%|█████▉    | 29/49 [00:03<00:02,  9.57it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  61%|██████    | 30/49 [00:03<00:01,  9.66it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  63%|██████▎   | 31/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  65%|██████▌   | 32/49 [00:03<00:01,  9.93it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  67%|██████▋   | 33/49 [00:03<00:01, 10.06it/s, v_num=crps, train_loss_step=0.836, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  69%|██████▉   | 34/49 [00:03<00:01,  9.57it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  71%|███████▏  | 35/49 [00:03<00:01,  9.67it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  73%|███████▎  | 36/49 [00:03<00:01,  9.73it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  76%|███████▌  | 37/49 [00:03<00:01,  9.84it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  78%|███████▊  | 38/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  80%|███████▉  | 39/49 [00:04<00:01,  9.57it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  82%|████████▏ | 40/49 [00:04<00:00,  9.65it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  84%|████████▎ | 41/49 [00:04<00:00,  9.76it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  86%|████████▌ | 42/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  88%|████████▊ | 43/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  90%|████████▉ | 44/49 [00:04<00:00, 10.07it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  92%|█████████▏| 45/49 [00:04<00:00,  9.74it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  94%|█████████▍| 46/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  96%|█████████▌| 47/49 [00:04<00:00,  9.93it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.820]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 21:  98%|█████████▊| 48/49 [00:04<00:00, 10.02it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.820]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 22:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.837, train_loss_epoch=0.817]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:   2%|▏         | 1/49 [00:00<00:13,  3.66it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:   4%|▍         | 2/49 [00:00<00:08,  5.52it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:   6%|▌         | 3/49 [00:00<00:06,  6.91it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:   8%|▊         | 4/49 [00:00<00:05,  8.16it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  10%|█         | 5/49 [00:00<00:05,  8.63it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  12%|█▏        | 6/49 [00:00<00:04,  9.46it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  14%|█▍        | 7/49 [00:00<00:05,  7.89it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  16%|█▋        | 8/49 [00:00<00:04,  8.48it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  18%|█▊        | 9/49 [00:00<00:04,  9.02it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  20%|██        | 10/49 [00:01<00:04,  9.48it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  22%|██▏       | 11/49 [00:01<00:03,  9.90it/s, v_num=crps, train_loss_step=0.829, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  24%|██▍       | 12/49 [00:01<00:04,  8.68it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  27%|██▋       | 13/49 [00:01<00:03,  9.04it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  29%|██▊       | 14/49 [00:01<00:03,  9.38it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  31%|███       | 15/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  33%|███▎      | 16/49 [00:01<00:03,  9.97it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  35%|███▍      | 17/49 [00:01<00:03,  9.12it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  37%|███▋      | 18/49 [00:01<00:03,  9.30it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  39%|███▉      | 19/49 [00:01<00:03,  9.54it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  41%|████      | 20/49 [00:02<00:02,  9.78it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  43%|████▎     | 21/49 [00:02<00:02,  9.99it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  45%|████▍     | 22/49 [00:02<00:02, 10.20it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  47%|████▋     | 23/49 [00:02<00:02,  9.54it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  49%|████▉     | 24/49 [00:02<00:02,  9.73it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  51%|█████     | 25/49 [00:02<00:02,  9.91it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  53%|█████▎    | 26/49 [00:02<00:02, 10.09it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  55%|█████▌    | 27/49 [00:02<00:02, 10.26it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  57%|█████▋    | 28/49 [00:02<00:02,  9.68it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  59%|█████▉    | 29/49 [00:02<00:02,  9.84it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  61%|██████    | 30/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  63%|██████▎   | 31/49 [00:03<00:01, 10.08it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  65%|██████▌   | 32/49 [00:03<00:01, 10.21it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  67%|██████▋   | 33/49 [00:03<00:01,  9.61it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  69%|██████▉   | 34/49 [00:03<00:01,  9.74it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  71%|███████▏  | 35/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  73%|███████▎  | 36/49 [00:03<00:01, 10.00it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  76%|███████▌  | 37/49 [00:03<00:01, 10.12it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  78%|███████▊  | 38/49 [00:03<00:01, 10.18it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  80%|███████▉  | 39/49 [00:04<00:01,  9.74it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  82%|████████▏ | 40/49 [00:04<00:00,  9.85it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  84%|████████▎ | 41/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  86%|████████▌ | 42/49 [00:04<00:00, 10.07it/s, v_num=crps, train_loss_step=0.811, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  88%|████████▊ | 43/49 [00:04<00:00, 10.17it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  90%|████████▉ | 44/49 [00:04<00:00,  9.81it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  92%|█████████▏| 45/49 [00:04<00:00,  9.92it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  94%|█████████▍| 46/49 [00:04<00:00, 10.00it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  96%|█████████▌| 47/49 [00:04<00:00, 10.06it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.817]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 22:  98%|█████████▊| 48/49 [00:04<00:00, 10.14it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.817]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 23:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.815]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:   2%|▏         | 1/49 [00:00<00:14,  3.29it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:   4%|▍         | 2/49 [00:00<00:08,  5.56it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:   6%|▌         | 3/49 [00:00<00:06,  7.25it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:   8%|▊         | 4/49 [00:00<00:05,  8.53it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  10%|█         | 5/49 [00:00<00:04,  9.55it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  12%|█▏        | 6/49 [00:00<00:05,  7.71it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  14%|█▍        | 7/49 [00:00<00:05,  8.39it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  16%|█▋        | 8/49 [00:00<00:04,  8.98it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  18%|█▊        | 9/49 [00:00<00:04,  9.51it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  20%|██        | 10/49 [00:01<00:03,  9.98it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  22%|██▏       | 11/49 [00:01<00:04,  8.75it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  24%|██▍       | 12/49 [00:01<00:04,  9.14it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  27%|██▋       | 13/49 [00:01<00:03,  9.26it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  29%|██▊       | 14/49 [00:01<00:03,  9.43it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  31%|███       | 15/49 [00:01<00:03,  9.71it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  33%|███▎      | 16/49 [00:01<00:03,  8.88it/s, v_num=crps, train_loss_step=0.827, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  35%|███▍      | 17/49 [00:01<00:03,  9.16it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  37%|███▋      | 18/49 [00:01<00:03,  9.41it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  39%|███▉      | 19/49 [00:02<00:03,  9.47it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  41%|████      | 20/49 [00:02<00:03,  9.65it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  43%|████▎     | 21/49 [00:02<00:02,  9.76it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  45%|████▍     | 22/49 [00:02<00:02,  9.09it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  47%|████▋     | 23/49 [00:02<00:02,  9.27it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  49%|████▉     | 24/49 [00:02<00:02,  9.40it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  51%|█████     | 25/49 [00:02<00:02,  9.57it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  53%|█████▎    | 26/49 [00:02<00:02,  9.74it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  55%|█████▌    | 27/49 [00:02<00:02,  9.18it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  57%|█████▋    | 28/49 [00:03<00:02,  9.30it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  59%|█████▉    | 29/49 [00:03<00:02,  9.40it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  61%|██████    | 30/49 [00:03<00:02,  9.48it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  63%|██████▎   | 31/49 [00:03<00:01,  9.63it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  65%|██████▌   | 32/49 [00:03<00:01,  9.21it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  67%|██████▋   | 33/49 [00:03<00:01,  9.36it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  69%|██████▉   | 34/49 [00:03<00:01,  9.49it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  71%|███████▏  | 35/49 [00:03<00:01,  9.51it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  73%|███████▎  | 36/49 [00:03<00:01,  9.63it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  76%|███████▌  | 37/49 [00:03<00:01,  9.26it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  78%|███████▊  | 38/49 [00:04<00:01,  9.37it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  80%|███████▉  | 39/49 [00:04<00:01,  9.49it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  82%|████████▏ | 40/49 [00:04<00:00,  9.51it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  84%|████████▎ | 41/49 [00:04<00:00,  9.59it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  86%|████████▌ | 42/49 [00:04<00:00,  9.69it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  88%|████████▊ | 43/49 [00:04<00:00,  9.34it/s, v_num=crps, train_loss_step=0.833, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  90%|████████▉ | 44/49 [00:04<00:00,  9.44it/s, v_num=crps, train_loss_step=0.802, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  92%|█████████▏| 45/49 [00:04<00:00,  9.50it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  94%|█████████▍| 46/49 [00:04<00:00,  9.57it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  96%|█████████▌| 47/49 [00:04<00:00,  9.66it/s, v_num=crps, train_loss_step=0.796, train_loss_epoch=0.815]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 23:  98%|█████████▊| 48/49 [00:05<00:00,  9.34it/s, v_num=crps, train_loss_step=0.802, train_loss_epoch=0.815]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 24:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.813]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:   2%|▏         | 1/49 [00:00<00:03, 12.42it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:   4%|▍         | 2/49 [00:00<00:03, 14.53it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:   6%|▌         | 3/49 [00:00<00:03, 14.13it/s, v_num=crps, train_loss_step=0.825, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:   8%|▊         | 4/49 [00:00<00:03, 14.69it/s, v_num=crps, train_loss_step=0.828, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  10%|█         | 5/49 [00:00<00:04,  8.90it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  12%|█▏        | 6/49 [00:00<00:04,  9.71it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  14%|█▍        | 7/49 [00:00<00:04, 10.41it/s, v_num=crps, train_loss_step=0.794, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  16%|█▋        | 8/49 [00:00<00:03, 11.00it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  18%|█▊        | 9/49 [00:00<00:03, 11.49it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  20%|██        | 10/49 [00:01<00:04,  9.34it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  22%|██▏       | 11/49 [00:01<00:03,  9.78it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  24%|██▍       | 12/49 [00:01<00:03, 10.16it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  27%|██▋       | 13/49 [00:01<00:03, 10.52it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  29%|██▊       | 14/49 [00:01<00:03, 10.84it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  31%|███       | 15/49 [00:01<00:03, 11.14it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  33%|███▎      | 16/49 [00:01<00:03, 10.00it/s, v_num=crps, train_loss_step=0.795, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  35%|███▍      | 17/49 [00:01<00:03, 10.19it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  37%|███▋      | 18/49 [00:01<00:02, 10.44it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  39%|███▉      | 19/49 [00:01<00:02, 10.68it/s, v_num=crps, train_loss_step=0.823, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  41%|████      | 20/49 [00:01<00:02, 10.91it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  43%|████▎     | 21/49 [00:02<00:02, 10.04it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  45%|████▍     | 22/49 [00:02<00:02, 10.25it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  47%|████▋     | 23/49 [00:02<00:02, 10.45it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  49%|████▉     | 24/49 [00:02<00:02, 10.62it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  51%|█████     | 25/49 [00:02<00:02, 10.80it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  53%|█████▎    | 26/49 [00:02<00:02, 10.09it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  55%|█████▌    | 27/49 [00:02<00:02, 10.25it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  57%|█████▋    | 28/49 [00:02<00:02, 10.42it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  59%|█████▉    | 29/49 [00:02<00:01, 10.57it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  61%|██████    | 30/49 [00:02<00:01, 10.72it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  63%|██████▎   | 31/49 [00:02<00:01, 10.84it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  65%|██████▌   | 32/49 [00:03<00:01, 10.21it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  67%|██████▋   | 33/49 [00:03<00:01, 10.28it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  69%|██████▉   | 34/49 [00:03<00:01, 10.41it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  71%|███████▏  | 35/49 [00:03<00:01, 10.54it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  73%|███████▎  | 36/49 [00:03<00:01, 10.66it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  76%|███████▌  | 37/49 [00:03<00:01, 10.19it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  78%|███████▊  | 38/49 [00:03<00:01, 10.31it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  80%|███████▉  | 39/49 [00:03<00:00, 10.43it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  82%|████████▏ | 40/49 [00:03<00:00, 10.54it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  84%|████████▎ | 41/49 [00:03<00:00, 10.65it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  86%|████████▌ | 42/49 [00:04<00:00, 10.18it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  88%|████████▊ | 43/49 [00:04<00:00, 10.21it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  90%|████████▉ | 44/49 [00:04<00:00, 10.27it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  92%|█████████▏| 45/49 [00:04<00:00, 10.35it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  94%|█████████▍| 46/49 [00:04<00:00, 10.40it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  96%|█████████▌| 47/49 [00:04<00:00, 10.04it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.813]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 24:  98%|█████████▊| 48/49 [00:04<00:00, 10.14it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.813]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 25:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.815, train_loss_epoch=0.811]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:   2%|▏         | 1/49 [00:00<00:06,  7.87it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:   4%|▍         | 2/49 [00:00<00:04,  9.97it/s, v_num=crps, train_loss_step=0.796, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:   6%|▌         | 3/49 [00:00<00:03, 11.68it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:   8%|▊         | 4/49 [00:00<00:05,  7.85it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  10%|█         | 5/49 [00:00<00:04,  8.83it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  12%|█▏        | 6/49 [00:00<00:04,  9.46it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  14%|█▍        | 7/49 [00:00<00:04,  9.64it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  16%|█▋        | 8/49 [00:00<00:04, 10.23it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  18%|█▊        | 9/49 [00:01<00:04,  8.68it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  20%|██        | 10/49 [00:01<00:04,  9.01it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  22%|██▏       | 11/49 [00:01<00:04,  9.29it/s, v_num=crps, train_loss_step=0.797, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  24%|██▍       | 12/49 [00:01<00:03,  9.70it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  27%|██▋       | 13/49 [00:01<00:03, 10.05it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  29%|██▊       | 14/49 [00:01<00:03, 10.38it/s, v_num=crps, train_loss_step=0.821, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  31%|███       | 15/49 [00:01<00:03,  9.25it/s, v_num=crps, train_loss_step=0.830, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  33%|███▎      | 16/49 [00:01<00:03,  9.53it/s, v_num=crps, train_loss_step=0.802, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  35%|███▍      | 17/49 [00:01<00:03,  9.80it/s, v_num=crps, train_loss_step=0.800, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  37%|███▋      | 18/49 [00:01<00:03, 10.05it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  39%|███▉      | 19/49 [00:01<00:02, 10.30it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  41%|████      | 20/49 [00:02<00:03,  9.52it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  43%|████▎     | 21/49 [00:02<00:02,  9.74it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  45%|████▍     | 22/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  47%|████▋     | 23/49 [00:02<00:02, 10.02it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  49%|████▉     | 24/49 [00:02<00:02, 10.10it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  51%|█████     | 25/49 [00:02<00:02, 10.28it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  53%|█████▎    | 26/49 [00:02<00:02,  9.68it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  55%|█████▌    | 27/49 [00:02<00:02,  9.85it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  57%|█████▋    | 28/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  59%|█████▉    | 29/49 [00:02<00:01, 10.16it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  61%|██████    | 30/49 [00:02<00:01, 10.30it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  63%|██████▎   | 31/49 [00:03<00:01,  9.75it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  65%|██████▌   | 32/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  67%|██████▋   | 33/49 [00:03<00:01,  9.92it/s, v_num=crps, train_loss_step=0.822, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  69%|██████▉   | 34/49 [00:03<00:01, 10.04it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  71%|███████▏  | 35/49 [00:03<00:01, 10.11it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  73%|███████▎  | 36/49 [00:03<00:01,  9.64it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  76%|███████▌  | 37/49 [00:03<00:01,  9.76it/s, v_num=crps, train_loss_step=0.814, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  78%|███████▊  | 38/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  80%|███████▉  | 39/49 [00:03<00:01,  9.92it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  82%|████████▏ | 40/49 [00:03<00:00, 10.03it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  84%|████████▎ | 41/49 [00:04<00:00,  9.65it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  86%|████████▌ | 42/49 [00:04<00:00,  9.76it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  88%|████████▊ | 43/49 [00:04<00:00,  9.86it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  90%|████████▉ | 44/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  92%|█████████▏| 45/49 [00:04<00:00, 10.06it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  94%|█████████▍| 46/49 [00:04<00:00, 10.16it/s, v_num=crps, train_loss_step=0.826, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  96%|█████████▌| 47/49 [00:04<00:00,  9.77it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.811]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 25:  98%|█████████▊| 48/49 [00:04<00:00,  9.87it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.811]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 26:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=0.824, train_loss_epoch=0.809]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:   2%|▏         | 1/49 [00:00<00:03, 12.90it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:   4%|▍         | 2/49 [00:00<00:03, 14.97it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:   6%|▌         | 3/49 [00:00<00:06,  7.00it/s, v_num=crps, train_loss_step=0.802, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:   8%|▊         | 4/49 [00:00<00:05,  8.26it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  10%|█         | 5/49 [00:00<00:04,  9.27it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  12%|█▏        | 6/49 [00:00<00:04, 10.08it/s, v_num=crps, train_loss_step=0.800, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  14%|█▍        | 7/49 [00:00<00:04, 10.47it/s, v_num=crps, train_loss_step=0.806, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  16%|█▋        | 8/49 [00:00<00:03, 11.01it/s, v_num=crps, train_loss_step=0.787, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  18%|█▊        | 9/49 [00:00<00:04,  9.04it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  20%|██        | 10/49 [00:01<00:04,  9.35it/s, v_num=crps, train_loss_step=0.797, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  22%|██▏       | 11/49 [00:01<00:03,  9.78it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  24%|██▍       | 12/49 [00:01<00:03, 10.17it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  27%|██▋       | 13/49 [00:01<00:03, 10.52it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  29%|██▊       | 14/49 [00:01<00:03,  9.03it/s, v_num=crps, train_loss_step=0.804, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  31%|███       | 15/49 [00:01<00:03,  9.32it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  33%|███▎      | 16/49 [00:01<00:03,  9.42it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  35%|███▍      | 17/49 [00:01<00:03,  9.68it/s, v_num=crps, train_loss_step=0.796, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  37%|███▋      | 18/49 [00:01<00:03,  9.82it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  39%|███▉      | 19/49 [00:01<00:02, 10.03it/s, v_num=crps, train_loss_step=0.794, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  41%|████      | 20/49 [00:02<00:03,  9.22it/s, v_num=crps, train_loss_step=0.793, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  43%|████▎     | 21/49 [00:02<00:02,  9.43it/s, v_num=crps, train_loss_step=0.805, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  45%|████▍     | 22/49 [00:02<00:02,  9.63it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  47%|████▋     | 23/49 [00:02<00:02,  9.74it/s, v_num=crps, train_loss_step=0.795, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  49%|████▉     | 24/49 [00:02<00:02,  9.80it/s, v_num=crps, train_loss_step=0.807, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  51%|█████     | 25/49 [00:02<00:02,  9.23it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  53%|█████▎    | 26/49 [00:02<00:02,  9.28it/s, v_num=crps, train_loss_step=0.801, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  55%|█████▌    | 27/49 [00:02<00:02,  9.36it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  57%|█████▋    | 28/49 [00:02<00:02,  9.46it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  59%|█████▉    | 29/49 [00:03<00:02,  9.56it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  61%|██████    | 30/49 [00:03<00:02,  9.10it/s, v_num=crps, train_loss_step=0.798, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  63%|██████▎   | 31/49 [00:03<00:01,  9.18it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  65%|██████▌   | 32/49 [00:03<00:01,  9.33it/s, v_num=crps, train_loss_step=0.803, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  67%|██████▋   | 33/49 [00:03<00:01,  9.47it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  69%|██████▉   | 34/49 [00:03<00:01,  9.60it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  71%|███████▏  | 35/49 [00:03<00:01,  9.73it/s, v_num=crps, train_loss_step=0.813, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  73%|███████▎  | 36/49 [00:03<00:01,  9.34it/s, v_num=crps, train_loss_step=0.795, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  76%|███████▌  | 37/49 [00:03<00:01,  9.47it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  78%|███████▊  | 38/49 [00:03<00:01,  9.58it/s, v_num=crps, train_loss_step=0.809, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  80%|███████▉  | 39/49 [00:04<00:01,  9.61it/s, v_num=crps, train_loss_step=0.820, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  82%|████████▏ | 40/49 [00:04<00:00,  9.64it/s, v_num=crps, train_loss_step=0.802, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  84%|████████▎ | 41/49 [00:04<00:00,  9.26it/s, v_num=crps, train_loss_step=0.799, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  86%|████████▌ | 42/49 [00:04<00:00,  9.33it/s, v_num=crps, train_loss_step=0.819, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  88%|████████▊ | 43/49 [00:04<00:00,  9.40it/s, v_num=crps, train_loss_step=0.812, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  90%|████████▉ | 44/49 [00:04<00:00,  9.47it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  92%|█████████▏| 45/49 [00:04<00:00,  9.54it/s, v_num=crps, train_loss_step=0.808, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  94%|█████████▍| 46/49 [00:04<00:00,  9.25it/s, v_num=crps, train_loss_step=0.810, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  96%|█████████▌| 47/49 [00:05<00:00,  9.34it/s, v_num=crps, train_loss_step=0.816, train_loss_epoch=0.809]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 26:  98%|█████████▊| 48/49 [00:05<00:00,  9.44it/s, v_num=crps, train_loss_step=0.817, train_loss_epoch=0.809]torch.Size([5610, 512])\n",
      "torch.Size([5610, 512])\n",
      "Epoch 26: 100%|██████████| 49/49 [00:05<00:00,  9.52it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.807]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=27` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 49/49 [00:05<00:00,  9.50it/s, v_num=crps, train_loss_step=0.818, train_loss_epoch=0.807]\n",
      "Final MSE Loss: tensor(0.8182)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.80671</td></tr><tr><td>train_loss_step</td><td>0.80957</td></tr><tr><td>trainer/global_step</td><td>1322</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_72h_crps</strong> at: <a href='https://wandb.ai/leachen_thesis/reproduction/runs/training_run_72h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction/runs/training_run_72h_crps</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/reproduction' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250314_172950-training_run_72h_crps/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate",
   "id": "34ba5ca5ab65fff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:32:52.677453Z",
     "start_time": "2025-03-14T16:32:51.675368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NN object, train, test\n",
    "test_rf_dataset = TensorDataset(torch.Tensor(test_rf[0].to_numpy()),\n",
    "                                torch.Tensor(y_scaler.transform(test_rf[1][[\"t2m\"]])))\n",
    "test_rf_loader = DataLoader(test_rf_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "test_f_dataset = TensorDataset(torch.Tensor(test_f[0].to_numpy()), torch.Tensor(y_scaler.transform(test_f[1][[\"t2m\"]])))\n",
    "test_f_loader = DataLoader(test_f_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    log_every_n_steps=1, accelerator=\"gpu\", enable_progress_bar=True, enable_model_summary=False\n",
    ")\n",
    "preds_list = []\n",
    "\n",
    "if DATASET == \"f\":\n",
    "    targets = test_f[1] # R2F\n",
    "    preds = trainer.predict(model=mydrn, dataloaders=test_f_loader) #R2F\n",
    "    print(\"test_rf[1]:\")\n",
    "    print(targets)\n",
    "if DATASET == \"rf\":\n",
    "    targets = test_rf[1]\n",
    "    preds = trainer.predict(model=mydrn, dataloaders=test_rf_loader)\n",
    "    print(\"test_rf[1]:\")\n",
    "    print(targets)\n",
    "\n",
    "preds = torch.cat(preds, dim=0)\n",
    "# Reverse transform of the y_scaler (only on the mean)\n",
    "preds[:, 0] = torch.Tensor(y_scaler.inverse_transform(preds[:, 0].view(-1, 1))).flatten()\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = torch.Tensor(targets.t2m.values)\n",
    "print(\"t2m values:\")\n",
    "print(targets)\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "print(\"final_preds\")\n",
    "print(final_preds)\n",
    "\n",
    "res = mydrn.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "7e10e698863fdae",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/11 [00:00<?, ?it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:   9%|▉         | 1/11 [00:00<00:00, 225.26it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  18%|█▊        | 2/11 [00:00<00:00, 32.21it/s] torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  27%|██▋       | 3/11 [00:00<00:00,  9.56it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  36%|███▋      | 4/11 [00:00<00:00, 11.33it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  45%|████▌     | 5/11 [00:00<00:00, 12.06it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  55%|█████▍    | 6/11 [00:00<00:00, 13.28it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  64%|██████▎   | 7/11 [00:00<00:00, 14.35it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  73%|███████▎  | 8/11 [00:00<00:00, 11.07it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  82%|████████▏ | 9/11 [00:00<00:00, 11.84it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  91%|█████████ | 10/11 [00:00<00:00, 12.26it/s]torch.Size([4822, 512])\n",
      "torch.Size([4822, 512])\n",
      "Predicting DataLoader 0: 100%|██████████| 11/11 [00:00<00:00, 13.13it/s]\n",
      "test_rf[1]:\n",
      "            time  station_id     t2m\n",
      "0     2017-01-01           0  281.05\n",
      "1     2017-01-01           1  280.55\n",
      "2     2017-01-01           2  281.35\n",
      "3     2017-01-01           3  281.05\n",
      "4     2017-01-01           4  280.25\n",
      "...          ...         ...     ...\n",
      "89055 2018-12-31         117  274.35\n",
      "89056 2018-12-31         118  273.45\n",
      "89057 2018-12-31         119  272.45\n",
      "89058 2018-12-31         120  264.55\n",
      "89059 2018-12-31         121  264.75\n",
      "\n",
      "[86742 rows x 3 columns]\n",
      "t2m values:\n",
      "tensor([281.0500, 280.5500, 281.3500,  ..., 272.4500, 264.5500, 264.7500])\n",
      "final_preds\n",
      "tensor([[281.4283,   0.6726],\n",
      "        [279.9496,   0.9345],\n",
      "        [281.1844,   0.5554],\n",
      "        ...,\n",
      "        [269.9550,   1.1981],\n",
      "        [264.7473,   1.0823],\n",
      "        [265.0664,   1.3306]])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.792911171913147\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:32:57.851577Z",
     "start_time": "2025-03-14T16:32:57.647251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "print(df)\n",
    "print(os.path.join(SAVEPATH, f\"{DATASET}_results.csv\"))\n",
    "df.to_csv(os.path.join(SAVEPATH, f\"{DATASET}_results.csv\"), index=False)"
   ],
   "id": "48bbd23e549a13e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              t2m          mu     sigma\n",
      "0      281.049988  281.428284  0.672618\n",
      "1      280.549988  279.949585  0.934458\n",
      "2      281.350006  281.184448  0.555419\n",
      "3      281.049988  281.539795  0.717791\n",
      "4      280.250000  279.904205  0.822881\n",
      "...           ...         ...       ...\n",
      "86737  274.350006  273.457062  1.803527\n",
      "86738  273.450012  270.874756  1.190170\n",
      "86739  272.450012  269.955048  1.198111\n",
      "86740  264.549988  264.747253  1.082289\n",
      "86741  264.750000  265.066376  1.330646\n",
      "\n",
      "[86742 rows x 3 columns]\n",
      "/home/ltchen/gnnpp/explored_models/drn_72h/models/f_results.csv\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### PIT Histogram",
   "id": "149b554da15cfc0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:09:38.104340Z",
     "start_time": "2025-03-19T04:09:37.911995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT\n",
    "predictions = pd.read_csv(os.path.join(SAVEPATH, f\"{DATASET}_results.csv\"))\n",
    "predictions = predictions.dropna(axis=0)\n",
    "\n",
    "y = torch.tensor(predictions[\"t2m\"].to_numpy())\n",
    "preds = torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy())\n",
    "\n",
    "print(SAVEPATH)"
   ],
   "id": "601b765e4d473ea2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SAVEPATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# PIT\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m predictions \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[43mSAVEPATH\u001B[49m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDATASET\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_results.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m      3\u001B[0m predictions \u001B[38;5;241m=\u001B[39m predictions\u001B[38;5;241m.\u001B[39mdropna(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m      5\u001B[0m y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(predictions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt2m\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy())\n",
      "\u001B[0;31mNameError\u001B[0m: name 'SAVEPATH' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:33:05.809328Z",
     "start_time": "2025-03-14T16:33:05.779380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = predictions[\"t2m\"].to_numpy()\n",
    "print(y)\n",
    "print(y.shape)\n",
    "mu = predictions[\"mu\"].to_numpy()\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "sigma = predictions[\"sigma\"].to_numpy()\n",
    "print(sigma)\n",
    "print(sigma.shape)\n",
    "\n",
    "normalCRPS = NormalCRPS()\n",
    "\n",
    "err = normalCRPS.crps(mu_sigma=torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy()),\n",
    "                      y=torch.tensor(predictions[\"t2m\"].to_numpy())).item()\n",
    "err"
   ],
   "id": "524e4efea01e91f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[281.05 280.55 281.35 ... 272.45 264.55 264.75]\n",
      "(86742,)\n",
      "[281.42828 279.9496  281.18445 ... 269.95505 264.74725 265.06638]\n",
      "(86742,)\n",
      "[0.67261755 0.93445754 0.5554187  ... 1.1981114  1.0822891  1.3306456 ]\n",
      "(86742,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.792911128634246"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:33:08.476680Z",
     "start_time": "2025-03-14T16:33:08.259232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT for 72h\n",
    "probs = norm.cdf(y.flatten(), loc=mu.flatten(), scale=sigma.flatten())  # scale is standard deviation\n",
    "n, bins, patches = plt.hist(probs, bins=15, density=True)\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= (max(col))\n",
    "\n",
    "plt.ylim(0, 1.5)  # Layout\n",
    "plt.hlines(xmin=0, xmax=1, y=1, colors=\"black\", linestyles=\"--\")"
   ],
   "id": "5ecd2c91f8d5a7ea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f80ed4df640>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 120h R2F",
   "id": "2f2a6fce6848c13e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dataframes for train and valid",
   "id": "e3c856527c2dedba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:18:20.981455Z",
     "start_time": "2025-03-14T16:18:14.921654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataloader and preprocessing\n",
    "MODEL = \"120h\"\n",
    "dataframes = load_dataframes(mode=\"train\",\n",
    "                             leadtime=MODEL)  # train mode => for training nn? Wie wird das im Paper beschrieben?\n",
    "dataframes = summary_statistics(\n",
    "    dataframes)  # wie sehen die daten von summary statistics aus? => wenn das nur die Daten von einer Station sind, dann über Zeitpunkte\n",
    "dataframes.pop(\"stations\")\n",
    "\n",
    "# test\n",
    "for X, y in dataframes.values():  # wofuer?\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train, valid_test = normalize_features(\n",
    "    training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    ")\n",
    "\n",
    "train = drop_nans(train)\n",
    "(test_rf, test_f) = valid_test\n",
    "test_rf = drop_nans(test_rf)\n",
    "test_f = drop_nans(test_f)\n",
    "\n",
    "SAVEPATH = os.path.join(PATH, f\"drn_{MODEL}/models\")"
   ],
   "id": "8f8cbb42d3babd9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "[INFO] Normalizing features...\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:18:37.056033Z",
     "start_time": "2025-03-14T16:18:37.047260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONFIG_FOLDER = os.path.join(DIRECTORY, f\"trained_models/drn_{MODEL}\")\n",
    "JSONPATH = os.path.join(CONFIG_FOLDER, \"params.json\")\n",
    "\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "print(args_dict)\n",
    "config = args_dict"
   ],
   "id": "63dfca6cfdb4bd94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/drn_120h/params.json\n",
      "{'leadtime': '120h', 'batch_size': 8192, 'lr': 0.001, 'hidden_channels': [512], 'max_epochs': 20, 'only_summary': 'True'}\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train",
   "id": "824d15d46e188052"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:20:44.417922Z",
     "start_time": "2025-03-14T16:19:03.530227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with wandb.init(\n",
    "        project=\"reproduction\",\n",
    "        id=f\"training_run_{MODEL}_crps\",\n",
    "        config=config,\n",
    "        tags=[\"exploration\"],\n",
    "):\n",
    "    config = wandb.config\n",
    "    y_scaler = StandardScaler(with_std=False)  # wieso scalen wir überhaupt? => robuster?\n",
    "    y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "    #batch_size =2048\n",
    "    #hidden_size=128\n",
    "    #lr=0.0002\n",
    "    #max_epochs=31\n",
    "\n",
    "    embed_dim = 20\n",
    "    in_channels = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "    train_dataset = TensorDataset(torch.Tensor(train[0].to_numpy()),\n",
    "                                  torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]])))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    mydrn = CRPSDRN(\n",
    "        embedding_dim=embed_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=config.hidden_channels,\n",
    "\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config.lr),\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"all_station_crps\")\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=f\"run_{MODEL}\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config.max_epochs,\n",
    "        log_every_n_steps=10,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=mydrn, train_dataloaders=train_loader)\n",
    "\n",
    "    final_loss = trainer.logged_metrics[\"train_loss_step\"]\n",
    "    print(\"Final MSE Loss:\", final_loss)"
   ],
   "id": "a730d3a4e2a6cabb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250314_171903-training_run_120h_crps</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/reproduction/runs/training_run_120h_crps' target=\"_blank\">training_run_120h_crps</a></strong> to <a href='https://wandb.ai/leachen_thesis/reproduction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/reproduction' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/reproduction/runs/training_run_120h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction/runs/training_run_120h_crps</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/drn_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 43.5 K | train\n",
      "2 | relu              | ReLU          | 0      | train\n",
      "3 | softplus          | Softplus      | 0      | train\n",
      "4 | last_linear_mu    | Linear        | 513    | train\n",
      "5 | last_linear_sigma | Linear        | 513    | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "47.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "47.0 K    Total params\n",
      "0.188     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/49 [00:00<?, ?it/s] torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   2%|▏         | 1/49 [00:00<00:13,  3.55it/s, v_num=crps, train_loss_step=4.840]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   4%|▍         | 2/49 [00:00<00:07,  5.91it/s, v_num=crps, train_loss_step=4.690]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   6%|▌         | 3/49 [00:00<00:06,  7.58it/s, v_num=crps, train_loss_step=4.500]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:   8%|▊         | 4/49 [00:00<00:05,  8.58it/s, v_num=crps, train_loss_step=4.330]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  10%|█         | 5/49 [00:00<00:04,  9.55it/s, v_num=crps, train_loss_step=4.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  12%|█▏        | 6/49 [00:00<00:05,  7.54it/s, v_num=crps, train_loss_step=3.950]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  14%|█▍        | 7/49 [00:00<00:05,  8.25it/s, v_num=crps, train_loss_step=3.720]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  16%|█▋        | 8/49 [00:00<00:04,  8.85it/s, v_num=crps, train_loss_step=3.650]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  18%|█▊        | 9/49 [00:00<00:04,  9.25it/s, v_num=crps, train_loss_step=3.440]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  20%|██        | 10/49 [00:01<00:04,  9.73it/s, v_num=crps, train_loss_step=3.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  22%|██▏       | 11/49 [00:01<00:03, 10.16it/s, v_num=crps, train_loss_step=3.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  24%|██▍       | 12/49 [00:01<00:04,  9.02it/s, v_num=crps, train_loss_step=2.880]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  27%|██▋       | 13/49 [00:01<00:03,  9.39it/s, v_num=crps, train_loss_step=2.790]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  29%|██▊       | 14/49 [00:01<00:03,  9.71it/s, v_num=crps, train_loss_step=2.620]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  31%|███       | 15/49 [00:01<00:03, 10.01it/s, v_num=crps, train_loss_step=2.490]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  33%|███▎      | 16/49 [00:01<00:03, 10.30it/s, v_num=crps, train_loss_step=2.320]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  35%|███▍      | 17/49 [00:01<00:03,  9.41it/s, v_num=crps, train_loss_step=2.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  37%|███▋      | 18/49 [00:01<00:03,  9.67it/s, v_num=crps, train_loss_step=2.080]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  39%|███▉      | 19/49 [00:01<00:03,  9.91it/s, v_num=crps, train_loss_step=2.000]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  41%|████      | 20/49 [00:01<00:02, 10.06it/s, v_num=crps, train_loss_step=1.900]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  43%|████▎     | 21/49 [00:02<00:02, 10.20it/s, v_num=crps, train_loss_step=1.830]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  45%|████▍     | 22/49 [00:02<00:02,  9.44it/s, v_num=crps, train_loss_step=1.790]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  47%|████▋     | 23/49 [00:02<00:02,  9.64it/s, v_num=crps, train_loss_step=1.780]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  49%|████▉     | 24/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=1.740]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  51%|█████     | 25/49 [00:02<00:02,  9.89it/s, v_num=crps, train_loss_step=1.700]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  53%|█████▎    | 26/49 [00:02<00:02, 10.03it/s, v_num=crps, train_loss_step=1.710]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  55%|█████▌    | 27/49 [00:02<00:02, 10.10it/s, v_num=crps, train_loss_step=1.720]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  57%|█████▋    | 28/49 [00:02<00:02,  9.50it/s, v_num=crps, train_loss_step=1.710]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  59%|█████▉    | 29/49 [00:03<00:02,  9.66it/s, v_num=crps, train_loss_step=1.720]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  61%|██████    | 30/49 [00:03<00:01,  9.81it/s, v_num=crps, train_loss_step=1.720]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  63%|██████▎   | 31/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=1.710]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  65%|██████▌   | 32/49 [00:03<00:01, 10.11it/s, v_num=crps, train_loss_step=1.690]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  67%|██████▋   | 33/49 [00:03<00:01,  9.64it/s, v_num=crps, train_loss_step=1.700]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  69%|██████▉   | 34/49 [00:03<00:01,  9.77it/s, v_num=crps, train_loss_step=1.680]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  71%|███████▏  | 35/49 [00:03<00:01,  9.78it/s, v_num=crps, train_loss_step=1.640]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  73%|███████▎  | 36/49 [00:03<00:01,  9.90it/s, v_num=crps, train_loss_step=1.640]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  76%|███████▌  | 37/49 [00:03<00:01, 10.02it/s, v_num=crps, train_loss_step=1.630]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  78%|███████▊  | 38/49 [00:03<00:01,  9.53it/s, v_num=crps, train_loss_step=1.590]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  80%|███████▉  | 39/49 [00:04<00:01,  9.65it/s, v_num=crps, train_loss_step=1.580]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  82%|████████▏ | 40/49 [00:04<00:00,  9.75it/s, v_num=crps, train_loss_step=1.580]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  84%|████████▎ | 41/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=1.550]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  86%|████████▌ | 42/49 [00:04<00:00,  9.88it/s, v_num=crps, train_loss_step=1.510]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  88%|████████▊ | 43/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=1.500]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  90%|████████▉ | 44/49 [00:04<00:00,  9.65it/s, v_num=crps, train_loss_step=1.490]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  92%|█████████▏| 45/49 [00:04<00:00,  9.74it/s, v_num=crps, train_loss_step=1.480]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  94%|█████████▍| 46/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=1.450]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  96%|█████████▌| 47/49 [00:04<00:00,  9.85it/s, v_num=crps, train_loss_step=1.470]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 0:  98%|█████████▊| 48/49 [00:04<00:00,  9.93it/s, v_num=crps, train_loss_step=1.430]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 1:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.440, train_loss_epoch=2.290]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   2%|▏         | 1/49 [00:00<00:03, 12.51it/s, v_num=crps, train_loss_step=1.430, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   4%|▍         | 2/49 [00:00<00:04, 11.65it/s, v_num=crps, train_loss_step=1.460, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   6%|▌         | 3/49 [00:00<00:03, 13.21it/s, v_num=crps, train_loss_step=1.420, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:   8%|▊         | 4/49 [00:00<00:03, 13.24it/s, v_num=crps, train_loss_step=1.420, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  10%|█         | 5/49 [00:00<00:03, 13.96it/s, v_num=crps, train_loss_step=1.420, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  12%|█▏        | 6/49 [00:00<00:04,  9.85it/s, v_num=crps, train_loss_step=1.400, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  14%|█▍        | 7/49 [00:00<00:04, 10.50it/s, v_num=crps, train_loss_step=1.420, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  16%|█▋        | 8/49 [00:00<00:03, 10.67it/s, v_num=crps, train_loss_step=1.400, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  18%|█▊        | 9/49 [00:00<00:03, 11.18it/s, v_num=crps, train_loss_step=1.390, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  20%|██        | 10/49 [00:01<00:04,  9.44it/s, v_num=crps, train_loss_step=1.360, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  22%|██▏       | 11/49 [00:01<00:03,  9.87it/s, v_num=crps, train_loss_step=1.350, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  24%|██▍       | 12/49 [00:01<00:03, 10.27it/s, v_num=crps, train_loss_step=1.370, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  27%|██▋       | 13/49 [00:01<00:03, 10.60it/s, v_num=crps, train_loss_step=1.360, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  29%|██▊       | 14/49 [00:01<00:03, 10.76it/s, v_num=crps, train_loss_step=1.350, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  31%|███       | 15/49 [00:01<00:03, 10.90it/s, v_num=crps, train_loss_step=1.370, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  33%|███▎      | 16/49 [00:01<00:03,  9.83it/s, v_num=crps, train_loss_step=1.360, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  35%|███▍      | 17/49 [00:01<00:03, 10.01it/s, v_num=crps, train_loss_step=1.360, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  37%|███▋      | 18/49 [00:01<00:03, 10.28it/s, v_num=crps, train_loss_step=1.370, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  39%|███▉      | 19/49 [00:01<00:02, 10.52it/s, v_num=crps, train_loss_step=1.350, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  41%|████      | 20/49 [00:01<00:02, 10.76it/s, v_num=crps, train_loss_step=1.350, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  43%|████▎     | 21/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=1.330, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  45%|████▍     | 22/49 [00:02<00:02, 10.08it/s, v_num=crps, train_loss_step=1.340, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  47%|████▋     | 23/49 [00:02<00:02, 10.27it/s, v_num=crps, train_loss_step=1.330, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  49%|████▉     | 24/49 [00:02<00:02, 10.47it/s, v_num=crps, train_loss_step=1.340, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  51%|█████     | 25/49 [00:02<00:02, 10.64it/s, v_num=crps, train_loss_step=1.330, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  53%|█████▎    | 26/49 [00:02<00:02,  9.97it/s, v_num=crps, train_loss_step=1.330, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  55%|█████▌    | 27/49 [00:02<00:02, 10.13it/s, v_num=crps, train_loss_step=1.330, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  57%|█████▋    | 28/49 [00:02<00:02, 10.29it/s, v_num=crps, train_loss_step=1.300, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  59%|█████▉    | 29/49 [00:02<00:01, 10.41it/s, v_num=crps, train_loss_step=1.310, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  61%|██████    | 30/49 [00:02<00:01, 10.56it/s, v_num=crps, train_loss_step=1.320, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  63%|██████▎   | 31/49 [00:02<00:01, 10.71it/s, v_num=crps, train_loss_step=1.300, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  65%|██████▌   | 32/49 [00:03<00:01, 10.16it/s, v_num=crps, train_loss_step=1.310, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  67%|██████▋   | 33/49 [00:03<00:01, 10.30it/s, v_num=crps, train_loss_step=1.320, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  69%|██████▉   | 34/49 [00:03<00:01, 10.43it/s, v_num=crps, train_loss_step=1.320, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  71%|███████▏  | 35/49 [00:03<00:01, 10.52it/s, v_num=crps, train_loss_step=1.300, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  73%|███████▎  | 36/49 [00:03<00:01, 10.53it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  76%|███████▌  | 37/49 [00:03<00:01, 10.08it/s, v_num=crps, train_loss_step=1.310, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  78%|███████▊  | 38/49 [00:03<00:01, 10.20it/s, v_num=crps, train_loss_step=1.300, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  80%|███████▉  | 39/49 [00:03<00:00, 10.22it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  82%|████████▏ | 40/49 [00:03<00:00, 10.33it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  84%|████████▎ | 41/49 [00:03<00:00, 10.44it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  86%|████████▌ | 42/49 [00:04<00:00, 10.04it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  88%|████████▊ | 43/49 [00:04<00:00, 10.06it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  90%|████████▉ | 44/49 [00:04<00:00, 10.17it/s, v_num=crps, train_loss_step=1.280, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  92%|█████████▏| 45/49 [00:04<00:00, 10.27it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  94%|█████████▍| 46/49 [00:04<00:00, 10.36it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  96%|█████████▌| 47/49 [00:04<00:00, 10.35it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=2.290]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 1:  98%|█████████▊| 48/49 [00:04<00:00, 10.01it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=2.290]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 2:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   2%|▏         | 1/49 [00:00<00:03, 13.26it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   4%|▍         | 2/49 [00:00<00:03, 15.33it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   6%|▌         | 3/49 [00:00<00:02, 15.70it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:   8%|▊         | 4/49 [00:00<00:05,  8.27it/s, v_num=crps, train_loss_step=1.290, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  10%|█         | 5/49 [00:00<00:04,  9.27it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  12%|█▏        | 6/49 [00:00<00:04,  9.61it/s, v_num=crps, train_loss_step=1.260, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  14%|█▍        | 7/49 [00:00<00:04, 10.03it/s, v_num=crps, train_loss_step=1.260, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  16%|█▋        | 8/49 [00:00<00:03, 10.63it/s, v_num=crps, train_loss_step=1.260, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  18%|█▊        | 9/49 [00:00<00:03, 11.15it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  20%|██        | 10/49 [00:01<00:04,  9.43it/s, v_num=crps, train_loss_step=1.260, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  22%|██▏       | 11/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  24%|██▍       | 12/49 [00:01<00:03,  9.88it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  27%|██▋       | 13/49 [00:01<00:03,  9.97it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  29%|██▊       | 14/49 [00:01<00:03,  9.94it/s, v_num=crps, train_loss_step=1.260, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  31%|███       | 15/49 [00:01<00:03,  8.91it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  33%|███▎      | 16/49 [00:01<00:03,  9.20it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  35%|███▍      | 17/49 [00:01<00:03,  9.48it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  37%|███▋      | 18/49 [00:01<00:03,  9.74it/s, v_num=crps, train_loss_step=1.270, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  39%|███▉      | 19/49 [00:01<00:03,  9.99it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  41%|████      | 20/49 [00:02<00:03,  9.27it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  43%|████▎     | 21/49 [00:02<00:02,  9.49it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  45%|████▍     | 22/49 [00:02<00:02,  9.70it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  47%|████▋     | 23/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  49%|████▉     | 24/49 [00:02<00:02, 10.10it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  51%|█████     | 25/49 [00:02<00:02, 10.28it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  53%|█████▎    | 26/49 [00:02<00:02,  9.70it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  55%|█████▌    | 27/49 [00:02<00:02,  9.87it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  57%|█████▋    | 28/49 [00:02<00:02, 10.03it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  59%|█████▉    | 29/49 [00:02<00:01, 10.14it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  61%|██████    | 30/49 [00:02<00:01, 10.22it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  63%|██████▎   | 31/49 [00:03<00:01,  9.73it/s, v_num=crps, train_loss_step=1.260, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  65%|██████▌   | 32/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  67%|██████▋   | 33/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=1.250, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  69%|██████▉   | 34/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  71%|███████▏  | 35/49 [00:03<00:01, 10.22it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  73%|███████▎  | 36/49 [00:03<00:01,  9.71it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  76%|███████▌  | 37/49 [00:03<00:01,  9.84it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  78%|███████▊  | 38/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  80%|███████▉  | 39/49 [00:03<00:00, 10.08it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  82%|████████▏ | 40/49 [00:03<00:00, 10.15it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  84%|████████▎ | 41/49 [00:04<00:00, 10.14it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  86%|████████▌ | 42/49 [00:04<00:00,  9.73it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  88%|████████▊ | 43/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  90%|████████▉ | 44/49 [00:04<00:00,  9.94it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  92%|█████████▏| 45/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  94%|█████████▍| 46/49 [00:04<00:00, 10.08it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  96%|█████████▌| 47/49 [00:04<00:00,  9.73it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.340]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 2:  98%|█████████▊| 48/49 [00:04<00:00,  9.79it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.340]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 3:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   2%|▏         | 1/49 [00:00<00:04, 11.52it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   4%|▍         | 2/49 [00:00<00:03, 13.93it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   6%|▌         | 3/49 [00:00<00:03, 13.89it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:   8%|▊         | 4/49 [00:00<00:05,  8.48it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  10%|█         | 5/49 [00:00<00:04,  9.12it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  12%|█▏        | 6/49 [00:00<00:04,  9.95it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  14%|█▍        | 7/49 [00:00<00:03, 10.66it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  16%|█▋        | 8/49 [00:00<00:03, 10.70it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  18%|█▊        | 9/49 [00:00<00:04,  9.01it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  20%|██        | 10/49 [00:01<00:04,  9.51it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  22%|██▏       | 11/49 [00:01<00:03,  9.94it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  24%|██▍       | 12/49 [00:01<00:03, 10.34it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  27%|██▋       | 13/49 [00:01<00:03, 10.71it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  29%|██▊       | 14/49 [00:01<00:03,  9.54it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  31%|███       | 15/49 [00:01<00:03,  9.85it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  33%|███▎      | 16/49 [00:01<00:03, 10.15it/s, v_num=crps, train_loss_step=1.230, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  35%|███▍      | 17/49 [00:01<00:03, 10.43it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  37%|███▋      | 18/49 [00:01<00:02, 10.69it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  39%|███▉      | 19/49 [00:01<00:02, 10.90it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  41%|████      | 20/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  43%|████▎     | 21/49 [00:02<00:02, 10.16it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  45%|████▍     | 22/49 [00:02<00:02, 10.28it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  47%|████▋     | 23/49 [00:02<00:02, 10.47it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  49%|████▉     | 24/49 [00:02<00:02, 10.66it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  51%|█████     | 25/49 [00:02<00:02,  9.98it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  53%|█████▎    | 26/49 [00:02<00:02, 10.06it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  55%|█████▌    | 27/49 [00:02<00:02, 10.17it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  57%|█████▋    | 28/49 [00:02<00:02, 10.30it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  59%|█████▉    | 29/49 [00:02<00:01, 10.41it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  61%|██████    | 30/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  63%|██████▎   | 31/49 [00:03<00:01, 10.02it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  65%|██████▌   | 32/49 [00:03<00:01, 10.15it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  67%|██████▋   | 33/49 [00:03<00:01, 10.28it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  69%|██████▉   | 34/49 [00:03<00:01, 10.41it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  71%|███████▏  | 35/49 [00:03<00:01, 10.54it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  73%|███████▎  | 36/49 [00:03<00:01, 10.07it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  76%|███████▌  | 37/49 [00:03<00:01, 10.15it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  78%|███████▊  | 38/49 [00:03<00:01, 10.27it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  80%|███████▉  | 39/49 [00:03<00:00, 10.39it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  82%|████████▏ | 40/49 [00:03<00:00, 10.50it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  84%|████████▎ | 41/49 [00:04<00:00, 10.10it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  86%|████████▌ | 42/49 [00:04<00:00, 10.21it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  88%|████████▊ | 43/49 [00:04<00:00, 10.32it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  90%|████████▉ | 44/49 [00:04<00:00, 10.42it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  92%|█████████▏| 45/49 [00:04<00:00, 10.48it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  94%|█████████▍| 46/49 [00:04<00:00, 10.58it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  96%|█████████▌| 47/49 [00:04<00:00, 10.22it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.240]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 3:  98%|█████████▊| 48/49 [00:04<00:00, 10.31it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.240]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 4:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   2%|▏         | 1/49 [00:00<00:03, 13.13it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   4%|▍         | 2/49 [00:00<00:03, 13.59it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   6%|▌         | 3/49 [00:00<00:06,  7.52it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:   8%|▊         | 4/49 [00:00<00:05,  8.47it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  10%|█         | 5/49 [00:00<00:04,  9.49it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  12%|█▏        | 6/49 [00:00<00:04, 10.35it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  14%|█▍        | 7/49 [00:00<00:03, 11.06it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  16%|█▋        | 8/49 [00:00<00:04,  9.01it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  18%|█▊        | 9/49 [00:00<00:04,  9.54it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  20%|██        | 10/49 [00:00<00:03, 10.03it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  22%|██▏       | 11/49 [00:01<00:03, 10.46it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  24%|██▍       | 12/49 [00:01<00:03, 10.86it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  27%|██▋       | 13/49 [00:01<00:03, 11.21it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  29%|██▊       | 14/49 [00:01<00:03,  9.93it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  31%|███       | 15/49 [00:01<00:03, 10.23it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  33%|███▎      | 16/49 [00:01<00:03, 10.39it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  35%|███▍      | 17/49 [00:01<00:03, 10.67it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  37%|███▋      | 18/49 [00:01<00:02, 10.92it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  39%|███▉      | 19/49 [00:01<00:02, 10.01it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  41%|████      | 20/49 [00:01<00:02, 10.14it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  43%|████▎     | 21/49 [00:02<00:02, 10.35it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  45%|████▍     | 22/49 [00:02<00:02, 10.31it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  47%|████▋     | 23/49 [00:02<00:02, 10.51it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  49%|████▉     | 24/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  51%|█████     | 25/49 [00:02<00:02, 10.02it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  53%|█████▎    | 26/49 [00:02<00:02, 10.18it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  55%|█████▌    | 27/49 [00:02<00:02, 10.29it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  57%|█████▋    | 28/49 [00:02<00:02, 10.39it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  59%|█████▉    | 29/49 [00:02<00:01, 10.48it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  61%|██████    | 30/49 [00:03<00:01,  9.86it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  63%|██████▎   | 31/49 [00:03<00:01, 10.00it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  65%|██████▌   | 32/49 [00:03<00:01, 10.14it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  67%|██████▋   | 33/49 [00:03<00:01, 10.28it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  69%|██████▉   | 34/49 [00:03<00:01, 10.42it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  71%|███████▏  | 35/49 [00:03<00:01,  9.95it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  73%|███████▎  | 36/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  76%|███████▌  | 37/49 [00:03<00:01, 10.07it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  78%|███████▊  | 38/49 [00:03<00:01, 10.11it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  80%|███████▉  | 39/49 [00:03<00:00, 10.21it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  82%|████████▏ | 40/49 [00:04<00:00,  9.77it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  84%|████████▎ | 41/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  86%|████████▌ | 42/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  88%|████████▊ | 43/49 [00:04<00:00, 10.02it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  90%|████████▉ | 44/49 [00:04<00:00, 10.12it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  92%|█████████▏| 45/49 [00:04<00:00,  9.77it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  94%|█████████▍| 46/49 [00:04<00:00,  9.87it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  96%|█████████▌| 47/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.210]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 4:  98%|█████████▊| 48/49 [00:04<00:00, 10.02it/s, v_num=crps, train_loss_step=1.220, train_loss_epoch=1.210]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 5:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   2%|▏         | 1/49 [00:00<00:03, 13.57it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   4%|▍         | 2/49 [00:00<00:08,  5.82it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   6%|▌         | 3/49 [00:00<00:06,  7.54it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:   8%|▊         | 4/49 [00:00<00:05,  8.84it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  10%|█         | 5/49 [00:00<00:04,  9.59it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  12%|█▏        | 6/49 [00:00<00:04, 10.41it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  14%|█▍        | 7/49 [00:00<00:04,  8.45it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  16%|█▋        | 8/49 [00:00<00:04,  9.06it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  18%|█▊        | 9/49 [00:00<00:04,  9.60it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  20%|██        | 10/49 [00:00<00:03, 10.03it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  22%|██▏       | 11/49 [00:01<00:03, 10.31it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  24%|██▍       | 12/49 [00:01<00:03, 10.69it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  27%|██▋       | 13/49 [00:01<00:03,  9.46it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  29%|██▊       | 14/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  31%|███       | 15/49 [00:01<00:03, 10.00it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  33%|███▎      | 16/49 [00:01<00:03, 10.28it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  35%|███▍      | 17/49 [00:01<00:03, 10.54it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  37%|███▋      | 18/49 [00:01<00:03,  9.64it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  39%|███▉      | 19/49 [00:01<00:03,  9.90it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  41%|████      | 20/49 [00:01<00:02, 10.13it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  43%|████▎     | 21/49 [00:02<00:02, 10.35it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  45%|████▍     | 22/49 [00:02<00:02, 10.57it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  47%|████▋     | 23/49 [00:02<00:02,  9.84it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  49%|████▉     | 24/49 [00:02<00:02, 10.04it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  51%|█████     | 25/49 [00:02<00:02, 10.22it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  53%|█████▎    | 26/49 [00:02<00:02, 10.40it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  55%|█████▌    | 27/49 [00:02<00:02, 10.58it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  57%|█████▋    | 28/49 [00:02<00:01, 10.74it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  59%|█████▉    | 29/49 [00:02<00:01, 10.15it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  61%|██████    | 30/49 [00:02<00:01, 10.22it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  63%|██████▎   | 31/49 [00:03<00:01, 10.23it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  65%|██████▌   | 32/49 [00:03<00:01, 10.31it/s, v_num=crps, train_loss_step=1.200, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  67%|██████▋   | 33/49 [00:03<00:01, 10.45it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  69%|██████▉   | 34/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  71%|███████▏  | 35/49 [00:03<00:01, 10.11it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  73%|███████▎  | 36/49 [00:03<00:01, 10.24it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  76%|███████▌  | 37/49 [00:03<00:01, 10.36it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  78%|███████▊  | 38/49 [00:03<00:01, 10.49it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  80%|███████▉  | 39/49 [00:03<00:00, 10.04it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  82%|████████▏ | 40/49 [00:03<00:00, 10.16it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  84%|████████▎ | 41/49 [00:03<00:00, 10.27it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  86%|████████▌ | 42/49 [00:04<00:00, 10.38it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  88%|████████▊ | 43/49 [00:04<00:00, 10.48it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  90%|████████▉ | 44/49 [00:04<00:00, 10.58it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  92%|█████████▏| 45/49 [00:04<00:00, 10.17it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  94%|█████████▍| 46/49 [00:04<00:00, 10.24it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  96%|█████████▌| 47/49 [00:04<00:00, 10.29it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.190]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 5:  98%|█████████▊| 48/49 [00:04<00:00, 10.34it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.190]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 6:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   2%|▏         | 1/49 [00:00<00:14,  3.28it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   4%|▍         | 2/49 [00:00<00:08,  5.31it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   6%|▌         | 3/49 [00:00<00:06,  6.77it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:   8%|▊         | 4/49 [00:00<00:05,  7.98it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  10%|█         | 5/49 [00:00<00:05,  8.62it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  12%|█▏        | 6/49 [00:00<00:04,  9.43it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  14%|█▍        | 7/49 [00:00<00:05,  7.86it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  16%|█▋        | 8/49 [00:00<00:04,  8.45it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  18%|█▊        | 9/49 [00:01<00:04,  8.99it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  20%|██        | 10/49 [00:01<00:04,  9.46it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  22%|██▏       | 11/49 [00:01<00:03,  9.74it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  24%|██▍       | 12/49 [00:01<00:04,  8.68it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  27%|██▋       | 13/49 [00:01<00:04,  8.90it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  29%|██▊       | 14/49 [00:01<00:03,  9.25it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  31%|███       | 15/49 [00:01<00:03,  9.56it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  33%|███▎      | 16/49 [00:01<00:03,  9.86it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  35%|███▍      | 17/49 [00:01<00:03,  9.06it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  37%|███▋      | 18/49 [00:01<00:03,  9.32it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  39%|███▉      | 19/49 [00:01<00:03,  9.57it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  41%|████      | 20/49 [00:02<00:03,  9.61it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  43%|████▎     | 21/49 [00:02<00:02,  9.72it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  45%|████▍     | 22/49 [00:02<00:02,  9.79it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  47%|████▋     | 23/49 [00:02<00:02,  9.09it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  49%|████▉     | 24/49 [00:02<00:02,  9.29it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  51%|█████     | 25/49 [00:02<00:02,  9.37it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  53%|█████▎    | 26/49 [00:02<00:02,  9.54it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  55%|█████▌    | 27/49 [00:02<00:02,  9.72it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  57%|█████▋    | 28/49 [00:03<00:02,  9.24it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  59%|█████▉    | 29/49 [00:03<00:02,  9.40it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  61%|██████    | 30/49 [00:03<00:01,  9.51it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  63%|██████▎   | 31/49 [00:03<00:01,  9.66it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  65%|██████▌   | 32/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  67%|██████▋   | 33/49 [00:03<00:01,  9.37it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  69%|██████▉   | 34/49 [00:03<00:01,  9.50it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  71%|███████▏  | 35/49 [00:03<00:01,  9.63it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  73%|███████▎  | 36/49 [00:03<00:01,  9.76it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  76%|███████▌  | 37/49 [00:03<00:01,  9.88it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  78%|███████▊  | 38/49 [00:03<00:01, 10.00it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  80%|███████▉  | 39/49 [00:04<00:01,  9.56it/s, v_num=crps, train_loss_step=1.190, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  82%|████████▏ | 40/49 [00:04<00:00,  9.66it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  84%|████████▎ | 41/49 [00:04<00:00,  9.78it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  86%|████████▌ | 42/49 [00:04<00:00,  9.89it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  88%|████████▊ | 43/49 [00:04<00:00, 10.00it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  90%|████████▉ | 44/49 [00:04<00:00,  9.67it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  92%|█████████▏| 45/49 [00:04<00:00,  9.77it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  94%|█████████▍| 46/49 [00:04<00:00,  9.84it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  96%|█████████▌| 47/49 [00:04<00:00,  9.94it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.180]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 6:  98%|█████████▊| 48/49 [00:04<00:00, 10.03it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.180]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 7:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   2%|▏         | 1/49 [00:00<00:12,  3.74it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   4%|▍         | 2/49 [00:00<00:07,  6.23it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   6%|▌         | 3/49 [00:00<00:05,  7.97it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:   8%|▊         | 4/49 [00:00<00:04,  9.29it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  10%|█         | 5/49 [00:00<00:06,  7.29it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  12%|█▏        | 6/49 [00:00<00:05,  8.10it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  14%|█▍        | 7/49 [00:00<00:04,  8.81it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  16%|█▋        | 8/49 [00:00<00:04,  9.42it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  18%|█▊        | 9/49 [00:00<00:04,  9.93it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  20%|██        | 10/49 [00:00<00:03, 10.22it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  22%|██▏       | 11/49 [00:01<00:04,  8.96it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  24%|██▍       | 12/49 [00:01<00:04,  9.13it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  27%|██▋       | 13/49 [00:01<00:03,  9.47it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  29%|██▊       | 14/49 [00:01<00:03,  9.70it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  31%|███       | 15/49 [00:01<00:03, 10.02it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  33%|███▎      | 16/49 [00:01<00:03,  9.15it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  35%|███▍      | 17/49 [00:01<00:03,  9.43it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  37%|███▋      | 18/49 [00:01<00:03,  9.68it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  39%|███▉      | 19/49 [00:01<00:03,  9.92it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  41%|████      | 20/49 [00:01<00:02, 10.15it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  43%|████▎     | 21/49 [00:02<00:02,  9.45it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  45%|████▍     | 22/49 [00:02<00:02,  9.66it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  47%|████▋     | 23/49 [00:02<00:02,  9.87it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  49%|████▉     | 24/49 [00:02<00:02, 10.06it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  51%|█████     | 25/49 [00:02<00:02, 10.25it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  53%|█████▎    | 26/49 [00:02<00:02, 10.42it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  55%|█████▌    | 27/49 [00:02<00:02,  9.84it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  57%|█████▋    | 28/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  59%|█████▉    | 29/49 [00:02<00:01, 10.17it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  61%|██████    | 30/49 [00:02<00:01, 10.32it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  63%|██████▎   | 31/49 [00:02<00:01, 10.48it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  65%|██████▌   | 32/49 [00:03<00:01,  9.97it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  67%|██████▋   | 33/49 [00:03<00:01, 10.11it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  69%|██████▉   | 34/49 [00:03<00:01, 10.24it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  71%|███████▏  | 35/49 [00:03<00:01, 10.38it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  73%|███████▎  | 36/49 [00:03<00:01, 10.50it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  76%|███████▌  | 37/49 [00:03<00:01, 10.06it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  78%|███████▊  | 38/49 [00:03<00:01, 10.18it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  80%|███████▉  | 39/49 [00:03<00:00, 10.30it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  82%|████████▏ | 40/49 [00:03<00:00, 10.41it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  84%|████████▎ | 41/49 [00:03<00:00, 10.53it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  86%|████████▌ | 42/49 [00:03<00:00, 10.63it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  88%|████████▊ | 43/49 [00:04<00:00, 10.24it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  90%|████████▉ | 44/49 [00:04<00:00, 10.34it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  92%|█████████▏| 45/49 [00:04<00:00, 10.36it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  94%|█████████▍| 46/49 [00:04<00:00, 10.46it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  96%|█████████▌| 47/49 [00:04<00:00, 10.56it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 7:  98%|█████████▊| 48/49 [00:04<00:00, 10.16it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.170]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 8:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   2%|▏         | 1/49 [00:00<00:03, 13.31it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   4%|▍         | 2/49 [00:00<00:03, 15.20it/s, v_num=crps, train_loss_step=1.180, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   6%|▌         | 3/49 [00:00<00:03, 13.74it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:   8%|▊         | 4/49 [00:00<00:03, 14.57it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  10%|█         | 5/49 [00:00<00:04,  9.56it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  12%|█▏        | 6/49 [00:00<00:04, 10.40it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  14%|█▍        | 7/49 [00:00<00:03, 11.10it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  16%|█▋        | 8/49 [00:00<00:03, 11.68it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  18%|█▊        | 9/49 [00:00<00:04,  9.56it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  20%|██        | 10/49 [00:00<00:03, 10.04it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  22%|██▏       | 11/49 [00:01<00:03, 10.47it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  24%|██▍       | 12/49 [00:01<00:03, 10.86it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  27%|██▋       | 13/49 [00:01<00:03, 11.18it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  29%|██▊       | 14/49 [00:01<00:03, 11.49it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  31%|███       | 15/49 [00:01<00:03, 10.20it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  33%|███▎      | 16/49 [00:01<00:03, 10.49it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  35%|███▍      | 17/49 [00:01<00:02, 10.77it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  37%|███▋      | 18/49 [00:01<00:02, 11.02it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  39%|███▉      | 19/49 [00:01<00:02, 11.22it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  41%|████      | 20/49 [00:01<00:02, 10.11it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  43%|████▎     | 21/49 [00:02<00:02, 10.26it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  45%|████▍     | 22/49 [00:02<00:02, 10.38it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  47%|████▋     | 23/49 [00:02<00:02, 10.57it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  49%|████▉     | 24/49 [00:02<00:02, 10.57it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  51%|█████     | 25/49 [00:02<00:02,  9.77it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  53%|█████▎    | 26/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  55%|█████▌    | 27/49 [00:02<00:02, 10.12it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  57%|█████▋    | 28/49 [00:02<00:02, 10.16it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  59%|█████▉    | 29/49 [00:02<00:01, 10.24it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  61%|██████    | 30/49 [00:02<00:01, 10.39it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  63%|██████▎   | 31/49 [00:03<00:01,  9.81it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  65%|██████▌   | 32/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  67%|██████▋   | 33/49 [00:03<00:01, 10.10it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  69%|██████▉   | 34/49 [00:03<00:01, 10.24it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  71%|███████▏  | 35/49 [00:03<00:01, 10.35it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  73%|███████▎  | 36/49 [00:03<00:01,  9.88it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  76%|███████▌  | 37/49 [00:03<00:01,  9.99it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  78%|███████▊  | 38/49 [00:03<00:01, 10.01it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  80%|███████▉  | 39/49 [00:03<00:00, 10.01it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  82%|████████▏ | 40/49 [00:03<00:00, 10.08it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  84%|████████▎ | 41/49 [00:04<00:00,  9.71it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  86%|████████▌ | 42/49 [00:04<00:00,  9.82it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  88%|████████▊ | 43/49 [00:04<00:00,  9.89it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  90%|████████▉ | 44/49 [00:04<00:00, 10.00it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  92%|█████████▏| 45/49 [00:04<00:00, 10.10it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  94%|█████████▍| 46/49 [00:04<00:00, 10.09it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  96%|█████████▌| 47/49 [00:04<00:00,  9.71it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.160]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 8:  98%|█████████▊| 48/49 [00:04<00:00,  9.80it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.160]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 9:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   2%|▏         | 1/49 [00:00<00:05,  9.01it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   4%|▍         | 2/49 [00:00<00:03, 12.09it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   6%|▌         | 3/49 [00:00<00:07,  6.47it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:   8%|▊         | 4/49 [00:00<00:06,  7.07it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  10%|█         | 5/49 [00:00<00:05,  8.05it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  12%|█▏        | 6/49 [00:00<00:04,  8.63it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  14%|█▍        | 7/49 [00:00<00:04,  9.14it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  16%|█▋        | 8/49 [00:00<00:04,  9.53it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  18%|█▊        | 9/49 [00:01<00:04,  8.22it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  20%|██        | 10/49 [00:01<00:04,  8.57it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  22%|██▏       | 11/49 [00:01<00:04,  9.01it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  24%|██▍       | 12/49 [00:01<00:04,  9.21it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  27%|██▋       | 13/49 [00:01<00:03,  9.53it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  29%|██▊       | 14/49 [00:01<00:04,  8.51it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  31%|███       | 15/49 [00:01<00:03,  8.68it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  33%|███▎      | 16/49 [00:01<00:03,  8.88it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  35%|███▍      | 17/49 [00:01<00:03,  9.15it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  37%|███▋      | 18/49 [00:01<00:03,  9.42it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  39%|███▉      | 19/49 [00:02<00:03,  8.77it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  41%|████      | 20/49 [00:02<00:03,  9.01it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  43%|████▎     | 21/49 [00:02<00:03,  9.20it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  45%|████▍     | 22/49 [00:02<00:02,  9.41it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  47%|████▋     | 23/49 [00:02<00:02,  9.61it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  49%|████▉     | 24/49 [00:02<00:02,  9.80it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  51%|█████     | 25/49 [00:02<00:02,  9.26it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  53%|█████▎    | 26/49 [00:02<00:02,  9.44it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  55%|█████▌    | 27/49 [00:02<00:02,  9.56it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  57%|█████▋    | 28/49 [00:02<00:02,  9.72it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  59%|█████▉    | 29/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  61%|██████    | 30/49 [00:03<00:02,  9.34it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  63%|██████▎   | 31/49 [00:03<00:01,  9.37it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  65%|██████▌   | 32/49 [00:03<00:01,  9.46it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  67%|██████▋   | 33/49 [00:03<00:01,  9.60it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  69%|██████▉   | 34/49 [00:03<00:01,  9.74it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  71%|███████▏  | 35/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  73%|███████▎  | 36/49 [00:03<00:01,  9.43it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  76%|███████▌  | 37/49 [00:03<00:01,  9.44it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  78%|███████▊  | 38/49 [00:03<00:01,  9.56it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  80%|███████▉  | 39/49 [00:04<00:01,  9.68it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  82%|████████▏ | 40/49 [00:04<00:00,  9.80it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  84%|████████▎ | 41/49 [00:04<00:00,  9.42it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  86%|████████▌ | 42/49 [00:04<00:00,  9.50it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  88%|████████▊ | 43/49 [00:04<00:00,  9.60it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  90%|████████▉ | 44/49 [00:04<00:00,  9.71it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  92%|█████████▏| 45/49 [00:04<00:00,  9.76it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  94%|█████████▍| 46/49 [00:04<00:00,  9.39it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  96%|█████████▌| 47/49 [00:04<00:00,  9.43it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 9:  98%|█████████▊| 48/49 [00:05<00:00,  9.53it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 10:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]        torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   2%|▏         | 1/49 [00:00<00:03, 12.54it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   4%|▍         | 2/49 [00:00<00:03, 14.78it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   6%|▌         | 3/49 [00:00<00:06,  7.17it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:   8%|▊         | 4/49 [00:00<00:05,  8.46it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  10%|█         | 5/49 [00:00<00:04,  9.49it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  12%|█▏        | 6/49 [00:00<00:04, 10.32it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  14%|█▍        | 7/49 [00:00<00:03, 11.03it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  16%|█▋        | 8/49 [00:00<00:04,  9.03it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  18%|█▊        | 9/49 [00:00<00:04,  9.57it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  20%|██        | 10/49 [00:00<00:03, 10.04it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  22%|██▏       | 11/49 [00:01<00:03, 10.44it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  24%|██▍       | 12/49 [00:01<00:03, 10.55it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  27%|██▋       | 13/49 [00:01<00:03,  9.35it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  29%|██▊       | 14/49 [00:01<00:03,  9.68it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  31%|███       | 15/49 [00:01<00:03, 10.00it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  33%|███▎      | 16/49 [00:01<00:03, 10.30it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  35%|███▍      | 17/49 [00:01<00:03, 10.57it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  37%|███▋      | 18/49 [00:01<00:02, 10.83it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  39%|███▉      | 19/49 [00:01<00:03,  9.94it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  41%|████      | 20/49 [00:01<00:02, 10.15it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  43%|████▎     | 21/49 [00:02<00:02, 10.29it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  45%|████▍     | 22/49 [00:02<00:02, 10.51it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  47%|████▋     | 23/49 [00:02<00:02, 10.71it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  49%|████▉     | 24/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  51%|█████     | 25/49 [00:02<00:02, 10.20it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  53%|█████▎    | 26/49 [00:02<00:02, 10.37it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  55%|█████▌    | 27/49 [00:02<00:02, 10.55it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  57%|█████▋    | 28/49 [00:02<00:01, 10.71it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  59%|█████▉    | 29/49 [00:02<00:01, 10.05it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  61%|██████    | 30/49 [00:02<00:01, 10.20it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  63%|██████▎   | 31/49 [00:02<00:01, 10.35it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  65%|██████▌   | 32/49 [00:03<00:01, 10.49it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  67%|██████▋   | 33/49 [00:03<00:01, 10.63it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  69%|██████▉   | 34/49 [00:03<00:01, 10.12it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  71%|███████▏  | 35/49 [00:03<00:01, 10.25it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  73%|███████▎  | 36/49 [00:03<00:01, 10.38it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  76%|███████▌  | 37/49 [00:03<00:01, 10.49it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  78%|███████▊  | 38/49 [00:03<00:01, 10.61it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  80%|███████▉  | 39/49 [00:03<00:00, 10.63it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  82%|████████▏ | 40/49 [00:03<00:00, 10.20it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  84%|████████▎ | 41/49 [00:03<00:00, 10.31it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  86%|████████▌ | 42/49 [00:04<00:00, 10.43it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  88%|████████▊ | 43/49 [00:04<00:00, 10.51it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  90%|████████▉ | 44/49 [00:04<00:00, 10.55it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  92%|█████████▏| 45/49 [00:04<00:00, 10.17it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  94%|█████████▍| 46/49 [00:04<00:00, 10.22it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  96%|█████████▌| 47/49 [00:04<00:00, 10.27it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 10:  98%|█████████▊| 48/49 [00:04<00:00, 10.36it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.150]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 11:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   2%|▏         | 1/49 [00:00<00:05,  8.82it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   4%|▍         | 2/49 [00:00<00:08,  5.53it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   6%|▌         | 3/49 [00:00<00:06,  7.19it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:   8%|▊         | 4/49 [00:00<00:05,  8.19it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  10%|█         | 5/49 [00:00<00:04,  9.24it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  12%|█▏        | 6/49 [00:00<00:04,  9.79it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  14%|█▍        | 7/49 [00:00<00:05,  8.07it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  16%|█▋        | 8/49 [00:00<00:04,  8.68it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  18%|█▊        | 9/49 [00:00<00:04,  9.17it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  20%|██        | 10/49 [00:01<00:04,  9.36it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  22%|██▏       | 11/49 [00:01<00:03,  9.75it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  24%|██▍       | 12/49 [00:01<00:04,  8.69it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  27%|██▋       | 13/49 [00:01<00:03,  9.06it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  29%|██▊       | 14/49 [00:01<00:03,  9.38it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  31%|███       | 15/49 [00:01<00:03,  9.46it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  33%|███▎      | 16/49 [00:01<00:03,  9.53it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  35%|███▍      | 17/49 [00:01<00:03,  8.78it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  37%|███▋      | 18/49 [00:02<00:03,  8.92it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  39%|███▉      | 19/49 [00:02<00:03,  9.08it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  41%|████      | 20/49 [00:02<00:03,  9.32it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  43%|████▎     | 21/49 [00:02<00:02,  9.55it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  45%|████▍     | 22/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  47%|████▋     | 23/49 [00:02<00:02,  9.04it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  49%|████▉     | 24/49 [00:02<00:02,  9.15it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  51%|█████     | 25/49 [00:02<00:02,  9.28it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  53%|█████▎    | 26/49 [00:02<00:02,  9.46it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  55%|█████▌    | 27/49 [00:02<00:02,  9.63it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  57%|█████▋    | 28/49 [00:03<00:02,  9.11it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  59%|█████▉    | 29/49 [00:03<00:02,  9.27it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  61%|██████    | 30/49 [00:03<00:02,  9.43it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  63%|██████▎   | 31/49 [00:03<00:01,  9.57it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  65%|██████▌   | 32/49 [00:03<00:01,  9.71it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  67%|██████▋   | 33/49 [00:03<00:01,  9.23it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  69%|██████▉   | 34/49 [00:03<00:01,  9.37it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  71%|███████▏  | 35/49 [00:03<00:01,  9.50it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  73%|███████▎  | 36/49 [00:03<00:01,  9.63it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  76%|███████▌  | 37/49 [00:03<00:01,  9.76it/s, v_num=crps, train_loss_step=1.160, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  78%|███████▊  | 38/49 [00:04<00:01,  9.33it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  80%|███████▉  | 39/49 [00:04<00:01,  9.45it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  82%|████████▏ | 40/49 [00:04<00:00,  9.56it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  84%|████████▎ | 41/49 [00:04<00:00,  9.68it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  86%|████████▌ | 42/49 [00:04<00:00,  9.79it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  88%|████████▊ | 43/49 [00:04<00:00,  9.90it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  90%|████████▉ | 44/49 [00:04<00:00,  9.56it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  92%|█████████▏| 45/49 [00:04<00:00,  9.61it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  94%|█████████▍| 46/49 [00:04<00:00,  9.70it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  96%|█████████▌| 47/49 [00:04<00:00,  9.73it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 11:  98%|█████████▊| 48/49 [00:04<00:00,  9.75it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 12:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   2%|▏         | 1/49 [00:00<00:03, 12.49it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   4%|▍         | 2/49 [00:00<00:03, 12.56it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   6%|▌         | 3/49 [00:00<00:03, 12.36it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:   8%|▊         | 4/49 [00:00<00:03, 12.54it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  10%|█         | 5/49 [00:00<00:03, 13.16it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  12%|█▏        | 6/49 [00:00<00:04,  9.42it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  14%|█▍        | 7/49 [00:00<00:04, 10.11it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  16%|█▋        | 8/49 [00:00<00:03, 10.72it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  18%|█▊        | 9/49 [00:00<00:03, 11.24it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  20%|██        | 10/49 [00:00<00:03, 11.67it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  22%|██▏       | 11/49 [00:01<00:03,  9.74it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  24%|██▍       | 12/49 [00:01<00:03, 10.13it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  27%|██▋       | 13/49 [00:01<00:03, 10.50it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  29%|██▊       | 14/49 [00:01<00:03, 10.82it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  31%|███       | 15/49 [00:01<00:03, 10.91it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  33%|███▎      | 16/49 [00:01<00:03,  9.56it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  35%|███▍      | 17/49 [00:01<00:03,  9.84it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  37%|███▋      | 18/49 [00:01<00:03,  9.95it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  39%|███▉      | 19/49 [00:01<00:02, 10.07it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  41%|████      | 20/49 [00:01<00:02, 10.12it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  43%|████▎     | 21/49 [00:02<00:02, 10.27it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  45%|████▍     | 22/49 [00:02<00:02,  9.46it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  47%|████▋     | 23/49 [00:02<00:02,  9.66it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  49%|████▉     | 24/49 [00:02<00:02,  9.85it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  51%|█████     | 25/49 [00:02<00:02, 10.04it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  53%|█████▎    | 26/49 [00:02<00:02, 10.22it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  55%|█████▌    | 27/49 [00:02<00:02,  9.65it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  57%|█████▋    | 28/49 [00:02<00:02,  9.81it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  59%|█████▉    | 29/49 [00:02<00:02,  9.95it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  61%|██████    | 30/49 [00:02<00:01, 10.05it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  63%|██████▎   | 31/49 [00:03<00:01, 10.19it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  65%|██████▌   | 32/49 [00:03<00:01,  9.62it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  67%|██████▋   | 33/49 [00:03<00:01,  9.76it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  69%|██████▉   | 34/49 [00:03<00:01,  9.89it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  71%|███████▏  | 35/49 [00:03<00:01, 10.02it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  73%|███████▎  | 36/49 [00:03<00:01, 10.15it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  76%|███████▌  | 37/49 [00:03<00:01, 10.27it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  78%|███████▊  | 38/49 [00:03<00:01,  9.81it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  80%|███████▉  | 39/49 [00:03<00:01,  9.91it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  82%|████████▏ | 40/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  84%|████████▎ | 41/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  86%|████████▌ | 42/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  88%|████████▊ | 43/49 [00:04<00:00,  9.63it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  90%|████████▉ | 44/49 [00:04<00:00,  9.64it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  92%|█████████▏| 45/49 [00:04<00:00,  9.73it/s, v_num=crps, train_loss_step=1.170, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  94%|█████████▍| 46/49 [00:04<00:00,  9.77it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  96%|█████████▌| 47/49 [00:04<00:00,  9.78it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.140]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 12:  98%|█████████▊| 48/49 [00:05<00:00,  9.46it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.140]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 13:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   2%|▏         | 1/49 [00:00<00:04,  9.93it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   4%|▍         | 2/49 [00:00<00:03, 12.87it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   6%|▌         | 3/49 [00:00<00:03, 11.86it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:   8%|▊         | 4/49 [00:00<00:03, 12.27it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  10%|█         | 5/49 [00:00<00:05,  8.61it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  12%|█▏        | 6/49 [00:00<00:04,  9.45it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  14%|█▍        | 7/49 [00:00<00:04, 10.15it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  16%|█▋        | 8/49 [00:00<00:03, 10.54it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  18%|█▊        | 9/49 [00:00<00:03, 11.05it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  20%|██        | 10/49 [00:00<00:03, 11.51it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  22%|██▏       | 11/49 [00:01<00:03,  9.83it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  24%|██▍       | 12/49 [00:01<00:03, 10.23it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  27%|██▋       | 13/49 [00:01<00:03, 10.55it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  29%|██▊       | 14/49 [00:01<00:03, 10.86it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  31%|███       | 15/49 [00:01<00:03, 11.17it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  33%|███▎      | 16/49 [00:01<00:03, 10.03it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  35%|███▍      | 17/49 [00:01<00:03, 10.30it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  37%|███▋      | 18/49 [00:01<00:02, 10.57it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  39%|███▉      | 19/49 [00:01<00:02, 10.81it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  41%|████      | 20/49 [00:01<00:02, 10.83it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  43%|████▎     | 21/49 [00:01<00:02, 11.02it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  45%|████▍     | 22/49 [00:02<00:02, 10.20it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  47%|████▋     | 23/49 [00:02<00:02, 10.39it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  49%|████▉     | 24/49 [00:02<00:02, 10.39it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  51%|█████     | 25/49 [00:02<00:02, 10.56it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  53%|█████▎    | 26/49 [00:02<00:02, 10.74it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  55%|█████▌    | 27/49 [00:02<00:02, 10.10it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  57%|█████▋    | 28/49 [00:02<00:02, 10.26it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  59%|█████▉    | 29/49 [00:02<00:01, 10.36it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  61%|██████    | 30/49 [00:02<00:01, 10.41it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  63%|██████▎   | 31/49 [00:02<00:01, 10.46it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  65%|██████▌   | 32/49 [00:03<00:01,  9.85it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  67%|██████▋   | 33/49 [00:03<00:01,  9.89it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  69%|██████▉   | 34/49 [00:03<00:01, 10.02it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  71%|███████▏  | 35/49 [00:03<00:01, 10.07it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  73%|███████▎  | 36/49 [00:03<00:01, 10.19it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  76%|███████▌  | 37/49 [00:03<00:01, 10.32it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  78%|███████▊  | 38/49 [00:03<00:01,  9.90it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  80%|███████▉  | 39/49 [00:03<00:00, 10.02it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  82%|████████▏ | 40/49 [00:03<00:00, 10.14it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  84%|████████▎ | 41/49 [00:04<00:00, 10.25it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  86%|████████▌ | 42/49 [00:04<00:00, 10.24it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  88%|████████▊ | 43/49 [00:04<00:00,  9.82it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  90%|████████▉ | 44/49 [00:04<00:00,  9.89it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  92%|█████████▏| 45/49 [00:04<00:00,  9.98it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  94%|█████████▍| 46/49 [00:04<00:00, 10.07it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  96%|█████████▌| 47/49 [00:04<00:00, 10.17it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 13:  98%|█████████▊| 48/49 [00:04<00:00,  9.85it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 14:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   2%|▏         | 1/49 [00:00<00:03, 12.83it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   4%|▍         | 2/49 [00:00<00:03, 13.44it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   6%|▌         | 3/49 [00:00<00:03, 14.56it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:   8%|▊         | 4/49 [00:00<00:03, 13.36it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  10%|█         | 5/49 [00:00<00:05,  8.68it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  12%|█▏        | 6/49 [00:00<00:04,  9.53it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  14%|█▍        | 7/49 [00:00<00:04, 10.24it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  16%|█▋        | 8/49 [00:00<00:03, 10.84it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  18%|█▊        | 9/49 [00:00<00:03, 11.36it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  20%|██        | 10/49 [00:01<00:04,  9.56it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  22%|██▏       | 11/49 [00:01<00:03, 10.00it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  24%|██▍       | 12/49 [00:01<00:03, 10.40it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  27%|██▋       | 13/49 [00:01<00:03, 10.76it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  29%|██▊       | 14/49 [00:01<00:03, 11.09it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  31%|███       | 15/49 [00:01<00:02, 11.39it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  33%|███▎      | 16/49 [00:01<00:03, 10.08it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  35%|███▍      | 17/49 [00:01<00:03, 10.36it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  37%|███▋      | 18/49 [00:01<00:02, 10.62it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  39%|███▉      | 19/49 [00:01<00:02, 10.86it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  41%|████      | 20/49 [00:01<00:02, 11.09it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  43%|████▎     | 21/49 [00:02<00:02, 10.23it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  45%|████▍     | 22/49 [00:02<00:02, 10.45it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  47%|████▋     | 23/49 [00:02<00:02, 10.64it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  49%|████▉     | 24/49 [00:02<00:02, 10.84it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  51%|█████     | 25/49 [00:02<00:02, 11.02it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  53%|█████▎    | 26/49 [00:02<00:02, 10.31it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  55%|█████▌    | 27/49 [00:02<00:02, 10.40it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  57%|█████▋    | 28/49 [00:02<00:02, 10.50it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  59%|█████▉    | 29/49 [00:02<00:01, 10.59it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  61%|██████    | 30/49 [00:02<00:01, 10.66it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  63%|██████▎   | 31/49 [00:02<00:01, 10.78it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  65%|██████▌   | 32/49 [00:03<00:01, 10.17it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  67%|██████▋   | 33/49 [00:03<00:01, 10.29it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  69%|██████▉   | 34/49 [00:03<00:01, 10.43it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  71%|███████▏  | 35/49 [00:03<00:01, 10.55it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  73%|███████▎  | 36/49 [00:03<00:01, 10.67it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  76%|███████▌  | 37/49 [00:03<00:01, 10.20it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  78%|███████▊  | 38/49 [00:03<00:01, 10.32it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  80%|███████▉  | 39/49 [00:03<00:00, 10.35it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  82%|████████▏ | 40/49 [00:03<00:00, 10.39it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  84%|████████▎ | 41/49 [00:03<00:00, 10.49it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  86%|████████▌ | 42/49 [00:04<00:00, 10.08it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  88%|████████▊ | 43/49 [00:04<00:00, 10.12it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  90%|████████▉ | 44/49 [00:04<00:00, 10.16it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  92%|█████████▏| 45/49 [00:04<00:00, 10.26it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  94%|█████████▍| 46/49 [00:04<00:00, 10.29it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  96%|█████████▌| 47/49 [00:04<00:00, 10.38it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.130]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 14:  98%|█████████▊| 48/49 [00:04<00:00,  9.97it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.130]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 15:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   2%|▏         | 1/49 [00:00<00:03, 12.72it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   4%|▍         | 2/49 [00:00<00:03, 15.08it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   6%|▌         | 3/49 [00:00<00:02, 15.91it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:   8%|▊         | 4/49 [00:00<00:04,  9.07it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  10%|█         | 5/49 [00:00<00:04, 10.10it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  12%|█▏        | 6/49 [00:00<00:03, 10.89it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  14%|█▍        | 7/49 [00:00<00:03, 11.57it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  16%|█▋        | 8/49 [00:00<00:03, 12.13it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  18%|█▊        | 9/49 [00:00<00:03, 12.61it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  20%|██        | 10/49 [00:00<00:03, 10.37it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  22%|██▏       | 11/49 [00:01<00:03, 10.75it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  24%|██▍       | 12/49 [00:01<00:03, 10.89it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  27%|██▋       | 13/49 [00:01<00:03, 11.08it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  29%|██▊       | 14/49 [00:01<00:03, 11.41it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  31%|███       | 15/49 [00:01<00:03, 10.03it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  33%|███▎      | 16/49 [00:01<00:03, 10.21it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  35%|███▍      | 17/49 [00:01<00:03, 10.48it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  37%|███▋      | 18/49 [00:01<00:02, 10.74it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  39%|███▉      | 19/49 [00:01<00:02, 10.98it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  41%|████      | 20/49 [00:01<00:02, 10.06it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  43%|████▎     | 21/49 [00:02<00:02, 10.21it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  45%|████▍     | 22/49 [00:02<00:02, 10.42it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  47%|████▋     | 23/49 [00:02<00:02, 10.62it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  49%|████▉     | 24/49 [00:02<00:02, 10.81it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  51%|█████     | 25/49 [00:02<00:02, 11.00it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  53%|█████▎    | 26/49 [00:02<00:02, 10.30it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  55%|█████▌    | 27/49 [00:02<00:02, 10.48it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  57%|█████▋    | 28/49 [00:02<00:01, 10.64it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  59%|█████▉    | 29/49 [00:02<00:01, 10.79it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  61%|██████    | 30/49 [00:02<00:01, 10.94it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  63%|██████▎   | 31/49 [00:03<00:01, 10.31it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  65%|██████▌   | 32/49 [00:03<00:01, 10.39it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  67%|██████▋   | 33/49 [00:03<00:01, 10.52it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  69%|██████▉   | 34/49 [00:03<00:01, 10.65it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  71%|███████▏  | 35/49 [00:03<00:01, 10.67it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  73%|███████▎  | 36/49 [00:03<00:01, 10.16it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  76%|███████▌  | 37/49 [00:03<00:01, 10.29it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  78%|███████▊  | 38/49 [00:03<00:01, 10.40it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  80%|███████▉  | 39/49 [00:03<00:00, 10.52it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  82%|████████▏ | 40/49 [00:03<00:00, 10.63it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  84%|████████▎ | 41/49 [00:03<00:00, 10.63it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  86%|████████▌ | 42/49 [00:04<00:00, 10.21it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  88%|████████▊ | 43/49 [00:04<00:00, 10.31it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  90%|████████▉ | 44/49 [00:04<00:00, 10.41it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  92%|█████████▏| 45/49 [00:04<00:00, 10.52it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  94%|█████████▍| 46/49 [00:04<00:00, 10.62it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  96%|█████████▌| 47/49 [00:04<00:00, 10.25it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 15:  98%|█████████▊| 48/49 [00:04<00:00, 10.34it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 16:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   2%|▏         | 1/49 [00:00<00:04, 10.83it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   4%|▍         | 2/49 [00:00<00:03, 12.14it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   6%|▌         | 3/49 [00:00<00:03, 13.64it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:   8%|▊         | 4/49 [00:00<00:05,  8.51it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  10%|█         | 5/49 [00:00<00:04,  9.52it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  12%|█▏        | 6/49 [00:00<00:04, 10.37it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  14%|█▍        | 7/49 [00:00<00:03, 11.08it/s, v_num=crps, train_loss_step=1.150, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  16%|█▋        | 8/49 [00:00<00:03, 11.66it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  18%|█▊        | 9/49 [00:00<00:04,  9.57it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  20%|██        | 10/49 [00:00<00:03, 10.06it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  22%|██▏       | 11/49 [00:01<00:03, 10.51it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  24%|██▍       | 12/49 [00:01<00:03, 10.90it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  27%|██▋       | 13/49 [00:01<00:03, 11.25it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  29%|██▊       | 14/49 [00:01<00:03,  9.95it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  31%|███       | 15/49 [00:01<00:03, 10.25it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  33%|███▎      | 16/49 [00:01<00:03, 10.55it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  35%|███▍      | 17/49 [00:01<00:02, 10.81it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  37%|███▋      | 18/49 [00:01<00:02, 11.05it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  39%|███▉      | 19/49 [00:01<00:02, 11.27it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  41%|████      | 20/49 [00:01<00:02, 10.24it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  43%|████▎     | 21/49 [00:02<00:02, 10.38it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  45%|████▍     | 22/49 [00:02<00:02, 10.59it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  47%|████▋     | 23/49 [00:02<00:02, 10.79it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  49%|████▉     | 24/49 [00:02<00:02, 10.98it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  51%|█████     | 25/49 [00:02<00:02, 10.26it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  53%|█████▎    | 26/49 [00:02<00:02, 10.43it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  55%|█████▌    | 27/49 [00:02<00:02, 10.60it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  57%|█████▋    | 28/49 [00:02<00:01, 10.75it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  59%|█████▉    | 29/49 [00:02<00:01, 10.91it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  61%|██████    | 30/49 [00:02<00:01, 10.23it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  63%|██████▎   | 31/49 [00:03<00:01, 10.26it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  65%|██████▌   | 32/49 [00:03<00:01, 10.37it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  67%|██████▋   | 33/49 [00:03<00:01, 10.42it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  69%|██████▉   | 34/49 [00:03<00:01, 10.55it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  71%|███████▏  | 35/49 [00:03<00:01, 10.67it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  73%|███████▎  | 36/49 [00:03<00:01, 10.21it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  76%|███████▌  | 37/49 [00:03<00:01, 10.33it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  78%|███████▊  | 38/49 [00:03<00:01, 10.40it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  80%|███████▉  | 39/49 [00:03<00:00, 10.47it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  82%|████████▏ | 40/49 [00:03<00:00, 10.58it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  84%|████████▎ | 41/49 [00:04<00:00, 10.16it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  86%|████████▌ | 42/49 [00:04<00:00, 10.26it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  88%|████████▊ | 43/49 [00:04<00:00, 10.32it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  90%|████████▉ | 44/49 [00:04<00:00, 10.42it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  92%|█████████▏| 45/49 [00:04<00:00, 10.53it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  94%|█████████▍| 46/49 [00:04<00:00, 10.62it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  96%|█████████▌| 47/49 [00:04<00:00, 10.23it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.120]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 16:  98%|█████████▊| 48/49 [00:04<00:00, 10.33it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.120]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 17:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   2%|▏         | 1/49 [00:00<00:05,  9.36it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   4%|▍         | 2/49 [00:00<00:04, 10.78it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   6%|▌         | 3/49 [00:00<00:06,  6.63it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:   8%|▊         | 4/49 [00:00<00:06,  7.31it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  10%|█         | 5/49 [00:00<00:05,  8.31it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  12%|█▏        | 6/49 [00:00<00:04,  9.13it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  14%|█▍        | 7/49 [00:00<00:04,  9.83it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  16%|█▋        | 8/49 [00:00<00:04,  8.27it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  18%|█▊        | 9/49 [00:01<00:04,  8.81it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  20%|██        | 10/49 [00:01<00:04,  9.28it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  22%|██▏       | 11/49 [00:01<00:03,  9.73it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  24%|██▍       | 12/49 [00:01<00:03, 10.11it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  27%|██▋       | 13/49 [00:01<00:03, 10.33it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  29%|██▊       | 14/49 [00:01<00:03,  9.28it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  31%|███       | 15/49 [00:01<00:03,  9.60it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  33%|███▎      | 16/49 [00:01<00:03,  9.66it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  35%|███▍      | 17/49 [00:01<00:03,  9.93it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  37%|███▋      | 18/49 [00:01<00:03, 10.19it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  39%|███▉      | 19/49 [00:02<00:03,  9.23it/s, v_num=crps, train_loss_step=1.130, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  41%|████      | 20/49 [00:02<00:03,  9.47it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  43%|████▎     | 21/49 [00:02<00:02,  9.69it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  45%|████▍     | 22/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  47%|████▋     | 23/49 [00:02<00:02, 10.10it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  49%|████▉     | 24/49 [00:02<00:02,  9.48it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  51%|█████     | 25/49 [00:02<00:02,  9.66it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  53%|█████▎    | 26/49 [00:02<00:02,  9.84it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  55%|█████▌    | 27/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  57%|█████▋    | 28/49 [00:02<00:02, 10.17it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  59%|█████▉    | 29/49 [00:02<00:01, 10.33it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  61%|██████    | 30/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  63%|██████▎   | 31/49 [00:03<00:01,  9.95it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  65%|██████▌   | 32/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  67%|██████▋   | 33/49 [00:03<00:01, 10.23it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  69%|██████▉   | 34/49 [00:03<00:01, 10.26it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  71%|███████▏  | 35/49 [00:03<00:01,  9.73it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  73%|███████▎  | 36/49 [00:03<00:01,  9.86it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  76%|███████▌  | 37/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  78%|███████▊  | 38/49 [00:03<00:01, 10.10it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  80%|███████▉  | 39/49 [00:03<00:00, 10.22it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  82%|████████▏ | 40/49 [00:04<00:00,  9.82it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  84%|████████▎ | 41/49 [00:04<00:00,  9.94it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  86%|████████▌ | 42/49 [00:04<00:00, 10.05it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  88%|████████▊ | 43/49 [00:04<00:00, 10.15it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  90%|████████▉ | 44/49 [00:04<00:00, 10.26it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  92%|█████████▏| 45/49 [00:04<00:00,  9.90it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  94%|█████████▍| 46/49 [00:04<00:00, 10.00it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  96%|█████████▌| 47/49 [00:04<00:00, 10.10it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 17:  98%|█████████▊| 48/49 [00:04<00:00, 10.19it/s, v_num=crps, train_loss_step=1.140, train_loss_epoch=1.110]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 18:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   2%|▏         | 1/49 [00:00<00:03, 13.33it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   4%|▍         | 2/49 [00:00<00:07,  6.09it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   6%|▌         | 3/49 [00:00<00:05,  7.83it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:   8%|▊         | 4/49 [00:00<00:04,  9.14it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  10%|█         | 5/49 [00:00<00:04, 10.13it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  12%|█▏        | 6/49 [00:00<00:03, 10.97it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  14%|█▍        | 7/49 [00:00<00:04,  8.72it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  16%|█▋        | 8/49 [00:00<00:04,  9.33it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  18%|█▊        | 9/49 [00:00<00:04,  9.87it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  20%|██        | 10/49 [00:00<00:03, 10.34it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  22%|██▏       | 11/49 [00:01<00:03, 10.77it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  24%|██▍       | 12/49 [00:01<00:03, 11.16it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  27%|██▋       | 13/49 [00:01<00:03,  9.78it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  29%|██▊       | 14/49 [00:01<00:03, 10.08it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  31%|███       | 15/49 [00:01<00:03, 10.12it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  33%|███▎      | 16/49 [00:01<00:03, 10.29it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  35%|███▍      | 17/49 [00:01<00:03, 10.56it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  37%|███▋      | 18/49 [00:01<00:03,  9.62it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  39%|███▉      | 19/49 [00:01<00:03,  9.69it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  41%|████      | 20/49 [00:02<00:02,  9.92it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  43%|████▎     | 21/49 [00:02<00:02, 10.15it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  45%|████▍     | 22/49 [00:02<00:02, 10.35it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  47%|████▋     | 23/49 [00:02<00:02,  9.66it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  49%|████▉     | 24/49 [00:02<00:02,  9.71it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  51%|█████     | 25/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  53%|█████▎    | 26/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  55%|█████▌    | 27/49 [00:02<00:02, 10.18it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  57%|█████▋    | 28/49 [00:02<00:02, 10.27it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  59%|█████▉    | 29/49 [00:02<00:02,  9.75it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  61%|██████    | 30/49 [00:03<00:01,  9.91it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  63%|██████▎   | 31/49 [00:03<00:01, 10.05it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  65%|██████▌   | 32/49 [00:03<00:01, 10.20it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  67%|██████▋   | 33/49 [00:03<00:01, 10.34it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  69%|██████▉   | 34/49 [00:03<00:01,  9.87it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  71%|███████▏  | 35/49 [00:03<00:01, 10.00it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  73%|███████▎  | 36/49 [00:03<00:01, 10.13it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  76%|███████▌  | 37/49 [00:03<00:01, 10.25it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  78%|███████▊  | 38/49 [00:03<00:01, 10.37it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  80%|███████▉  | 39/49 [00:03<00:01,  9.96it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  82%|████████▏ | 40/49 [00:03<00:00, 10.07it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  84%|████████▎ | 41/49 [00:04<00:00, 10.19it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  86%|████████▌ | 42/49 [00:04<00:00, 10.30it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  88%|████████▊ | 43/49 [00:04<00:00, 10.40it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  90%|████████▉ | 44/49 [00:04<00:00, 10.51it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  92%|█████████▏| 45/49 [00:04<00:00, 10.14it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  94%|█████████▍| 46/49 [00:04<00:00, 10.20it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  96%|█████████▌| 47/49 [00:04<00:00, 10.30it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 18:  98%|█████████▊| 48/49 [00:04<00:00, 10.40it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 19:   0%|          | 0/49 [00:00<?, ?it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]         torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   2%|▏         | 1/49 [00:00<00:13,  3.64it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   4%|▍         | 2/49 [00:00<00:07,  6.07it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   6%|▌         | 3/49 [00:00<00:05,  7.82it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:   8%|▊         | 4/49 [00:00<00:04,  9.14it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  10%|█         | 5/49 [00:00<00:04, 10.14it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  12%|█▏        | 6/49 [00:00<00:03, 10.98it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  14%|█▍        | 7/49 [00:00<00:04,  8.77it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  16%|█▋        | 8/49 [00:00<00:04,  9.33it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  18%|█▊        | 9/49 [00:00<00:04,  9.87it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  20%|██        | 10/49 [00:00<00:03, 10.33it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  22%|██▏       | 11/49 [00:01<00:03, 10.75it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  24%|██▍       | 12/49 [00:01<00:03,  9.41it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  27%|██▋       | 13/49 [00:01<00:03,  9.77it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  29%|██▊       | 14/49 [00:01<00:03, 10.11it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  31%|███       | 15/49 [00:01<00:03, 10.40it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  33%|███▎      | 16/49 [00:01<00:03, 10.69it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  35%|███▍      | 17/49 [00:01<00:03,  9.70it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  37%|███▋      | 18/49 [00:01<00:03,  9.97it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  39%|███▉      | 19/49 [00:01<00:02, 10.21it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  41%|████      | 20/49 [00:01<00:02, 10.44it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  43%|████▎     | 21/49 [00:01<00:02, 10.55it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  45%|████▍     | 22/49 [00:02<00:02, 10.61it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  47%|████▋     | 23/49 [00:02<00:02,  9.80it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  49%|████▉     | 24/49 [00:02<00:02,  9.90it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  51%|█████     | 25/49 [00:02<00:02, 10.01it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  53%|█████▎    | 26/49 [00:02<00:02, 10.18it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  55%|█████▌    | 27/49 [00:02<00:02, 10.21it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  57%|█████▋    | 28/49 [00:02<00:02,  9.67it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  59%|█████▉    | 29/49 [00:02<00:02,  9.83it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  61%|██████    | 30/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  63%|██████▎   | 31/49 [00:03<00:01, 10.13it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  65%|██████▌   | 32/49 [00:03<00:01, 10.27it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  67%|██████▋   | 33/49 [00:03<00:01,  9.80it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  69%|██████▉   | 34/49 [00:03<00:01,  9.89it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  71%|███████▏  | 35/49 [00:03<00:01,  9.98it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  73%|███████▎  | 36/49 [00:03<00:01, 10.09it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  76%|███████▌  | 37/49 [00:03<00:01, 10.15it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  78%|███████▊  | 38/49 [00:03<00:01, 10.24it/s, v_num=crps, train_loss_step=1.080, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  80%|███████▉  | 39/49 [00:03<00:01,  9.79it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  82%|████████▏ | 40/49 [00:04<00:00,  9.91it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  84%|████████▎ | 41/49 [00:04<00:00, 10.02it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  86%|████████▌ | 42/49 [00:04<00:00, 10.12it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  88%|████████▊ | 43/49 [00:04<00:00, 10.23it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  90%|████████▉ | 44/49 [00:04<00:00,  9.87it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  92%|█████████▏| 45/49 [00:04<00:00,  9.96it/s, v_num=crps, train_loss_step=1.090, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  94%|█████████▍| 46/49 [00:04<00:00, 10.02it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  96%|█████████▌| 47/49 [00:04<00:00, 10.08it/s, v_num=crps, train_loss_step=1.110, train_loss_epoch=1.110]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Epoch 19:  98%|█████████▊| 48/49 [00:04<00:00, 10.14it/s, v_num=crps, train_loss_step=1.120, train_loss_epoch=1.110]torch.Size([5625, 512])\n",
      "torch.Size([5625, 512])\n",
      "Epoch 19: 100%|██████████| 49/49 [00:04<00:00, 10.21it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 49/49 [00:04<00:00, 10.20it/s, v_num=crps, train_loss_step=1.100, train_loss_epoch=1.100]\n",
      "Final MSE Loss: tensor(1.1014)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss_epoch</td><td>1.10362</td></tr><tr><td>train_loss_step</td><td>1.10145</td></tr><tr><td>trainer/global_step</td><td>979</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_120h_crps</strong> at: <a href='https://wandb.ai/leachen_thesis/reproduction/runs/training_run_120h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction/runs/training_run_120h_crps</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/reproduction' target=\"_blank\">https://wandb.ai/leachen_thesis/reproduction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250314_171903-training_run_120h_crps/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluate",
   "id": "bd48f1e070729938"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:24:29.187827Z",
     "start_time": "2025-03-14T16:24:28.247419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NN object, train, test\n",
    "test_rf_dataset = TensorDataset(torch.Tensor(test_rf[0].to_numpy()),\n",
    "                                torch.Tensor(y_scaler.transform(test_rf[1][[\"t2m\"]])))\n",
    "test_rf_loader = DataLoader(test_rf_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "test_f_dataset = TensorDataset(torch.Tensor(test_f[0].to_numpy()), torch.Tensor(y_scaler.transform(test_f[1][[\"t2m\"]])))\n",
    "test_f_loader = DataLoader(test_f_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    log_every_n_steps=1, accelerator=\"gpu\", enable_progress_bar=True, enable_model_summary=False\n",
    ")\n",
    "preds_list = []\n",
    "\n",
    "if DATASET == \"f\":\n",
    "    targets = test_f[1] # R2F\n",
    "    preds = trainer.predict(model=mydrn, dataloaders=test_f_loader) #R2F\n",
    "    print(\"test_rf[1]:\")\n",
    "    print(targets)\n",
    "if DATASET == \"rf\":\n",
    "    targets = test_rf[1]\n",
    "    preds = trainer.predict(model=mydrn, dataloaders=test_rf_loader)\n",
    "    print(\"test_rf[1]:\")\n",
    "    print(targets)\n",
    "\n",
    "preds = torch.cat(preds, dim=0)\n",
    "# Reverse transform of the y_scaler (only on the mean)\n",
    "preds[:, 0] = torch.Tensor(y_scaler.inverse_transform(preds[:, 0].view(-1, 1))).flatten()\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = torch.Tensor(targets.t2m.values)\n",
    "print(\"t2m values:\")\n",
    "print(targets)\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "print(\"final_preds\")\n",
    "print(final_preds)\n",
    "\n",
    "res = mydrn.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n"
   ],
   "id": "37b0e09b41535ec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   0%|          | 0/11 [00:00<?, ?it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:   9%|▉         | 1/11 [00:00<00:00, 280.99it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  18%|█▊        | 2/11 [00:00<00:00, 36.70it/s] torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  27%|██▋       | 3/11 [00:00<00:00, 30.79it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  36%|███▋      | 4/11 [00:00<00:00, 29.44it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  45%|████▌     | 5/11 [00:00<00:00, 28.57it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  55%|█████▍    | 6/11 [00:00<00:00, 14.65it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  64%|██████▎   | 7/11 [00:00<00:00, 15.71it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  73%|███████▎  | 8/11 [00:00<00:00, 16.62it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  82%|████████▏ | 9/11 [00:00<00:00, 17.41it/s]torch.Size([8192, 512])\n",
      "torch.Size([8192, 512])\n",
      "Predicting DataLoader 0:  91%|█████████ | 10/11 [00:00<00:00, 18.09it/s]torch.Size([4822, 512])\n",
      "torch.Size([4822, 512])\n",
      "Predicting DataLoader 0: 100%|██████████| 11/11 [00:00<00:00, 19.10it/s]\n",
      "test_rf[1]:\n",
      "            time  station_id     t2m\n",
      "0     2017-01-01           0  270.95\n",
      "1     2017-01-01           1  269.65\n",
      "2     2017-01-01           2  272.25\n",
      "3     2017-01-01           3  271.85\n",
      "4     2017-01-01           4  268.05\n",
      "...          ...         ...     ...\n",
      "89055 2018-12-31         117  273.45\n",
      "89056 2018-12-31         118  273.55\n",
      "89057 2018-12-31         119  272.65\n",
      "89058 2018-12-31         120  266.95\n",
      "89059 2018-12-31         121  266.45\n",
      "\n",
      "[86742 rows x 3 columns]\n",
      "t2m values:\n",
      "tensor([270.9500, 269.6500, 272.2500,  ..., 272.6500, 266.9500, 266.4500])\n",
      "final_preds\n",
      "tensor([[275.1203,   3.0417],\n",
      "        [273.8546,   3.0174],\n",
      "        [275.6912,   2.4700],\n",
      "        ...,\n",
      "        [271.2552,   2.3878],\n",
      "        [264.9698,   2.9461],\n",
      "        [264.7246,   3.0584]])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 1.099965214729309\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:24:34.609723Z",
     "start_time": "2025-03-14T16:24:34.380095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "print(df)\n",
    "print(os.path.join(SAVEPATH, f\"{DATASET}_results.csv\"))\n",
    "df.to_csv(os.path.join(SAVEPATH, f\"{DATASET}_results.csv\"), index=False)"
   ],
   "id": "922fb442e2a4d1fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              t2m          mu     sigma\n",
      "0      270.950012  275.120300  3.041687\n",
      "1      269.649994  273.854645  3.017355\n",
      "2      272.250000  275.691223  2.470048\n",
      "3      271.850006  275.295532  2.836039\n",
      "4      268.049988  272.083435  3.203675\n",
      "...           ...         ...       ...\n",
      "86737  273.450012  272.920441  2.444381\n",
      "86738  273.549988  272.733582  1.970414\n",
      "86739  272.649994  271.255188  2.387770\n",
      "86740  266.950012  264.969849  2.946098\n",
      "86741  266.450012  264.724640  3.058407\n",
      "\n",
      "[86742 rows x 3 columns]\n",
      "/home/ltchen/gnnpp/explored_models/drn_120h/models/f_results.csv\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plot PIT Histogram",
   "id": "420aa43f7dee0ebc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:24:42.070381Z",
     "start_time": "2025-03-14T16:24:42.025524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT\n",
    "predictions = pd.read_csv(os.path.join(SAVEPATH, f\"{DATASET}_results.csv\"))\n",
    "predictions = predictions.dropna(axis=0)\n",
    "\n",
    "y = torch.tensor(predictions[\"t2m\"].to_numpy())\n",
    "preds = torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy())\n",
    "\n",
    "print(SAVEPATH)"
   ],
   "id": "897f0d90fb1fd942",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp/explored_models/drn_120h/models\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:24:53.819671Z",
     "start_time": "2025-03-14T16:24:53.795465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = predictions[\"t2m\"].to_numpy()\n",
    "print(y)\n",
    "print(y.shape)\n",
    "mu = predictions[\"mu\"].to_numpy()\n",
    "print(mu)\n",
    "print(mu.shape)\n",
    "sigma = predictions[\"sigma\"].to_numpy()\n",
    "print(sigma)\n",
    "print(sigma.shape)\n",
    "\n",
    "normalCRPS = NormalCRPS()\n",
    "\n",
    "err = normalCRPS.crps(mu_sigma=torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy()), y=torch.tensor(predictions[\"t2m\"].to_numpy())).item()\n",
    "err"
   ],
   "id": "409c193845c7f896",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[270.95 269.65 272.25 ... 272.65 266.95 266.45]\n",
      "(86742,)\n",
      "[275.1203  273.85464 275.69122 ... 271.2552  264.96985 264.72464]\n",
      "(86742,)\n",
      "[3.0416873 3.017355  2.4700477 ... 2.38777   2.9460976 3.0584073]\n",
      "(86742,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0999652056797955"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T16:24:57.076611Z",
     "start_time": "2025-03-14T16:24:56.649677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT for 120h\n",
    "probs = norm.cdf(y.flatten(), loc=mu.flatten(), scale=sigma.flatten())  # scale is standard deviation\n",
    "n, bins, patches = plt.hist(probs, bins=15, density=True)\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= (max(col))\n",
    "\n",
    "plt.ylim(0,1.5)  # Layout\n",
    "plt.hlines(xmin=0,xmax=1, y=1, colors=\"black\", linestyles=\"--\")"
   ],
   "id": "d64113df5d75a6f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f8118969c30>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Rank Histogram for ensemble members",
   "id": "95654b1d2a7c45e0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:38:15.555360Z",
     "start_time": "2025-03-23T00:38:04.757277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# all ensemble members into bins - in Moritz BA\n",
    "MODEL = \"24h\"\n",
    "dataframes = load_dataframes(mode=\"train\",leadtime=MODEL)\n",
    "train_df = dataframes['train'][0]\n",
    "train_target_df = dataframes['train'][1]\n",
    "#train_df[(train_df['time']=='1997-01-02')& (train_df['station_id']==0)]['t2m'].to_list()\n",
    "#train_target_df[(train_target_df['time']=='1997-01-02') & (train_target_df['station_id']==0)]['t2m'].to_numpy()\n",
    "# for every date and every station (two loops t and s)\n",
    "df2 = train_df.groupby(['time', 'station_id'])['t2m'].apply(list).reset_index(name=\"ensembles\")\n",
    "print(df2.shape)\n",
    "print(train_target_df.shape)\n",
    "df2, train_target_df = drop_nans((df2, train_target_df))\n",
    "df2"
   ],
   "id": "f3d3a7a7e30a2a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "(420656, 3)\n",
      "(420656, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "             time  station_id  \\\n",
       "0      1997-01-02           0   \n",
       "1      1997-01-02           1   \n",
       "2      1997-01-02           2   \n",
       "3      1997-01-02           3   \n",
       "4      1997-01-02           4   \n",
       "...           ...         ...   \n",
       "420651 2013-12-31         117   \n",
       "420652 2013-12-31         118   \n",
       "420653 2013-12-31         119   \n",
       "420654 2013-12-31         120   \n",
       "420655 2013-12-31         121   \n",
       "\n",
       "                                                ensembles  \n",
       "0       [278.9286193847656, 279.224365234375, 279.4683...  \n",
       "1       [278.7372131347656, 279.326904296875, 279.8228...  \n",
       "2       [278.4276428222656, 278.579833984375, 278.9283...  \n",
       "3       [277.3172912597656, 277.435302734375, 277.7076...  \n",
       "4       [278.6678771972656, 278.930419921875, 279.5386...  \n",
       "...                                                   ...  \n",
       "420651  [279.50262451171875, 279.7480163574219, 279.18...  \n",
       "420652  [276.43719482421875, 276.0917663574219, 276.30...  \n",
       "420653  [273.90399169921875, 273.6044616699219, 273.27...  \n",
       "420654  [270.63641357421875, 269.9755554199219, 270.43...  \n",
       "420655  [268.78778076171875, 268.0204772949219, 267.77...  \n",
       "\n",
       "[398866 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>station_id</th>\n",
       "      <th>ensembles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>[278.9286193847656, 279.224365234375, 279.4683...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>[278.7372131347656, 279.326904296875, 279.8228...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>[278.4276428222656, 278.579833984375, 278.9283...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>[277.3172912597656, 277.435302734375, 277.7076...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>[278.6678771972656, 278.930419921875, 279.5386...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420651</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>117</td>\n",
       "      <td>[279.50262451171875, 279.7480163574219, 279.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420652</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>118</td>\n",
       "      <td>[276.43719482421875, 276.0917663574219, 276.30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420653</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>119</td>\n",
       "      <td>[273.90399169921875, 273.6044616699219, 273.27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420654</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>120</td>\n",
       "      <td>[270.63641357421875, 269.9755554199219, 270.43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420655</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>121</td>\n",
       "      <td>[268.78778076171875, 268.0204772949219, 267.77...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398866 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:38:39.478935Z",
     "start_time": "2025-03-23T00:38:35.959580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_rank(row):\n",
    "    combined_list = row['ensembles'] + [row['t2m']]\n",
    "    sorted_list = sorted(combined_list)\n",
    "    rank = sorted_list.index(row['t2m'])  # +1 to convert from 0-based to 1-based index\n",
    "    return rank\n",
    "\n",
    "print(df2.shape)\n",
    "print(train_target_df.shape)\n",
    "df3 = pd.merge(df2, train_target_df, on=['time', 'station_id'])\n",
    "df3['rank'] = df3.apply(calculate_rank, axis=1)\n",
    "df3['rank'].value_counts()"
   ],
   "id": "ed0db050203f2d59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398866, 3)\n",
      "(398866, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rank\n",
       "11    140434\n",
       "0      99280\n",
       "10     26856\n",
       "1      22065\n",
       "9      18218\n",
       "2      15778\n",
       "8      14721\n",
       "3      13244\n",
       "7      12630\n",
       "4      12133\n",
       "6      11782\n",
       "5      11725\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T00:48:37.986345Z",
     "start_time": "2025-03-23T00:48:37.799416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n, bins, patches = plt.hist((df3['rank']/12), edgecolor='black', density=True)\n",
    "plt.title('Histogram of Data')\n",
    "plt.xlabel('Data Points')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ],
   "id": "1d51eb61e6b8ec56",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T23:49:36.209248Z",
     "start_time": "2025-03-22T23:45:25.670963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ranks = []\n",
    "for t in train_df['time'].unique():\n",
    "    for s in train_df['station_id'].unique():\n",
    "        y = train_target_df[(train_target_df['time']==t) & (train_target_df['station_id']==s)]['t2m'].to_numpy()\n",
    "        ensembles = train_df[(train_df['time']==t)& (train_df['station_id']==s)]['t2m'].to_numpy()\n",
    "        if np.issubdtype(y.dtype, np.number):\n",
    "            all = np.concatenate([y, ensembles])\n",
    "            #print(all)\n",
    "            sorted_indices = np.argsort(all)\n",
    "            #print(sorted_indices)\n",
    "            rank = sorted_indices[0]\n",
    "            ranks.append(rank)\n",
    "        else:\n",
    "            print(f\"{y} is not in the array.\")\n",
    "\n",
    "print(ranks)\n"
   ],
   "id": "4b2372b1b97c8811",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m train_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstation_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munique():\n\u001B[1;32m      4\u001B[0m     y \u001B[38;5;241m=\u001B[39m train_target_df[(train_target_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39mt) \u001B[38;5;241m&\u001B[39m (train_target_df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstation_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m==\u001B[39ms)][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt2m\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy()\n\u001B[0;32m----> 5\u001B[0m     ensembles \u001B[38;5;241m=\u001B[39m train_df[\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m&\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_df\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstation_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt2m\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy()\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39missubdtype(y\u001B[38;5;241m.\u001B[39mdtype, np\u001B[38;5;241m.\u001B[39mnumber):\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28mall\u001B[39m \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mconcatenate([y, ensembles])\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/ops/common.py:81\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     79\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/arraylike.py:70\u001B[0m, in \u001B[0;36mOpsMixin.__and__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__and__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__and__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_logical_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mand_\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/series.py:6107\u001B[0m, in \u001B[0;36mSeries._logical_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   6104\u001B[0m lvalues \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m   6105\u001B[0m rvalues \u001B[38;5;241m=\u001B[39m extract_array(other, extract_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, extract_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m-> 6107\u001B[0m res_values \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogical_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6108\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_result(res_values, name\u001B[38;5;241m=\u001B[39mres_name)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:403\u001B[0m, in \u001B[0;36mlogical_op\u001B[0;34m(left, right, op)\u001B[0m\n\u001B[1;32m    401\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m na_logical_op(lvalues, rvalues, op)\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;66;03m# error: Cannot call function of unknown type\u001B[39;00m\n\u001B[0;32m--> 403\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mfiller\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres_values\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[operator]\u001B[39;00m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res_values\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:370\u001B[0m, in \u001B[0;36mlogical_op.<locals>.fill_bool\u001B[0;34m(x, left)\u001B[0m\n\u001B[1;32m    367\u001B[0m         x[mask] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m left \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m is_bool_dtype(left\u001B[38;5;241m.\u001B[39mdtype):\n\u001B[0;32m--> 370\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:09:58.140822Z",
     "start_time": "2025-03-19T04:09:58.084193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if DATASET == \"f\":\n",
    "    y = test_f[1][\"t2m\"].to_numpy()\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    mu = test_f[0][\"t2m_mean\"].to_numpy()\n",
    "    print(mu)\n",
    "    print(mu.shape)\n",
    "    sigma = test_f[0][\"t2m_std\"].to_numpy()\n",
    "    print(sigma)\n",
    "    print(sigma.shape)\n",
    "\n",
    "if DATASET == \"rf\":\n",
    "    y = test_rf[1][\"t2m\"].to_numpy()\n",
    "    print(y)\n",
    "    print(y.shape)\n",
    "    mu = test_rf[0][\"t2m_mean\"].to_numpy()\n",
    "    print(mu)\n",
    "    print(mu.shape)\n",
    "    sigma = test_rf[0][\"t2m_std\"].to_numpy()\n",
    "    print(sigma)\n",
    "    print(sigma.shape)\n",
    "\n",
    "normalCRPS = NormalCRPS()\n",
    "\n",
    "err = normalCRPS.crps(mu_sigma=torch.tensor(predictions[[\"mu\", \"sigma\"]].to_numpy()),\n",
    "                      y=torch.tensor(predictions[\"t2m\"].to_numpy())).item()\n",
    "err"
   ],
   "id": "c919b5c9c1543d59",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[278.65 275.25 279.75 ... 276.35 270.65 268.85]\n",
      "(86742,)\n",
      "[-0.09018177 -0.5147674  -0.07385244 ... -1.285776   -1.93341\n",
      " -2.39585   ]\n",
      "(86742,)\n",
      "[-0.80551374 -0.44987863 -0.64775991 ...  2.48722911  1.15606916\n",
      "  1.43095255]\n",
      "(86742,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 25\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28mprint\u001B[39m(sigma\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     23\u001B[0m normalCRPS \u001B[38;5;241m=\u001B[39m NormalCRPS()\n\u001B[0;32m---> 25\u001B[0m err \u001B[38;5;241m=\u001B[39m normalCRPS\u001B[38;5;241m.\u001B[39mcrps(mu_sigma\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mtensor(\u001B[43mpredictions\u001B[49m[[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmu\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msigma\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\u001B[38;5;241m.\u001B[39mto_numpy()),\n\u001B[1;32m     26\u001B[0m                       y\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mtensor(predictions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt2m\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy()))\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     27\u001B[0m err\n",
      "\u001B[0;31mNameError\u001B[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T18:46:19.353779Z",
     "start_time": "2025-03-14T18:46:19.122406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PIT for 120h\n",
    "probs = norm.cdf(y.flatten(), loc=mu.flatten(), scale=sigma.flatten())  # scale is standard deviation\n",
    "n, bins, patches = plt.hist(probs, bins=15, density=True)\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= (max(col))\n",
    "\n",
    "plt.ylim(0, 1.5)  # Layout\n",
    "plt.hlines(xmin=0, xmax=1, y=1, colors=\"black\", linestyles=\"--\")\n",
    "# raenge berechnen, plt.hist, fuer jede Möglichkeit (an jeder station zu jedem Zeitpunkt) so viele Raenge = Anzahl beobachtung,\n",
    "# für jedes Ensemble: was sagt das Ensemble, obs rein, sortieren, an welcher stelle steht die Beobachtung\n",
    "# fuer jede Beobachtung einen Rang => als Histogram plotten"
   ],
   "id": "93d78f2935725264",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f81189092d0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:10:10.222697Z",
     "start_time": "2025-03-19T04:10:09.621201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ensemble_forecasts = mu # ensemble forecasts geben auch nur mu und sigma?\n",
    "observations = y\n",
    "ranks = np.array([np.sum(ens < obs) + 1 for ens, obs in zip(ensemble_forecasts, observations)])\n",
    "\n",
    "n, bins, patches = plt.hist(ranks, bins=15, density=True)\n",
    "\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# scale values to interval [0,1]\n",
    "col = bin_centers - min(bin_centers)\n",
    "col /= (max(col))\n",
    "\n",
    "plt.ylim(0, 1.5)  # Layout\n",
    "plt.hlines(xmin=0, xmax=1, y=1, colors=\"black\", linestyles=\"--\")\n"
   ],
   "id": "b3d5c7b40b529c0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x7f296039d240>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# One Station",
   "id": "e082b2a18d2a4050"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T18:53:01.177123Z",
     "start_time": "2025-03-06T18:53:01.120337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "one_station_X = train[0][train[0][\"station_id\"]==1]\n",
    "one_station_y = train[1][train[1][\"station_id\"]==1]\n",
    "\n",
    "one_station_X = one_station_X.drop(\"station_id\", axis=1)\n",
    "one_station_y = one_station_y.drop(\"station_id\", axis=1)\n",
    "\n",
    "print(one_station_X)\n",
    "print(one_station_X.shape)\n",
    "print(one_station_y.shape)\n",
    "\n",
    "# test_rf\n",
    "s1_test_rf_X = test_rf[0][test_rf[0][\"station_id\"]==1]\n",
    "s1_test_rf_y = test_rf[1][test_rf[1][\"station_id\"]==1]\n",
    "\n",
    "s1_test_rf_X = s1_test_rf_X.drop(\"station_id\", axis=1)\n",
    "s1_test_rf_y = s1_test_rf_y.drop(\"station_id\", axis=1)\n",
    "\n",
    "# test_f\n",
    "s1_test_f_X = test_f[0][test_f[0][\"station_id\"]==1]\n",
    "s1_test_f_y = test_f[1][test_f[1][\"station_id\"]==1]\n",
    "\n",
    "s1_test_f_X = s1_test_f_X.drop(\"station_id\", axis=1)\n",
    "s1_test_f_y = s1_test_f_y.drop(\"station_id\", axis=1)"
   ],
   "id": "5318bf9a70ee77e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        model_orography  station_altitude  station_latitude  \\\n",
      "1             -0.737002           -0.7786          1.016052   \n",
      "123           -0.737002           -0.7786          1.016052   \n",
      "245           -0.737002           -0.7786          1.016052   \n",
      "367           -0.737002           -0.7786          1.016052   \n",
      "489           -0.737002           -0.7786          1.016052   \n",
      "...                 ...               ...               ...   \n",
      "420047        -0.737002           -0.7786          1.016052   \n",
      "420169        -0.737002           -0.7786          1.016052   \n",
      "420291        -0.737002           -0.7786          1.016052   \n",
      "420413        -0.737002           -0.7786          1.016052   \n",
      "420535        -0.737002           -0.7786          1.016052   \n",
      "\n",
      "        station_longitude  cape_mean  cape_std   sd_mean    sd_std  stl1_mean  \\\n",
      "1                -0.89124  -0.164696 -0.233030 -0.138945 -0.130389  -0.465046   \n",
      "123              -0.89124  -0.127184 -0.138486 -0.138945 -0.130389  -0.443237   \n",
      "245              -0.89124  -0.157128 -0.205144 -0.138945 -0.130389  -0.493848   \n",
      "367              -0.89124  -0.179347 -0.224208 -0.138569 -0.118298  -0.666115   \n",
      "489              -0.89124  -0.194627 -0.269349 -0.128484 -0.094832  -1.032942   \n",
      "...                   ...        ...       ...       ...       ...        ...   \n",
      "420047           -0.89124  -0.180904 -0.217095 -0.138945 -0.130389  -0.408190   \n",
      "420169           -0.89124  -0.194627 -0.269349  0.049665  0.074999  -1.481411   \n",
      "420291           -0.89124  -0.031851 -0.021526 -0.138945 -0.130389  -0.087933   \n",
      "420413           -0.89124  -0.193998 -0.266479  0.195411  0.577944  -1.399988   \n",
      "420535           -0.89124  -0.190646 -0.252057 -0.134015 -0.125014  -1.370927   \n",
      "\n",
      "        stl1_std  ...    q_mean     q_std    u_mean     u_std    v_mean  \\\n",
      "1       1.073037  ...  0.433324  0.358571 -0.746920  5.432942  1.666245   \n",
      "123    -0.729082  ... -0.331529 -0.091183 -0.846796  0.530674  0.527320   \n",
      "245    -0.433906  ...  0.209089  0.499193  0.630721  1.430988  2.049817   \n",
      "367     0.323421  ... -0.490498 -0.462935  0.708757  1.573345 -1.295278   \n",
      "489    -0.800538  ... -1.349110 -0.811540 -0.002433  2.239311 -2.185997   \n",
      "...          ...  ...       ...       ...       ...       ...       ...   \n",
      "420047 -0.003070  ... -0.810062  0.193430  0.240393 -0.045242  0.851824   \n",
      "420169 -0.919173  ... -0.777223  0.669660 -0.460586 -0.380500  0.012031   \n",
      "420291 -0.913360  ...  0.608128  0.277408 -0.048647  2.699677  0.350894   \n",
      "420413 -0.529402  ... -0.944738 -0.154593  1.177332  1.458282  0.345556   \n",
      "420535  0.409809  ... -1.229235 -1.199663  0.113013 -0.301731  0.078417   \n",
      "\n",
      "           v_std    t_mean     t_std   cos_doy       sin_doy  \n",
      "1       2.520527 -0.757580  1.563501  0.999407  3.442161e-02  \n",
      "123     0.694997 -0.886288  0.894268  0.996298  8.596480e-02  \n",
      "245     1.632936 -0.570165 -0.268864  0.988023  1.543088e-01  \n",
      "367     1.865991 -1.100426  0.275097  0.978740  2.051045e-01  \n",
      "489     0.384951 -2.033739 -0.047349  0.962309  2.719582e-01  \n",
      "...          ...       ...       ...       ...           ...  \n",
      "420047 -0.678904 -0.126731 -0.880790  0.992749 -1.202080e-01  \n",
      "420169 -0.118070 -1.776431  0.891437  0.994671 -1.031017e-01  \n",
      "420291  0.586493 -0.020650  0.483000  0.997630 -6.880243e-02  \n",
      "420413  0.454055 -1.693830 -0.947345  0.998667 -5.161967e-02  \n",
      "420535 -0.118211 -0.965405 -0.356925  1.000000  6.432491e-16  \n",
      "\n",
      "[3448 rows x 64 columns]\n",
      "(3448, 64)\n",
      "(3448, 2)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### One Station MSE and CRPS NNs\n",
    "Station (station_id=1) with one hidden layer and loss functions MSE or CRPS"
   ],
   "id": "241d282af9a75045"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T18:54:27.122703Z",
     "start_time": "2025-03-06T18:54:27.112935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSEStationNN(L.LightningModule):\n",
    "    def __init__(self, in_feat, hidden_size, optimizer_class, optimizer_params):\n",
    "        super(MSEStationNN, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear_t2m = torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_t2m(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0): # unterschied zwischen predict und test_step?\n",
    "        x, y = batch # wieso hat test_step auch y? => um score zu berechnen\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        return loss\n"
   ],
   "id": "89a9fc3e00e76201",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T18:56:42.841495Z",
     "start_time": "2025-03-06T18:56:42.822814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CRPSStationNN(L.LightningModule):\n",
    "    def __init__(self, in_feat, hidden_size, optimizer_class, optimizer_params):\n",
    "        super(CRPSStationNN, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        #self.linear_t2m = torch.nn.Linear(in_features=hidden_size, out_features=2) => wieso nicht direkt 2 outputs?\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.last_linear_mu = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.last_linear_sigma = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        self.loss_fn = NormalCRPS()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        mu = self.last_linear_mu(x)\n",
    "        sigma = self.softplus(self.last_linear_sigma(x))\n",
    "        res = torch.cat([mu, sigma], dim=1)\n",
    "        return res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        print(f'test_loss: {loss}')\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat\n"
   ],
   "id": "2a867835aca7d93c",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train One Station NN (MSE or CRPS)",
   "id": "85bca1426de9002b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T18:56:51.719558Z",
     "start_time": "2025-03-06T18:56:45.315672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with wandb.init(\n",
    "    project=\"exploration\",\n",
    "    id = f\"training_run_24h_crps\",\n",
    "    tags=[\"exploration\"],\n",
    "):\n",
    "\n",
    "    y_scaler = StandardScaler(with_std=False) # wieso scalen wir überhaupt? => robuster?\n",
    "    y_scaler = y_scaler.fit(one_station_y[[\"t2m\"]])\n",
    "\n",
    "    batch_size = 512\n",
    "    hidden_size=128\n",
    "    lr=0.0002\n",
    "    max_epochs=31\n",
    "    in_feat = one_station_X.shape[1]\n",
    "\n",
    "    one_station_train_ds = TensorDataset(torch.Tensor(one_station_X.to_numpy()), torch.Tensor(y_scaler.transform(one_station_y[[\"t2m\"]])))\n",
    "    one_station_loader = DataLoader(one_station_train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    s1_test_rf_ds = TensorDataset(torch.Tensor(s1_test_rf_X.to_numpy()), torch.Tensor(y_scaler.transform(s1_test_rf_y[[\"t2m\"]])))\n",
    "    s1_test_rf_loader = DataLoader(s1_test_rf_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    one_station_nn = CRPSStationNN(\n",
    "        in_feat=in_feat,\n",
    "        hidden_size=hidden_size,\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params={\"lr\": lr}\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"one_station_crps\")\n",
    "\n",
    "    os_checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    one_station_trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=os_checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    value = one_station_trainer.fit(model=one_station_nn, train_dataloaders=one_station_loader)\n",
    "\n",
    "    final_loss = one_station_trainer.logged_metrics[\"train_loss_step\"] # nochmal step und epoch nachschauen\n",
    "    print(\"Final MSE Loss:\", final_loss)\n",
    "\n",
    "\n",
    "# wo finde ich den tatsaechlichen wert? => bei test, jetzt wird nur das Modell trainiert"
   ],
   "id": "4b50303c1b2a53dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250306_195645-training_run_24h_crps</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps' target=\"_blank\">training_run_24h_crps</a></strong> to <a href='https://wandb.ai/leachen_thesis/exploration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/exploration' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/drn_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type       | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | linear            | Linear     | 8.3 K  | train\n",
      "1 | relu              | ReLU       | 0      | train\n",
      "2 | softplus          | Softplus   | 0      | train\n",
      "3 | last_linear_mu    | Linear     | 129    | train\n",
      "4 | last_linear_sigma | Linear     | 129    | train\n",
      "5 | loss_fn           | NormalCRPS | 0      | train\n",
      "---------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 57.77it/s, v_num=crps, train_loss_step=1.320, train_loss_epoch=1.280]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 52.44it/s, v_num=crps, train_loss_step=1.320, train_loss_epoch=1.280]\n",
      "Final MSE Loss: tensor(1.3195)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇██▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>1.28279</td></tr><tr><td>train_loss_step</td><td>1.31945</td></tr><tr><td>trainer/global_step</td><td>216</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_24h_crps</strong> at: <a href='https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/exploration' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250306_195645-training_run_24h_crps/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validate and test one station NN (MSE and CRPS)",
   "id": "a235b5768b328247"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T18:56:52.292576Z",
     "start_time": "2025-03-06T18:56:52.258300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# validation and test for both mse and crps\n",
    "s1_test_f_ds = TensorDataset(torch.Tensor(s1_test_f_X.to_numpy()), torch.Tensor(y_scaler.transform(s1_test_f_y[[\"t2m\"]])))\n",
    "s1_test_f_loader = DataLoader(s1_test_f_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss = one_station_trainer.test(model=one_station_nn, dataloaders=s1_test_f_loader)\n",
    "print(loss)\n"
   ],
   "id": "98751ab483dbdbad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]test_loss: 1.2687277793884277\n",
      "Testing DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 351.05it/s]test_loss: 1.2436953783035278\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 225.69it/s]\n",
      "[{}]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Other",
   "id": "20dd0c6f9582e7a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MyDRN without wandb and without saving",
   "id": "a44e0f7c4f14e32f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T00:54:14.513273Z",
     "start_time": "2025-03-01T00:51:34.099860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MyDRN train without wandb without saving\n",
    "\n",
    "y_scaler = StandardScaler(with_std=False)\n",
    "y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.Tensor(train[0].to_numpy()), torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]]))\n",
    ")\n",
    "\n",
    "#from params.json best_24h\n",
    "batch_size = 2048\n",
    "hidden_size=128\n",
    "lr=0.0002\n",
    "max_epochs=31\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "embed_dim = 20\n",
    "in_feat = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "mydrn = MyDRN(\n",
    "    hidden_size=hidden_size,\n",
    "    embedding_dim=embed_dim,\n",
    "    in_feat=in_feat,\n",
    "    optimizer_class=AdamW,\n",
    "    optimizer_params=dict(lr=lr),\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # dirpath=SAVEPATH, filename=f\"run_{args.id}\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=50,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    callbacks=checkpoint_callback,\n",
    ")\n",
    "\n",
    "trainer.fit(model=mydrn, train_dataloaders=train_loader)\n"
   ],
   "id": "934d8d83f8e0536b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/drn_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | embedding  | EmbedStations | 2.4 K  | train\n",
      "1 | linear     | Linear        | 10.9 K | train\n",
      "2 | relu       | ReLU          | 0      | train\n",
      "3 | linear_t2m | Linear        | 129    | train\n",
      "4 | loss       | MSELoss       | 0      | train\n",
      "-----------------------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 5/195 [00:00<00:05, 32.69it/s, v_num=28, train_loss_step=42.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2048, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 5/195 [00:00<00:05, 34.61it/s, v_num=28, train_loss_step=41.90, train_loss_epoch=41.20]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1554])) that is different to the input size (torch.Size([1554, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 195/195 [00:05<00:00, 37.23it/s, v_num=28, train_loss_step=39.70, train_loss_epoch=41.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 195/195 [00:05<00:00, 37.21it/s, v_num=28, train_loss_step=39.70, train_loss_epoch=41.20]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DRN with given DRN",
   "id": "fb626043627b8f87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:44:53.834541Z",
     "start_time": "2025-02-25T10:42:25.653661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with given models - funktioniert fuer summary statistics, aber nicht ohne? => woran liegt das?\n",
    "# => weil targets (y) einen actual Wert pro station besitzen und X noch 11 ensemble member pro Station hat\n",
    "DIRECTORY = os.getcwd()\n",
    "JSONPATH = os.path.join(DIRECTORY, \"trained_models/drn_24h/params.json\")\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"trained_models/drn_24h/models\")\n",
    "\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "\n",
    "with wandb.init(\n",
    "    project=\"multigraph\",\n",
    "    # id=f\"training_run_drn_{args_dict['leadtime']}_{args.id}\",\n",
    "    id = f\"training_run_{args_dict['leadtime']}\",\n",
    "    config=args_dict,\n",
    "    tags=[\"final_training\"],\n",
    "):\n",
    "    config=wandb.config\n",
    "    dataframes = load_dataframes(mode=\"train\", leadtime=config.leadtime)\n",
    "    dataframes = summary_statistics(dataframes)\n",
    "    dataframes.pop(\"stations\")\n",
    "\n",
    "    # print(list(dataframes.values()))\n",
    "    for df in dataframes.values():\n",
    "        print(type(df))\n",
    "\n",
    "    for X, y in dataframes.values():\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train, valid_test = normalize_features(\n",
    "        training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    "    )\n",
    "\n",
    "    print(f\"dataframes['train']: {dataframes['train']}\")\n",
    "    print(f\"train: {train}\")\n",
    "\n",
    "    train = drop_nans(train)\n",
    "\n",
    "    y_scaler = StandardScaler(with_std=False)\n",
    "    y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.Tensor(train[0].to_numpy()), torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]]))\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    embed_dim = 20 # why 20? => embed stations - instead of station_id - map into a latent vector space\n",
    "    in_channels = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "    drn = DRN(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=config.hidden_channels,\n",
    "        embedding_dim=embed_dim,\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config.lr),\n",
    "    )\n",
    "    wandb_logger = WandbLogger(project=\"multigraph\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        # dirpath=SAVEPATH, filename=f\"run_{args.id}\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "        dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config.max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "    trainer.fit(model=drn, train_dataloaders=train_loader)\n"
   ],
   "id": "ea6cdae3efed6f40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/drn_24h/params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250225_114225-training_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">training_run_24h</a></strong> to <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "[INFO] Normalizing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/trained_models/drn_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 21.8 K | train\n",
      "2 | last_linear_mu    | Linear        | 257    | train\n",
      "3 | last_linear_sigma | Linear        | 257    | train\n",
      "4 | relu              | ReLU          | 0      | train\n",
      "5 | softplus          | Softplus      | 0      | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "24.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.7 K    Total params\n",
      "0.099     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes['train']: (        station_id  model_orography  station_altitude  station_latitude  \\\n",
      "0                0        -0.738289         -0.764101          1.382904   \n",
      "1                1        -0.737002         -0.778600          1.016052   \n",
      "2                2        -0.731851         -0.733171          1.571141   \n",
      "3                3        -0.728793         -0.765712          1.661951   \n",
      "4                4        -0.724769         -0.761846          0.884948   \n",
      "...            ...              ...               ...               ...   \n",
      "420651         117         0.914135          0.298485         -2.911765   \n",
      "420652         118         1.443053          0.598123         -1.881973   \n",
      "420653         119         2.338639          0.646451         -2.021799   \n",
      "420654         120         4.799571          3.994016         -2.028314   \n",
      "420655         121         5.913903          4.345205         -2.201382   \n",
      "\n",
      "        station_longitude  cape_mean  cape_std   sd_mean    sd_std  stl1_mean  \\\n",
      "0               -0.895456  -0.162404 -0.211540 -0.138945 -0.130389  -0.065441   \n",
      "1               -0.891240  -0.164696 -0.233030 -0.138945 -0.130389  -0.465046   \n",
      "2               -0.829873  -0.181057 -0.231899 -0.138945 -0.130389  -0.105207   \n",
      "3               -0.630782  -0.186269 -0.245221 -0.138945 -0.130389  -0.069467   \n",
      "4               -0.708545  -0.161323 -0.233911 -0.138945 -0.130389  -0.526710   \n",
      "...                   ...        ...       ...       ...       ...        ...   \n",
      "420651          -1.659809  -0.194591 -0.269107 -0.138930 -0.130201  -0.492037   \n",
      "420652           1.430446  -0.163949 -0.223208  0.032781  0.187163  -1.013463   \n",
      "420653           1.362001  -0.102331 -0.162280  0.395475  0.354853  -1.218398   \n",
      "420654           1.636564  -0.096942 -0.130522  4.413006  6.996919  -1.389498   \n",
      "420655           1.636304   0.030651  0.044717  6.780401  8.101624  -1.560089   \n",
      "\n",
      "        ...    q_mean     q_std    u_mean     u_std    v_mean     v_std  \\\n",
      "0       ... -0.114795  1.470589 -1.416661  0.670551  1.931902  1.831306   \n",
      "1       ...  0.433324  0.358571 -0.746920  5.432942  1.666245  2.520527   \n",
      "2       ... -0.505220  1.064439 -1.522950 -0.090761  1.836820  2.540884   \n",
      "3       ... -0.754990  0.193510 -1.566283 -0.304501  1.628440  1.460403   \n",
      "4       ...  0.398675  0.609186 -0.647017  5.686270  1.696099  3.035443   \n",
      "...     ...       ...       ...       ...       ...       ...       ...   \n",
      "420651  ... -1.016872 -0.799413  0.360431 -1.581705 -0.965468 -0.314380   \n",
      "420652  ...  0.058197 -0.856260  0.196157 -0.088442 -0.267489 -1.145218   \n",
      "420653  ...  0.047738 -0.957247  0.136901 -0.865069 -0.395362 -1.303312   \n",
      "420654  ...  0.075580 -1.053509  0.060473 -0.875901 -0.419308 -1.240052   \n",
      "420655  ...  0.052665 -0.968504 -0.181117 -0.672254 -0.557093 -1.209932   \n",
      "\n",
      "          t_mean     t_std   cos_doy       sin_doy  \n",
      "0      -0.801722  0.618363  0.999407  3.442161e-02  \n",
      "1      -0.757580  1.563501  0.999407  3.442161e-02  \n",
      "2      -0.771376  0.765197  0.999407  3.442161e-02  \n",
      "3      -0.745605  0.401920  0.999407  3.442161e-02  \n",
      "4      -0.772579  0.788204  0.999407  3.442161e-02  \n",
      "...          ...       ...       ...           ...  \n",
      "420651 -0.445756 -0.222885  1.000000  6.432491e-16  \n",
      "420652 -0.731970 -1.510291  1.000000  6.432491e-16  \n",
      "420653 -0.684412 -1.302932  1.000000  6.432491e-16  \n",
      "420654 -0.665653 -1.386536  1.000000  6.432491e-16  \n",
      "420655 -0.630435 -1.469852  1.000000  6.432491e-16  \n",
      "\n",
      "[420656 rows x 65 columns],              time  station_id     t2m\n",
      "0      1997-01-02           0  277.75\n",
      "1      1997-01-02           1  279.55\n",
      "2      1997-01-02           2  276.45\n",
      "3      1997-01-02           3  275.75\n",
      "4      1997-01-02           4  279.35\n",
      "...           ...         ...     ...\n",
      "420651 2013-12-31         117  281.35\n",
      "420652 2013-12-31         118  279.35\n",
      "420653 2013-12-31         119  278.25\n",
      "420654 2013-12-31         120  273.15\n",
      "420655 2013-12-31         121  272.65\n",
      "\n",
      "[420656 rows x 3 columns])\n",
      "train: (        station_id  model_orography  station_altitude  station_latitude  \\\n",
      "0                0        -0.738289         -0.764101          1.382904   \n",
      "1                1        -0.737002         -0.778600          1.016052   \n",
      "2                2        -0.731851         -0.733171          1.571141   \n",
      "3                3        -0.728793         -0.765712          1.661951   \n",
      "4                4        -0.724769         -0.761846          0.884948   \n",
      "...            ...              ...               ...               ...   \n",
      "420651         117         0.914135          0.298485         -2.911765   \n",
      "420652         118         1.443053          0.598123         -1.881973   \n",
      "420653         119         2.338639          0.646451         -2.021799   \n",
      "420654         120         4.799571          3.994016         -2.028314   \n",
      "420655         121         5.913903          4.345205         -2.201382   \n",
      "\n",
      "        station_longitude  cape_mean  cape_std   sd_mean    sd_std  stl1_mean  \\\n",
      "0               -0.895456  -0.162404 -0.211540 -0.138945 -0.130389  -0.065441   \n",
      "1               -0.891240  -0.164696 -0.233030 -0.138945 -0.130389  -0.465046   \n",
      "2               -0.829873  -0.181057 -0.231899 -0.138945 -0.130389  -0.105207   \n",
      "3               -0.630782  -0.186269 -0.245221 -0.138945 -0.130389  -0.069467   \n",
      "4               -0.708545  -0.161323 -0.233911 -0.138945 -0.130389  -0.526710   \n",
      "...                   ...        ...       ...       ...       ...        ...   \n",
      "420651          -1.659809  -0.194591 -0.269107 -0.138930 -0.130201  -0.492037   \n",
      "420652           1.430446  -0.163949 -0.223208  0.032781  0.187163  -1.013463   \n",
      "420653           1.362001  -0.102331 -0.162280  0.395475  0.354853  -1.218398   \n",
      "420654           1.636564  -0.096942 -0.130522  4.413006  6.996919  -1.389498   \n",
      "420655           1.636304   0.030651  0.044717  6.780401  8.101624  -1.560089   \n",
      "\n",
      "        ...    q_mean     q_std    u_mean     u_std    v_mean     v_std  \\\n",
      "0       ... -0.114795  1.470589 -1.416661  0.670551  1.931902  1.831306   \n",
      "1       ...  0.433324  0.358571 -0.746920  5.432942  1.666245  2.520527   \n",
      "2       ... -0.505220  1.064439 -1.522950 -0.090761  1.836820  2.540884   \n",
      "3       ... -0.754990  0.193510 -1.566283 -0.304501  1.628440  1.460403   \n",
      "4       ...  0.398675  0.609186 -0.647017  5.686270  1.696099  3.035443   \n",
      "...     ...       ...       ...       ...       ...       ...       ...   \n",
      "420651  ... -1.016872 -0.799413  0.360431 -1.581705 -0.965468 -0.314380   \n",
      "420652  ...  0.058197 -0.856260  0.196157 -0.088442 -0.267489 -1.145218   \n",
      "420653  ...  0.047738 -0.957247  0.136901 -0.865069 -0.395362 -1.303312   \n",
      "420654  ...  0.075580 -1.053509  0.060473 -0.875901 -0.419308 -1.240052   \n",
      "420655  ...  0.052665 -0.968504 -0.181117 -0.672254 -0.557093 -1.209932   \n",
      "\n",
      "          t_mean     t_std   cos_doy       sin_doy  \n",
      "0      -0.801722  0.618363  0.999407  3.442161e-02  \n",
      "1      -0.757580  1.563501  0.999407  3.442161e-02  \n",
      "2      -0.771376  0.765197  0.999407  3.442161e-02  \n",
      "3      -0.745605  0.401920  0.999407  3.442161e-02  \n",
      "4      -0.772579  0.788204  0.999407  3.442161e-02  \n",
      "...          ...       ...       ...           ...  \n",
      "420651 -0.445756 -0.222885  1.000000  6.432491e-16  \n",
      "420652 -0.731970 -1.510291  1.000000  6.432491e-16  \n",
      "420653 -0.684412 -1.302932  1.000000  6.432491e-16  \n",
      "420654 -0.665653 -1.386536  1.000000  6.432491e-16  \n",
      "420655 -0.630435 -1.469852  1.000000  6.432491e-16  \n",
      "\n",
      "[420656 rows x 65 columns],              time  station_id     t2m\n",
      "0      1997-01-02           0  277.75\n",
      "1      1997-01-02           1  279.55\n",
      "2      1997-01-02           2  276.45\n",
      "3      1997-01-02           3  275.75\n",
      "4      1997-01-02           4  279.35\n",
      "...           ...         ...     ...\n",
      "420651 2013-12-31         117  281.35\n",
      "420652 2013-12-31         118  279.35\n",
      "420653 2013-12-31         119  278.25\n",
      "420654 2013-12-31         120  273.15\n",
      "420655 2013-12-31         121  272.65\n",
      "\n",
      "[420656 rows x 3 columns])\n",
      "Epoch 0:   0%|          | 0/52582 [12:45<?, ?it/s]it/s, v_num=_24h, train_loss_step=4.930]\n",
      "Epoch 25: 100%|██████████| 98/98 [00:05<00:00, 17.94it/s, v_num=_24h, train_loss_step=0.580, train_loss_epoch=0.597]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=26` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 98/98 [00:05<00:00, 17.92it/s, v_num=_24h, train_loss_step=0.580, train_loss_epoch=0.597]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▄▄▃▃▃▂▃▃▂▃▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▁▁▂▁▂▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.59687</td></tr><tr><td>train_loss_step</td><td>0.58027</td></tr><tr><td>trainer/global_step</td><td>2547</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_24h</strong> at: <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250225_114225-training_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T21:46:57.285357Z",
     "start_time": "2025-03-01T21:46:57.027801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ],
   "id": "ec3a3aae4f779f81",
   "outputs": [],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
