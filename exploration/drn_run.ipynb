{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run DRN",
   "id": "2a41077302fdadb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:00:46.080471Z",
     "start_time": "2025-03-01T17:00:43.361162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "# load data first\n",
    "%cd /home/ltchen/gnnpp\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from models.drn import DRN\n",
    "from models.model_utils import EmbedStations\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "\n",
    "from utils.data import load_dataframes, summary_statistics\n",
    "from utils.drn_utils import *\n",
    "from models.loss import NormalCRPS"
   ],
   "id": "259579d6231826a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load dataframes for train, valid, test",
   "id": "d2df53cbcd0652e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:00:53.779940Z",
     "start_time": "2025-03-01T17:00:47.610757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframes = load_dataframes(mode=\"train\", leadtime=\"24h\") # train mode => for training nn? Wie wird das im Paper beschrieben?\n",
    "dataframes = summary_statistics(dataframes)\n",
    "dataframes.pop(\"stations\") # .pop(\"stations\") => entfernt den df mit stations, wofuer brauche ich die dann überhaupt? Grafik?\n",
    "\n",
    "for X, y in dataframes.values(): # wofuer?\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train, valid_test = normalize_features(\n",
    "    training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    ")\n",
    "\n",
    "train = drop_nans(train)\n",
    "(test_rf, test_f) = valid_test\n",
    "test_rf = drop_nans(test_rf)\n",
    "test_f = drop_nans(test_f)\n",
    "\n",
    "DIRECTORY = os.getcwd()\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"explored_models/drn_24h/models\")\n",
    "#print(train[1].isna().sum()) #drop_nans does not work without summary_statistics"
   ],
   "id": "e6d863460e3de5e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "[INFO] Normalizing features...\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:00:53.792660Z",
     "start_time": "2025-03-01T17:00:53.787435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# one station only\n",
    "# train\n",
    "for i in train:\n",
    "    print(i.shape)\n",
    "    print(i.columns)\n",
    "\n",
    "# valid_test\n",
    "for (i, j) in valid_test:\n",
    "    print(i.shape)\n",
    "    print(i.columns)"
   ],
   "id": "6c927fe1e7b6ae17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(398866, 65)\n",
      "Index(['station_id', 'model_orography', 'station_altitude', 'station_latitude',\n",
      "       'station_longitude', 'cape_mean', 'cape_std', 'sd_mean', 'sd_std',\n",
      "       'stl1_mean', 'stl1_std', 'swvl1_mean', 'swvl1_std', 't2m_mean',\n",
      "       't2m_std', 'tcc_mean', 'tcc_std', 'tcw_mean', 'tcw_std', 'tcwv_mean',\n",
      "       'tcwv_std', 'u10_mean', 'u10_std', 'u100_mean', 'u100_std', 'v10_mean',\n",
      "       'v10_std', 'v100_mean', 'v100_std', 'vis_mean', 'vis_std', 'cp6_mean',\n",
      "       'cp6_std', 'mn2t6_mean', 'mn2t6_std', 'mx2t6_mean', 'mx2t6_std',\n",
      "       'p10fg6_mean', 'p10fg6_std', 'slhf6_mean', 'slhf6_std', 'sshf6_mean',\n",
      "       'sshf6_std', 'ssr6_mean', 'ssr6_std', 'ssrd6_mean', 'ssrd6_std',\n",
      "       'str6_mean', 'str6_std', 'strd6_mean', 'strd6_std', 'tp6_mean',\n",
      "       'tp6_std', 'z_mean', 'z_std', 'q_mean', 'q_std', 'u_mean', 'u_std',\n",
      "       'v_mean', 'v_std', 't_mean', 't_std', 'cos_doy', 'sin_doy'],\n",
      "      dtype='object')\n",
      "(398866, 3)\n",
      "Index(['time', 'station_id', 't2m'], dtype='object')\n",
      "(89304, 65)\n",
      "Index(['station_id', 'model_orography', 'station_altitude', 'station_latitude',\n",
      "       'station_longitude', 'cape_mean', 'cape_std', 'sd_mean', 'sd_std',\n",
      "       'stl1_mean', 'stl1_std', 'swvl1_mean', 'swvl1_std', 't2m_mean',\n",
      "       't2m_std', 'tcc_mean', 'tcc_std', 'tcw_mean', 'tcw_std', 'tcwv_mean',\n",
      "       'tcwv_std', 'u10_mean', 'u10_std', 'u100_mean', 'u100_std', 'v10_mean',\n",
      "       'v10_std', 'v100_mean', 'v100_std', 'vis_mean', 'vis_std', 'cp6_mean',\n",
      "       'cp6_std', 'mn2t6_mean', 'mn2t6_std', 'mx2t6_mean', 'mx2t6_std',\n",
      "       'p10fg6_mean', 'p10fg6_std', 'slhf6_mean', 'slhf6_std', 'sshf6_mean',\n",
      "       'sshf6_std', 'ssr6_mean', 'ssr6_std', 'ssrd6_mean', 'ssrd6_std',\n",
      "       'str6_mean', 'str6_std', 'strd6_mean', 'strd6_std', 'tp6_mean',\n",
      "       'tp6_std', 'z_mean', 'z_std', 'q_mean', 'q_std', 'u_mean', 'u_std',\n",
      "       'v_mean', 'v_std', 't_mean', 't_std', 'cos_doy', 'sin_doy'],\n",
      "      dtype='object')\n",
      "(89060, 65)\n",
      "Index(['station_id', 'model_orography', 'station_altitude', 'station_latitude',\n",
      "       'station_longitude', 'cape_mean', 'cape_std', 'sd_mean', 'sd_std',\n",
      "       'stl1_mean', 'stl1_std', 'swvl1_mean', 'swvl1_std', 't2m_mean',\n",
      "       't2m_std', 'tcc_mean', 'tcc_std', 'tcw_mean', 'tcw_std', 'tcwv_mean',\n",
      "       'tcwv_std', 'u10_mean', 'u10_std', 'u100_mean', 'u100_std', 'v10_mean',\n",
      "       'v10_std', 'v100_mean', 'v100_std', 'vis_mean', 'vis_std', 'cp6_mean',\n",
      "       'cp6_std', 'mn2t6_mean', 'mn2t6_std', 'mx2t6_mean', 'mx2t6_std',\n",
      "       'p10fg6_mean', 'p10fg6_std', 'slhf6_mean', 'slhf6_std', 'sshf6_mean',\n",
      "       'sshf6_std', 'ssr6_mean', 'ssr6_std', 'ssrd6_mean', 'ssrd6_std',\n",
      "       'str6_mean', 'str6_std', 'strd6_mean', 'strd6_std', 'tp6_mean',\n",
      "       'tp6_std', 'z_mean', 'z_std', 'q_mean', 'q_std', 'u_mean', 'u_std',\n",
      "       'v_mean', 'v_std', 't_mean', 't_std', 'cos_doy', 'sin_doy'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## One Station",
   "id": "e082b2a18d2a4050"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:00:57.183603Z",
     "start_time": "2025-03-01T17:00:57.123903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "one_station_X = train[0][train[0][\"station_id\"]==1]\n",
    "one_station_y = train[1][train[1][\"station_id\"]==1]\n",
    "\n",
    "one_station_X = one_station_X.drop(\"station_id\", axis=1)\n",
    "one_station_y = one_station_y.drop(\"station_id\", axis=1)\n",
    "\n",
    "print(one_station_X)\n",
    "print(one_station_X.shape)\n",
    "print(one_station_y.shape)\n",
    "\n",
    "# test_rf\n",
    "s1_test_rf_X = test_rf[0][test_rf[0][\"station_id\"]==1]\n",
    "s1_test_rf_y = test_rf[1][test_rf[1][\"station_id\"]==1]\n",
    "\n",
    "s1_test_rf_X = s1_test_rf_X.drop(\"station_id\", axis=1)\n",
    "s1_test_rf_y = s1_test_rf_y.drop(\"station_id\", axis=1)\n",
    "\n",
    "# test_f\n",
    "s1_test_f_X = test_f[0][test_f[0][\"station_id\"]==1]\n",
    "s1_test_f_y = test_f[1][test_f[1][\"station_id\"]==1]\n",
    "\n",
    "s1_test_f_X = s1_test_f_X.drop(\"station_id\", axis=1)\n",
    "s1_test_f_y = s1_test_f_y.drop(\"station_id\", axis=1)"
   ],
   "id": "5318bf9a70ee77e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        model_orography  station_altitude  station_latitude  \\\n",
      "1             -0.737002           -0.7786          1.016052   \n",
      "123           -0.737002           -0.7786          1.016052   \n",
      "245           -0.737002           -0.7786          1.016052   \n",
      "367           -0.737002           -0.7786          1.016052   \n",
      "489           -0.737002           -0.7786          1.016052   \n",
      "...                 ...               ...               ...   \n",
      "420047        -0.737002           -0.7786          1.016052   \n",
      "420169        -0.737002           -0.7786          1.016052   \n",
      "420291        -0.737002           -0.7786          1.016052   \n",
      "420413        -0.737002           -0.7786          1.016052   \n",
      "420535        -0.737002           -0.7786          1.016052   \n",
      "\n",
      "        station_longitude  cape_mean  cape_std   sd_mean    sd_std  stl1_mean  \\\n",
      "1                -0.89124  -0.164696 -0.233030 -0.138945 -0.130389  -0.465046   \n",
      "123              -0.89124  -0.127184 -0.138486 -0.138945 -0.130389  -0.443237   \n",
      "245              -0.89124  -0.157128 -0.205144 -0.138945 -0.130389  -0.493848   \n",
      "367              -0.89124  -0.179347 -0.224208 -0.138569 -0.118298  -0.666115   \n",
      "489              -0.89124  -0.194627 -0.269349 -0.128484 -0.094832  -1.032942   \n",
      "...                   ...        ...       ...       ...       ...        ...   \n",
      "420047           -0.89124  -0.180904 -0.217095 -0.138945 -0.130389  -0.408190   \n",
      "420169           -0.89124  -0.194627 -0.269349  0.049665  0.074999  -1.481411   \n",
      "420291           -0.89124  -0.031851 -0.021526 -0.138945 -0.130389  -0.087933   \n",
      "420413           -0.89124  -0.193998 -0.266479  0.195411  0.577944  -1.399988   \n",
      "420535           -0.89124  -0.190646 -0.252057 -0.134015 -0.125014  -1.370927   \n",
      "\n",
      "        stl1_std  ...    q_mean     q_std    u_mean     u_std    v_mean  \\\n",
      "1       1.073037  ...  0.433324  0.358571 -0.746920  5.432942  1.666245   \n",
      "123    -0.729082  ... -0.331529 -0.091183 -0.846796  0.530674  0.527320   \n",
      "245    -0.433906  ...  0.209089  0.499193  0.630721  1.430988  2.049817   \n",
      "367     0.323421  ... -0.490498 -0.462935  0.708757  1.573345 -1.295278   \n",
      "489    -0.800538  ... -1.349110 -0.811540 -0.002433  2.239311 -2.185997   \n",
      "...          ...  ...       ...       ...       ...       ...       ...   \n",
      "420047 -0.003070  ... -0.810062  0.193430  0.240393 -0.045242  0.851824   \n",
      "420169 -0.919173  ... -0.777223  0.669660 -0.460586 -0.380500  0.012031   \n",
      "420291 -0.913360  ...  0.608128  0.277408 -0.048647  2.699677  0.350894   \n",
      "420413 -0.529402  ... -0.944738 -0.154593  1.177332  1.458282  0.345556   \n",
      "420535  0.409809  ... -1.229235 -1.199663  0.113013 -0.301731  0.078417   \n",
      "\n",
      "           v_std    t_mean     t_std   cos_doy       sin_doy  \n",
      "1       2.520527 -0.757580  1.563501  0.999407  3.442161e-02  \n",
      "123     0.694997 -0.886288  0.894268  0.996298  8.596480e-02  \n",
      "245     1.632936 -0.570165 -0.268864  0.988023  1.543088e-01  \n",
      "367     1.865991 -1.100426  0.275097  0.978740  2.051045e-01  \n",
      "489     0.384951 -2.033739 -0.047349  0.962309  2.719582e-01  \n",
      "...          ...       ...       ...       ...           ...  \n",
      "420047 -0.678904 -0.126731 -0.880790  0.992749 -1.202080e-01  \n",
      "420169 -0.118070 -1.776431  0.891437  0.994671 -1.031017e-01  \n",
      "420291  0.586493 -0.020650  0.483000  0.997630 -6.880243e-02  \n",
      "420413  0.454055 -1.693830 -0.947345  0.998667 -5.161967e-02  \n",
      "420535 -0.118211 -0.965405 -0.356925  1.000000  6.432491e-16  \n",
      "\n",
      "[3448 rows x 64 columns]\n",
      "(3448, 64)\n",
      "(3448, 2)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### One Station MSE and CRPS NNs\n",
    "Station (station_id=1) with one hidden layer and loss functions MSE or CRPS"
   ],
   "id": "241d282af9a75045"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T17:01:03.531640Z",
     "start_time": "2025-03-01T17:01:03.515547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSEStationNN(L.LightningModule):\n",
    "    def __init__(self, in_feat, hidden_size, optimizer_class, optimizer_params):\n",
    "        super(MSEStationNN, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear_t2m = torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_t2m(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx): # unterschied zwischen predict und test_step?\n",
    "        x, y = batch # wieso hat test_step auch y? => um score zu berechnen\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        return loss\n"
   ],
   "id": "89a9fc3e00e76201",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T18:17:01.405759Z",
     "start_time": "2025-03-01T18:17:01.389988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CRPSStationNN(L.LightningModule):\n",
    "    def __init__(self, in_feat, hidden_size, optimizer_class, optimizer_params):\n",
    "        super(CRPSStationNN, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        #self.linear_t2m = torch.nn.Linear(in_features=hidden_size, out_features=2) => wieso nicht direkt 2 outputs?\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.last_linear_mu = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.last_linear_sigma = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        self.loss_fn = NormalCRPS()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        mu = self.last_linear_mu(x)\n",
    "        sigma = self.softplus(self.last_linear_sigma(x))\n",
    "        res = torch.cat([mu, sigma], dim=1)\n",
    "        return res\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=y.flatten())\n",
    "        print(f'test_loss: {loss}')\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat\n"
   ],
   "id": "2a867835aca7d93c",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train One Station NN (MSE or CRPS)",
   "id": "85bca1426de9002b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T18:17:11.780930Z",
     "start_time": "2025-03-01T18:17:04.139103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with wandb.init(\n",
    "    project=\"exploration\",\n",
    "    id = f\"training_run_24h_crps\",\n",
    "    tags=[\"exploration\"],\n",
    "):\n",
    "\n",
    "    y_scaler = StandardScaler(with_std=False) # wieso scalen wir überhaupt? => robuster?\n",
    "    y_scaler = y_scaler.fit(one_station_y[[\"t2m\"]])\n",
    "\n",
    "    batch_size = 512\n",
    "    hidden_size=128\n",
    "    lr=0.0002\n",
    "    max_epochs=31\n",
    "    in_feat = one_station_X.shape[1]\n",
    "\n",
    "    one_station_train_ds = TensorDataset(torch.Tensor(one_station_X.to_numpy()), torch.Tensor(y_scaler.transform(one_station_y[[\"t2m\"]])))\n",
    "    one_station_loader = DataLoader(one_station_train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    s1_test_rf_ds = TensorDataset(torch.Tensor(s1_test_rf_X.to_numpy()), torch.Tensor(y_scaler.transform(s1_test_rf_y[[\"t2m\"]])))\n",
    "    s1_test_rf_loader = DataLoader(s1_test_rf_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    one_station_nn = CRPSStationNN(\n",
    "        in_feat=in_feat,\n",
    "        hidden_size=hidden_size,\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params={\"lr\": lr}\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"one_station_crps\")\n",
    "\n",
    "    os_checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    one_station_trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=os_checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    value = one_station_trainer.fit(model=one_station_nn, train_dataloaders=one_station_loader, val_dataloaders=s1_test_rf_loader)\n",
    "\n",
    "    final_loss = one_station_trainer.logged_metrics[\"train_loss_step\"]\n",
    "    print(\"Final MSE Loss:\", final_loss)\n",
    "\n",
    "# wo finde ich den tatsaechlichen wert? => bei test, jetzt wird nur das Modell trainiert\n",
    "# validation step und test step\n",
    "# CRPS"
   ],
   "id": "4b50303c1b2a53dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250301_191704-training_run_24h_crps</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps' target=\"_blank\">training_run_24h_crps</a></strong> to <a href='https://wandb.ai/leachen_thesis/exploration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/exploration' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/drn_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type       | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | linear            | Linear     | 8.3 K  | train\n",
      "1 | relu              | ReLU       | 0      | train\n",
      "2 | softplus          | Softplus   | 0      | train\n",
      "3 | last_linear_mu    | Linear     | 129    | train\n",
      "4 | last_linear_sigma | Linear     | 129    | train\n",
      "5 | loss_fn           | NormalCRPS | 0      | train\n",
      "---------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [00:00<00:00, 82.25it/s, v_num=crps, train_loss_step=4.250]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 95.71it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 95.56it/s]\u001B[A\n",
      "Epoch 1: 100%|██████████| 7/7 [00:00<00:00, 65.82it/s, v_num=crps, train_loss_step=4.090, train_loss_epoch=4.210]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 112.24it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 103.46it/s]\u001B[A\n",
      "Epoch 2: 100%|██████████| 7/7 [00:00<00:00, 74.35it/s, v_num=crps, train_loss_step=3.830, train_loss_epoch=4.120]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 121.37it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 119.10it/s]\u001B[A\n",
      "Epoch 3: 100%|██████████| 7/7 [00:00<00:00, 60.05it/s, v_num=crps, train_loss_step=3.840, train_loss_epoch=4.030]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 88.85it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 92.38it/s]\u001B[A\n",
      "Epoch 4: 100%|██████████| 7/7 [00:00<00:00, 59.33it/s, v_num=crps, train_loss_step=3.500, train_loss_epoch=3.930]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 125.66it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 144.78it/s]\u001B[A\n",
      "Epoch 5: 100%|██████████| 7/7 [00:00<00:00, 70.91it/s, v_num=crps, train_loss_step=3.650, train_loss_epoch=3.840]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 134.95it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 142.17it/s]\u001B[A\n",
      "Epoch 6: 100%|██████████| 7/7 [00:00<00:00, 61.18it/s, v_num=crps, train_loss_step=3.810, train_loss_epoch=3.740]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 120.07it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 128.82it/s]\u001B[A\n",
      "Epoch 7: 100%|██████████| 7/7 [00:00<00:00, 55.15it/s, v_num=crps, train_loss_step=3.590, train_loss_epoch=3.640]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 111.52it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 105.01it/s]\u001B[A\n",
      "Epoch 8: 100%|██████████| 7/7 [00:00<00:00, 52.93it/s, v_num=crps, train_loss_step=3.330, train_loss_epoch=3.540]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 86.08it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 87.94it/s]\u001B[A\n",
      "Epoch 9: 100%|██████████| 7/7 [00:00<00:00, 54.26it/s, v_num=crps, train_loss_step=3.300, train_loss_epoch=3.430]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 86.01it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 87.79it/s]\u001B[A\n",
      "Epoch 10: 100%|██████████| 7/7 [00:00<00:00, 54.18it/s, v_num=crps, train_loss_step=3.340, train_loss_epoch=3.310]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 101.50it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 101.62it/s]\u001B[A\n",
      "Epoch 11: 100%|██████████| 7/7 [00:00<00:00, 52.27it/s, v_num=crps, train_loss_step=2.880, train_loss_epoch=3.190]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 107.82it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 100.72it/s]\u001B[A\n",
      "Epoch 12: 100%|██████████| 7/7 [00:00<00:00, 59.08it/s, v_num=crps, train_loss_step=2.850, train_loss_epoch=3.060]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 99.94it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 104.07it/s]\u001B[A\n",
      "Epoch 13: 100%|██████████| 7/7 [00:00<00:00, 70.87it/s, v_num=crps, train_loss_step=2.760, train_loss_epoch=2.930]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 98.32it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 107.49it/s]\u001B[A\n",
      "Epoch 14: 100%|██████████| 7/7 [00:00<00:00, 70.47it/s, v_num=crps, train_loss_step=2.660, train_loss_epoch=2.790]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 127.49it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 156.34it/s]\u001B[A\n",
      "Epoch 15: 100%|██████████| 7/7 [00:00<00:00, 77.57it/s, v_num=crps, train_loss_step=2.480, train_loss_epoch=2.650]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 98.98it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 109.58it/s]\u001B[A\n",
      "Epoch 16: 100%|██████████| 7/7 [00:00<00:00, 73.75it/s, v_num=crps, train_loss_step=2.440, train_loss_epoch=2.510]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 122.57it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 128.49it/s]\u001B[A\n",
      "Epoch 17: 100%|██████████| 7/7 [00:00<00:00, 58.29it/s, v_num=crps, train_loss_step=2.200, train_loss_epoch=2.370]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 96.60it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 95.48it/s]\u001B[A\n",
      "Epoch 18: 100%|██████████| 7/7 [00:00<00:00, 59.72it/s, v_num=crps, train_loss_step=2.070, train_loss_epoch=2.230]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 147.54it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 139.38it/s]\u001B[A\n",
      "Epoch 19: 100%|██████████| 7/7 [00:00<00:00, 63.61it/s, v_num=crps, train_loss_step=1.870, train_loss_epoch=2.100]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 117.59it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 124.87it/s]\u001B[A\n",
      "Epoch 20: 100%|██████████| 7/7 [00:00<00:00, 62.59it/s, v_num=crps, train_loss_step=1.720, train_loss_epoch=1.980]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 135.46it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 127.21it/s]\u001B[A\n",
      "Epoch 21: 100%|██████████| 7/7 [00:00<00:00, 64.31it/s, v_num=crps, train_loss_step=1.820, train_loss_epoch=1.870]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 111.16it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 111.55it/s]\u001B[A\n",
      "Epoch 22: 100%|██████████| 7/7 [00:00<00:00, 57.16it/s, v_num=crps, train_loss_step=1.550, train_loss_epoch=1.760]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 89.48it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 93.39it/s]\u001B[A\n",
      "Epoch 23: 100%|██████████| 7/7 [00:00<00:00, 52.18it/s, v_num=crps, train_loss_step=1.600, train_loss_epoch=1.670]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 87.18it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 90.81it/s]\u001B[A\n",
      "Epoch 24: 100%|██████████| 7/7 [00:00<00:00, 56.73it/s, v_num=crps, train_loss_step=1.500, train_loss_epoch=1.590]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 120.63it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 107.25it/s]\u001B[A\n",
      "Epoch 25: 100%|██████████| 7/7 [00:00<00:00, 58.59it/s, v_num=crps, train_loss_step=1.400, train_loss_epoch=1.520]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 119.40it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 117.45it/s]\u001B[A\n",
      "Epoch 26: 100%|██████████| 7/7 [00:00<00:00, 73.44it/s, v_num=crps, train_loss_step=1.410, train_loss_epoch=1.460]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 141.96it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 157.28it/s]\u001B[A\n",
      "Epoch 27: 100%|██████████| 7/7 [00:00<00:00, 77.22it/s, v_num=crps, train_loss_step=1.370, train_loss_epoch=1.410]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 95.82it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 98.11it/s]\u001B[A\n",
      "Epoch 28: 100%|██████████| 7/7 [00:00<00:00, 67.04it/s, v_num=crps, train_loss_step=1.320, train_loss_epoch=1.360]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 133.40it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 156.75it/s]\u001B[A\n",
      "Epoch 29: 100%|██████████| 7/7 [00:00<00:00, 51.51it/s, v_num=crps, train_loss_step=1.240, train_loss_epoch=1.320]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 128.67it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 149.67it/s]\u001B[A\n",
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 62.35it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.280]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 96.60it/s]\u001B[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 95.80it/s]\u001B[A\n",
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 44.76it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.250]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 7/7 [00:00<00:00, 41.98it/s, v_num=crps, train_loss_step=1.210, train_loss_epoch=1.250]\n",
      "Final MSE Loss: tensor(1.2118)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>███▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▇▇▇▇▆▇▆▇▆▆▇▆▅▅▅▅▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>1.25029</td></tr><tr><td>train_loss_step</td><td>1.21183</td></tr><tr><td>trainer/global_step</td><td>216</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_24h_crps</strong> at: <a href='https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration/runs/training_run_24h_crps</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/exploration' target=\"_blank\">https://wandb.ai/leachen_thesis/exploration</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250301_191704-training_run_24h_crps/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validate and test one station NN (MSE and CRPS)",
   "id": "a235b5768b328247"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T18:18:06.352717Z",
     "start_time": "2025-03-01T18:18:06.285337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# validation and test for both mse and crps\n",
    "s1_test_f_ds = TensorDataset(torch.Tensor(s1_test_f_X.to_numpy()), torch.Tensor(y_scaler.transform(s1_test_f_y[[\"t2m\"]])))\n",
    "s1_test_f_loader = DataLoader(s1_test_f_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "loss = one_station_trainer.test(model=one_station_nn, dataloaders=s1_test_f_loader)\n",
    "print(loss)\n"
   ],
   "id": "98751ab483dbdbad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]test_loss: 1.2276196479797363\n",
      "Testing DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 183.27it/s]test_loss: 1.2759742736816406\n",
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 131.92it/s]\n",
      "[{}]\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T19:06:19.952953Z",
     "start_time": "2025-03-01T19:06:19.900488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds_list = []\n",
    "preds = one_station_trainer.predict(model=one_station_nn, dataloaders=s1_test_f_loader)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "# Reverse transform of the y_scaler (only on the mean)\n",
    "preds[:, 0] = torch.Tensor(y_scaler.inverse_transform(preds[:, 0].view(-1, 1))).flatten()\n",
    "\n",
    "preds_list.append(preds)\n",
    "targets = s1_test_f_y\n",
    "targets = torch.Tensor(targets.t2m.values)\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = one_station_nn.loss_fn.crps(final_preds, targets)\n",
    "print(res)"
   ],
   "id": "3459bfa367dd8d7e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 258.93it/s]\n",
      "tensor(1.2421)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## All stations\n",
    "Deterministic NN with one hidden layer using MSE as loss and embeddings"
   ],
   "id": "b03ba0eb2efcc5ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T00:51:26.160383Z",
     "start_time": "2025-03-01T00:51:26.143810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nn mse loss with lightning\n",
    "class MyDRN(L.LightningModule):\n",
    "    def __init__(self, hidden_size, embedding_dim, in_feat, optimizer_class, optimizer_params):\n",
    "        super(MyDRN, self).__init__()\n",
    "        self.embedding = EmbedStations(num_stations_max=122, embedding_dim=embedding_dim)\n",
    "        self.linear = torch.nn.Linear(in_features=in_feat, out_features=hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear_t2m = torch.nn.Linear(in_features=hidden_size, out_features=1) # output t2m value\n",
    "\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #print(x.shape) # (8, 65)\n",
    "        x = self.embedding(x)\n",
    "        #print(f\"After embedding: {x.shape}\") # (8, 84)\n",
    "        x = self.linear(x)\n",
    "        #print(x.shape) # (8, 64)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.linear_t2m(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten()) # why y.flatten()?\n",
    "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        self.log(\"validation_loss\", loss.item(), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx): # unterschied zwischen predict und test_step?\n",
    "        x, y = batch # wieso hat test_step auch y?\n",
    "        y_hat = self.forward(x)\n",
    "        loss = self.loss(y_hat, y.flatten())\n",
    "        self.log(\"test_loss\", loss.item(), on_epoch=True, prog_bar=True)\n",
    "        return loss"
   ],
   "id": "44c786dfddc886f5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train All_station NN",
   "id": "42059a627190cbd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T00:54:14.513273Z",
     "start_time": "2025-03-01T00:51:34.099860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MyDRN train without wandb without saving\n",
    "\n",
    "y_scaler = StandardScaler(with_std=False)\n",
    "y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.Tensor(train[0].to_numpy()), torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]]))\n",
    ")\n",
    "\n",
    "#from params.json best_24h\n",
    "batch_size = 2048\n",
    "hidden_size=128\n",
    "lr=0.0002\n",
    "max_epochs=31\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "embed_dim = 20\n",
    "in_feat = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "mydrn = MyDRN(\n",
    "    hidden_size=hidden_size,\n",
    "    embedding_dim=embed_dim,\n",
    "    in_feat=in_feat,\n",
    "    optimizer_class=AdamW,\n",
    "    optimizer_params=dict(lr=lr),\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # dirpath=SAVEPATH, filename=f\"run_{args.id}\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=50,\n",
    "    accelerator=\"gpu\",\n",
    "    enable_progress_bar=True,\n",
    "    enable_model_summary=True,\n",
    "    callbacks=checkpoint_callback,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(model=mydrn, train_dataloaders=train_loader)\n"
   ],
   "id": "934d8d83f8e0536b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/drn_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type          | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | embedding  | EmbedStations | 2.4 K  | train\n",
      "1 | linear     | Linear        | 10.9 K | train\n",
      "2 | relu       | ReLU          | 0      | train\n",
      "3 | linear_t2m | Linear        | 129    | train\n",
      "4 | loss       | MSELoss       | 0      | train\n",
      "-----------------------------------------------------\n",
      "13.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "13.4 K    Total params\n",
      "0.054     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 5/195 [00:00<00:05, 32.69it/s, v_num=28, train_loss_step=42.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2048, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   3%|▎         | 5/195 [00:00<00:05, 34.61it/s, v_num=28, train_loss_step=41.90, train_loss_epoch=41.20]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([1554])) that is different to the input size (torch.Size([1554, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 195/195 [00:05<00:00, 37.23it/s, v_num=28, train_loss_step=39.70, train_loss_epoch=41.20]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 195/195 [00:05<00:00, 37.21it/s, v_num=28, train_loss_step=39.70, train_loss_epoch=41.20]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-25T10:44:53.834541Z",
     "start_time": "2025-02-25T10:42:25.653661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# with given models - funktioniert fuer summary statistics, aber nicht ohne? => woran liegt das?\n",
    "DIRECTORY = os.getcwd()\n",
    "JSONPATH = os.path.join(DIRECTORY, \"trained_models/drn_24h/params.json\")\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"trained_models/drn_24h/models\")\n",
    "\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "\n",
    "with wandb.init(\n",
    "    project=\"multigraph\",\n",
    "    # id=f\"training_run_drn_{args_dict['leadtime']}_{args.id}\",\n",
    "    id = f\"training_run_{args_dict['leadtime']}\",\n",
    "    config=args_dict,\n",
    "    tags=[\"final_training\"],\n",
    "):\n",
    "    config=wandb.config\n",
    "    dataframes = load_dataframes(mode=\"train\", leadtime=config.leadtime)\n",
    "    dataframes = summary_statistics(dataframes)\n",
    "    dataframes.pop(\"stations\")\n",
    "\n",
    "    # print(list(dataframes.values()))\n",
    "    for df in dataframes.values():\n",
    "        print(type(df))\n",
    "\n",
    "    for X, y in dataframes.values():\n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    train, valid_test = normalize_features(\n",
    "        training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    "    )\n",
    "\n",
    "    print(f\"dataframes['train']: {dataframes['train']}\")\n",
    "    print(f\"train: {train}\")\n",
    "\n",
    "    train = drop_nans(train)\n",
    "\n",
    "    y_scaler = StandardScaler(with_std=False)\n",
    "    y_scaler = y_scaler.fit(train[1][[\"t2m\"]])\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.Tensor(train[0].to_numpy()), torch.Tensor(y_scaler.transform(train[1][[\"t2m\"]]))\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    embed_dim = 20 # why 20? => embed stations - instead of station_id - map into a latent vector space\n",
    "    in_channels = train[0].shape[1] + embed_dim - 1\n",
    "\n",
    "    drn = DRN(\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels=config.hidden_channels,\n",
    "        embedding_dim=embed_dim,\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config.lr),\n",
    "    )\n",
    "    wandb_logger = WandbLogger(project=\"multigraph\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        # dirpath=SAVEPATH, filename=f\"run_{args.id}\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "        dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config.max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "    trainer.fit(model=drn, train_dataloaders=train_loader)\n"
   ],
   "id": "ea6cdae3efed6f40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/drn_24h/params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250225_114225-training_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">training_run_24h</a></strong> to <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "[INFO] Normalizing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/trained_models/drn_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type          | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | embedding         | EmbedStations | 2.4 K  | train\n",
      "1 | linear            | ModuleList    | 21.8 K | train\n",
      "2 | last_linear_mu    | Linear        | 257    | train\n",
      "3 | last_linear_sigma | Linear        | 257    | train\n",
      "4 | relu              | ReLU          | 0      | train\n",
      "5 | softplus          | Softplus      | 0      | train\n",
      "6 | loss_fn           | NormalCRPS    | 0      | train\n",
      "------------------------------------------------------------\n",
      "24.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "24.7 K    Total params\n",
      "0.099     Total estimated model params size (MB)\n",
      "9         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes['train']: (        station_id  model_orography  station_altitude  station_latitude  \\\n",
      "0                0        -0.738289         -0.764101          1.382904   \n",
      "1                1        -0.737002         -0.778600          1.016052   \n",
      "2                2        -0.731851         -0.733171          1.571141   \n",
      "3                3        -0.728793         -0.765712          1.661951   \n",
      "4                4        -0.724769         -0.761846          0.884948   \n",
      "...            ...              ...               ...               ...   \n",
      "420651         117         0.914135          0.298485         -2.911765   \n",
      "420652         118         1.443053          0.598123         -1.881973   \n",
      "420653         119         2.338639          0.646451         -2.021799   \n",
      "420654         120         4.799571          3.994016         -2.028314   \n",
      "420655         121         5.913903          4.345205         -2.201382   \n",
      "\n",
      "        station_longitude  cape_mean  cape_std   sd_mean    sd_std  stl1_mean  \\\n",
      "0               -0.895456  -0.162404 -0.211540 -0.138945 -0.130389  -0.065441   \n",
      "1               -0.891240  -0.164696 -0.233030 -0.138945 -0.130389  -0.465046   \n",
      "2               -0.829873  -0.181057 -0.231899 -0.138945 -0.130389  -0.105207   \n",
      "3               -0.630782  -0.186269 -0.245221 -0.138945 -0.130389  -0.069467   \n",
      "4               -0.708545  -0.161323 -0.233911 -0.138945 -0.130389  -0.526710   \n",
      "...                   ...        ...       ...       ...       ...        ...   \n",
      "420651          -1.659809  -0.194591 -0.269107 -0.138930 -0.130201  -0.492037   \n",
      "420652           1.430446  -0.163949 -0.223208  0.032781  0.187163  -1.013463   \n",
      "420653           1.362001  -0.102331 -0.162280  0.395475  0.354853  -1.218398   \n",
      "420654           1.636564  -0.096942 -0.130522  4.413006  6.996919  -1.389498   \n",
      "420655           1.636304   0.030651  0.044717  6.780401  8.101624  -1.560089   \n",
      "\n",
      "        ...    q_mean     q_std    u_mean     u_std    v_mean     v_std  \\\n",
      "0       ... -0.114795  1.470589 -1.416661  0.670551  1.931902  1.831306   \n",
      "1       ...  0.433324  0.358571 -0.746920  5.432942  1.666245  2.520527   \n",
      "2       ... -0.505220  1.064439 -1.522950 -0.090761  1.836820  2.540884   \n",
      "3       ... -0.754990  0.193510 -1.566283 -0.304501  1.628440  1.460403   \n",
      "4       ...  0.398675  0.609186 -0.647017  5.686270  1.696099  3.035443   \n",
      "...     ...       ...       ...       ...       ...       ...       ...   \n",
      "420651  ... -1.016872 -0.799413  0.360431 -1.581705 -0.965468 -0.314380   \n",
      "420652  ...  0.058197 -0.856260  0.196157 -0.088442 -0.267489 -1.145218   \n",
      "420653  ...  0.047738 -0.957247  0.136901 -0.865069 -0.395362 -1.303312   \n",
      "420654  ...  0.075580 -1.053509  0.060473 -0.875901 -0.419308 -1.240052   \n",
      "420655  ...  0.052665 -0.968504 -0.181117 -0.672254 -0.557093 -1.209932   \n",
      "\n",
      "          t_mean     t_std   cos_doy       sin_doy  \n",
      "0      -0.801722  0.618363  0.999407  3.442161e-02  \n",
      "1      -0.757580  1.563501  0.999407  3.442161e-02  \n",
      "2      -0.771376  0.765197  0.999407  3.442161e-02  \n",
      "3      -0.745605  0.401920  0.999407  3.442161e-02  \n",
      "4      -0.772579  0.788204  0.999407  3.442161e-02  \n",
      "...          ...       ...       ...           ...  \n",
      "420651 -0.445756 -0.222885  1.000000  6.432491e-16  \n",
      "420652 -0.731970 -1.510291  1.000000  6.432491e-16  \n",
      "420653 -0.684412 -1.302932  1.000000  6.432491e-16  \n",
      "420654 -0.665653 -1.386536  1.000000  6.432491e-16  \n",
      "420655 -0.630435 -1.469852  1.000000  6.432491e-16  \n",
      "\n",
      "[420656 rows x 65 columns],              time  station_id     t2m\n",
      "0      1997-01-02           0  277.75\n",
      "1      1997-01-02           1  279.55\n",
      "2      1997-01-02           2  276.45\n",
      "3      1997-01-02           3  275.75\n",
      "4      1997-01-02           4  279.35\n",
      "...           ...         ...     ...\n",
      "420651 2013-12-31         117  281.35\n",
      "420652 2013-12-31         118  279.35\n",
      "420653 2013-12-31         119  278.25\n",
      "420654 2013-12-31         120  273.15\n",
      "420655 2013-12-31         121  272.65\n",
      "\n",
      "[420656 rows x 3 columns])\n",
      "train: (        station_id  model_orography  station_altitude  station_latitude  \\\n",
      "0                0        -0.738289         -0.764101          1.382904   \n",
      "1                1        -0.737002         -0.778600          1.016052   \n",
      "2                2        -0.731851         -0.733171          1.571141   \n",
      "3                3        -0.728793         -0.765712          1.661951   \n",
      "4                4        -0.724769         -0.761846          0.884948   \n",
      "...            ...              ...               ...               ...   \n",
      "420651         117         0.914135          0.298485         -2.911765   \n",
      "420652         118         1.443053          0.598123         -1.881973   \n",
      "420653         119         2.338639          0.646451         -2.021799   \n",
      "420654         120         4.799571          3.994016         -2.028314   \n",
      "420655         121         5.913903          4.345205         -2.201382   \n",
      "\n",
      "        station_longitude  cape_mean  cape_std   sd_mean    sd_std  stl1_mean  \\\n",
      "0               -0.895456  -0.162404 -0.211540 -0.138945 -0.130389  -0.065441   \n",
      "1               -0.891240  -0.164696 -0.233030 -0.138945 -0.130389  -0.465046   \n",
      "2               -0.829873  -0.181057 -0.231899 -0.138945 -0.130389  -0.105207   \n",
      "3               -0.630782  -0.186269 -0.245221 -0.138945 -0.130389  -0.069467   \n",
      "4               -0.708545  -0.161323 -0.233911 -0.138945 -0.130389  -0.526710   \n",
      "...                   ...        ...       ...       ...       ...        ...   \n",
      "420651          -1.659809  -0.194591 -0.269107 -0.138930 -0.130201  -0.492037   \n",
      "420652           1.430446  -0.163949 -0.223208  0.032781  0.187163  -1.013463   \n",
      "420653           1.362001  -0.102331 -0.162280  0.395475  0.354853  -1.218398   \n",
      "420654           1.636564  -0.096942 -0.130522  4.413006  6.996919  -1.389498   \n",
      "420655           1.636304   0.030651  0.044717  6.780401  8.101624  -1.560089   \n",
      "\n",
      "        ...    q_mean     q_std    u_mean     u_std    v_mean     v_std  \\\n",
      "0       ... -0.114795  1.470589 -1.416661  0.670551  1.931902  1.831306   \n",
      "1       ...  0.433324  0.358571 -0.746920  5.432942  1.666245  2.520527   \n",
      "2       ... -0.505220  1.064439 -1.522950 -0.090761  1.836820  2.540884   \n",
      "3       ... -0.754990  0.193510 -1.566283 -0.304501  1.628440  1.460403   \n",
      "4       ...  0.398675  0.609186 -0.647017  5.686270  1.696099  3.035443   \n",
      "...     ...       ...       ...       ...       ...       ...       ...   \n",
      "420651  ... -1.016872 -0.799413  0.360431 -1.581705 -0.965468 -0.314380   \n",
      "420652  ...  0.058197 -0.856260  0.196157 -0.088442 -0.267489 -1.145218   \n",
      "420653  ...  0.047738 -0.957247  0.136901 -0.865069 -0.395362 -1.303312   \n",
      "420654  ...  0.075580 -1.053509  0.060473 -0.875901 -0.419308 -1.240052   \n",
      "420655  ...  0.052665 -0.968504 -0.181117 -0.672254 -0.557093 -1.209932   \n",
      "\n",
      "          t_mean     t_std   cos_doy       sin_doy  \n",
      "0      -0.801722  0.618363  0.999407  3.442161e-02  \n",
      "1      -0.757580  1.563501  0.999407  3.442161e-02  \n",
      "2      -0.771376  0.765197  0.999407  3.442161e-02  \n",
      "3      -0.745605  0.401920  0.999407  3.442161e-02  \n",
      "4      -0.772579  0.788204  0.999407  3.442161e-02  \n",
      "...          ...       ...       ...           ...  \n",
      "420651 -0.445756 -0.222885  1.000000  6.432491e-16  \n",
      "420652 -0.731970 -1.510291  1.000000  6.432491e-16  \n",
      "420653 -0.684412 -1.302932  1.000000  6.432491e-16  \n",
      "420654 -0.665653 -1.386536  1.000000  6.432491e-16  \n",
      "420655 -0.630435 -1.469852  1.000000  6.432491e-16  \n",
      "\n",
      "[420656 rows x 65 columns],              time  station_id     t2m\n",
      "0      1997-01-02           0  277.75\n",
      "1      1997-01-02           1  279.55\n",
      "2      1997-01-02           2  276.45\n",
      "3      1997-01-02           3  275.75\n",
      "4      1997-01-02           4  279.35\n",
      "...           ...         ...     ...\n",
      "420651 2013-12-31         117  281.35\n",
      "420652 2013-12-31         118  279.35\n",
      "420653 2013-12-31         119  278.25\n",
      "420654 2013-12-31         120  273.15\n",
      "420655 2013-12-31         121  272.65\n",
      "\n",
      "[420656 rows x 3 columns])\n",
      "Epoch 0:   0%|          | 0/52582 [12:45<?, ?it/s]it/s, v_num=_24h, train_loss_step=4.930]\n",
      "Epoch 25: 100%|██████████| 98/98 [00:05<00:00, 17.94it/s, v_num=_24h, train_loss_step=0.580, train_loss_epoch=0.597]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=26` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 98/98 [00:05<00:00, 17.92it/s, v_num=_24h, train_loss_step=0.580, train_loss_epoch=0.597]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▄▄▃▃▃▂▃▃▂▃▂▂▂▂▃▂▂▂▂▂▂▃▂▂▂▂▂▁▁▂▁▂▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.59687</td></tr><tr><td>train_loss_step</td><td>0.58027</td></tr><tr><td>trainer/global_step</td><td>2547</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_24h</strong> at: <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250225_114225-training_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
