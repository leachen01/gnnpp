{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T05:43:24.970057Z",
     "start_time": "2025-05-12T05:43:20.773031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /home/ltchen/gnnpp\n",
    "import sys\n",
    "import os\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch_geometric\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "from typing import Tuple\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import scatter\n",
    "from torch.nn import Linear, ModuleList, ReLU\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from models.loss import NormalCRPS\n",
    "from models.model_utils import MakePositive, EmbedStations\n",
    "from utils.data import (\n",
    "    load_dataframes,\n",
    "    summary_statistics,\n",
    ")\n",
    "from exploration.graph_creation import *\n",
    "from models.graphensemble.multigraph import *\n",
    "from exploration.get_graphs_and_data import *\n",
    "from exploration.explainability_utils import *"
   ],
   "id": "1615f8bc5f2693b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 72h Leadtime Graphs\n",
    "- adjust distance 2 and distance 3 metrics!!"
   ],
   "id": "835337a386143311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:58:50.253137Z",
     "start_time": "2025-05-12T22:58:44.287116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "leadtime = \"120h\"\n",
    "graph_name = \"g5\"\n",
    "# data_type = \"f\"\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "DIRECTORY = os.getcwd()\n",
    "\n",
    "JSONPATH, SAVEPATH, RESULTPATH = get_json_save_result_paths(leadtime=leadtime, graph_name=graph_name)\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "config = args_dict\n",
    "\n",
    "dataframes = load_dataframes(leadtime=leadtime)\n",
    "dataframes = summary_statistics(dataframes)\n",
    "g_train_rf, g_valid_rf, g_test_rf, g_test_f = get_train_valid_graph_data(leadtime=leadtime, graph_name=graph_name)"
   ],
   "id": "1eafdbb7ea5a0ed3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/params.json\n",
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for valid\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "Loading precomputed graph data on g5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pycharm_project_408/exploration/get_graphs_and_data.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_path)\n",
      "/tmp/pycharm_project_408/exploration/get_graphs_and_data.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid_data = torch.load(valid_path)\n",
      "/tmp/pycharm_project_408/exploration/get_graphs_and_data.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_rf = torch.load(test_rf_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded precomputed data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/pycharm_project_408/exploration/get_graphs_and_data.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_f = torch.load(test_f_path)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T22:58:50.291012Z",
     "start_time": "2025-05-12T22:58:50.260141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g_train_loader = DataLoader(g_train_rf, batch_size=config['batch_size'], shuffle=True)\n",
    "g_valid_loader = DataLoader(g_valid_rf, batch_size=config['batch_size'], shuffle=True)\n",
    "g_test_f_loader = DataLoader(g_test_f, batch_size=config['batch_size'], shuffle=False)\n",
    "g_test_rf_loader = DataLoader(g_test_rf, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "train_loader = g_train_loader\n",
    "valid_loader = g_valid_loader\n",
    "test_f_loader = g_test_f_loader\n",
    "test_rf_loader = g_test_rf_loader\n",
    "test_loader = [test_f_loader, test_rf_loader]\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = g_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = g_train_rf[0].num_edge_features\n",
    "num_nodes = g_train_rf[0].num_nodes\n",
    "max_epochs = 100"
   ],
   "id": "49de90627d2b6028",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:20:35.436099Z",
     "start_time": "2025-05-12T22:58:51.107277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = f\"gnn_run_{leadtime}\"\n",
    "\n",
    "for i in range(0, 10):\n",
    "    # FILENAME = graph_name + f\"run{i}_\" + leadtime\n",
    "    TRAINNAME = f\"{graph_name}_{leadtime}_train_run{i}\"\n",
    "\n",
    "    with wandb.init(\n",
    "            project=PROJECTNAME, id=TRAINNAME, config=args_dict, tags=[\"final\"], resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=num_nodes, #\n",
    "            embedding_dim=emb_dim,\n",
    "            edge_dim=edge_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config['gnn_hidden'],\n",
    "            out_channels_gnn=config['gnn_hidden'],\n",
    "            num_layers_gnn=config['gnn_layers'],\n",
    "            heads=config['heads'],\n",
    "            hidden_channels_deepset=config['gnn_hidden'],\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config['lr']),\n",
    "        )\n",
    "        torch.compile(multigraph)\n",
    "        batch = next(iter(train_loader))\n",
    "        multigraph.forward(batch)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=0)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "                max_epochs=max_epochs,\n",
    "                log_every_n_steps=1,\n",
    "                accelerator=\"gpu\",\n",
    "                devices = 1,\n",
    "                enable_progress_bar=True,\n",
    "                logger=wandb_logger,\n",
    "                callbacks=[early_stop, progress_bar, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=multigraph, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "id": "74003f2df57fc4e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_005851-g5_120h_train_run0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run0' target=\"_blank\">g5_120h_train_run0</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▄▅▅▃▃▆█▅▄▄▄▃▁▄▂▁▃▅▃▄▇▃▄▃▅▂▂▄▃▄▃▃▂▃▂▃▂▃▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▇▃▃▂▂▂▃▁▁▂▂▂▅▂▂▂▁▁▂▁▂▁▃▄▃▃▃▂▅▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>train_loss_epoch</td><td>1.02921</td></tr><tr><td>train_loss_step</td><td>1.00234</td></tr><tr><td>trainer/global_step</td><td>10790</td></tr><tr><td>val_loss</td><td>1.27656</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run0</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run0</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_005851-g5_120h_train_run0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_010139-g5_120h_train_run1</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run1' target=\"_blank\">g5_120h_train_run1</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▆▅▆█▄▃▇▃▄▃▂▄▄▅▂▃▁▂▅▃▄▂▃▃▂▃▄▄▄▅▅▂▄▅▁▁▂▃▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▄▃▅▂▁▁▁▁▂▂▂▃▁▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>train_loss_epoch</td><td>1.11384</td></tr><tr><td>train_loss_step</td><td>1.18005</td></tr><tr><td>trainer/global_step</td><td>5885</td></tr><tr><td>val_loss</td><td>1.23928</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run1</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run1</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_010139-g5_120h_train_run1/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_010312-g5_120h_train_run2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run2' target=\"_blank\">g5_120h_train_run2</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▄▃▂▃▂▂▄▃▂▁▁▄▄▁▁▂▂▁▁▁▁▃▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>1.07194</td></tr><tr><td>train_loss_step</td><td>1.15164</td></tr><tr><td>trainer/global_step</td><td>8501</td></tr><tr><td>val_loss</td><td>1.2382</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run2</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run2</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_010312-g5_120h_train_run2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_010525-g5_120h_train_run3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run3' target=\"_blank\">g5_120h_train_run3</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▂▄▄▆▃▃▂▄▄█▃▁▂▄▃▂▅▂▃▆▂▁▃▁▂▃▁▃▁▄▂▁▆▂▂▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▅▃▂▃▂▂▂▁▁▂▁▃▂▂▁▃▃▄▃▂▄▂▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_loss_epoch</td><td>1.06921</td></tr><tr><td>train_loss_step</td><td>0.96641</td></tr><tr><td>trainer/global_step</td><td>9155</td></tr><tr><td>val_loss</td><td>1.28845</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run3</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run3</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_010525-g5_120h_train_run3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_010749-g5_120h_train_run4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run4' target=\"_blank\">g5_120h_train_run4</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▃▂▂▂▂▃▃▁▂▁▁▁▂▂▃▂▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>21</td></tr><tr><td>train_loss_epoch</td><td>1.08824</td></tr><tr><td>train_loss_step</td><td>1.09584</td></tr><tr><td>trainer/global_step</td><td>7193</td></tr><tr><td>val_loss</td><td>1.2272</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run4</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run4</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_010749-g5_120h_train_run4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_010942-g5_120h_train_run5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run5' target=\"_blank\">g5_120h_train_run5</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▄▂▄▃▃▄▃▂▅▃▇▃█▂▄▂▃▄▁▂▄▂▃▅▃▃▃▅▃▇▃▁▄▂▃▃▃▃▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▂▁▁▁▁▂▂▁▃▁▁▃▂▂▁▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>1.07081</td></tr><tr><td>train_loss_step</td><td>1.71874</td></tr><tr><td>trainer/global_step</td><td>8828</td></tr><tr><td>val_loss</td><td>1.24504</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run5</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run5</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_010942-g5_120h_train_run5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "creating run (0.3s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_011201-g5_120h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run6' target=\"_blank\">g5_120h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▄▆▄▇▇▅▄▃▆▃▃▄▃▅▂▆▅▃▄▃▅▅▅█▄▄▂▃▂▃▅▂▃▅▅▅▅▃▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▄▂▂▂▂▂▂▃▂▁▁▃▂▁▃▁▁▂▂▂▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_loss_epoch</td><td>1.06075</td></tr><tr><td>train_loss_step</td><td>1.09199</td></tr><tr><td>trainer/global_step</td><td>9155</td></tr><tr><td>val_loss</td><td>1.22923</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_011201-g5_120h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_011427-g5_120h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run7' target=\"_blank\">g5_120h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▄▅▄▅▄▄▆▄▄▂▆▂▁▅▁▃▅▅▅▆▄▂▂▂▂▄▅▂▅▄▅▃▁▄▃█▁▃▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▄▄▄▂▄▂▄▄▂▂▃▁▂▂▂▅▂▂▃▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>train_loss_epoch</td><td>1.08156</td></tr><tr><td>train_loss_step</td><td>0.75471</td></tr><tr><td>trainer/global_step</td><td>7520</td></tr><tr><td>val_loss</td><td>1.25241</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_011427-g5_120h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_011628-g5_120h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run8' target=\"_blank\">g5_120h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▂▂▁▁▂▂▂▂▁▁▂▂▂▂▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▂▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▄▃▂▃▂▃▂▃▁▁▃▁▂▁▁▁▂▂▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>train_loss_epoch</td><td>1.08988</td></tr><tr><td>train_loss_step</td><td>0.97633</td></tr><tr><td>trainer/global_step</td><td>7520</td></tr><tr><td>val_loss</td><td>1.25213</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_011628-g5_120h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250513_011830-g5_120h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run9' target=\"_blank\">g5_120h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 314 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "366 K     Trainable params\n",
      "0         Non-trainable params\n",
      "366 K     Total params\n",
      "1.467     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:475: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▅▄▂▂▄▅▂▃▂▃▃▂▂▂▃▄▂▃▂▄▄▂▃▂▃▂▂▃▁▂▂▃▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▂▃▂▂▁▂▄▄▃▁▁▁▂▁▃▁▂▄▇▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>train_loss_epoch</td><td>1.0733</td></tr><tr><td>train_loss_step</td><td>1.05442</td></tr><tr><td>trainer/global_step</td><td>7847</td></tr><tr><td>val_loss</td><td>1.23353</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_120h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h/runs/g5_120h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run_120h</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250513_011830-g5_120h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:20:51.569543Z",
     "start_time": "2025-05-12T23:20:35.460859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [\"f\", \"rf\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            # Load Model from checkpoint\n",
    "\n",
    "            multigraph = Multigraph.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                num_nodes=num_nodes,\n",
    "                embedding_dim=emb_dim,\n",
    "                edge_dim=edge_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels_gnn=config['gnn_hidden'],\n",
    "                out_channels_gnn=config['gnn_hidden'],\n",
    "                num_layers_gnn=config['gnn_layers'],\n",
    "                heads=config['heads'],\n",
    "                hidden_channels_deepset=config['gnn_hidden'],\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=config['lr']),\n",
    "            )\n",
    "            multigraph.eval()\n",
    "            batch = next(iter(train_loader))\n",
    "            batch = batch.to(\"cuda\")\n",
    "            multigraph.to(\"cuda\")\n",
    "            multigraph.forward(batch)\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "            ####################################################################################################\n",
    "            preds = trainer.predict(model=multigraph, dataloaders=[tl])\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            preds_list.append(preds)\n",
    "            print()\n",
    "            print(preds.shape)\n",
    "\n",
    "    targets = dataframes[f\"test_{data}\"][1]\n",
    "    targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "    res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps for {data}: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_{graph_name}_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")"
   ],
   "id": "8d4b30542ca0e0b3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g5_120h_train_run8.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 142.95it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 134.21it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 154.42it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 143.13it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 142.72it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 138.16it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 147.68it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 144.76it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 140.61it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_120h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 153.17it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps for f: 1.102037799315447\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g5_120h_train_run8.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 148.97it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 144.06it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 153.73it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 146.35it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 144.09it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 134.91it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 142.20it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 140.89it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 134.31it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_120h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 138.41it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps for rf: 1.1905915546993322\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_120h/g5_120h/\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- SAVEPATH for model saving\n",
    "- JSONPATH for parameters\n",
    "- RESULTPATH for test results (f.txt, f_results.csv, rf.txt., rf_results.csv)"
   ],
   "id": "7ad6aae06647f350"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 24h Leadtime Graphs",
   "id": "46ce2de1ac49cef0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-09T19:27:16.369308Z",
     "start_time": "2025-05-09T19:27:16.360154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "leadtime = \"24h\"\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "DIRECTORY = os.getcwd()\n",
    "JSONPATH = os.path.join(DIRECTORY, f\"trained_models/no_ensemble_{leadtime}/params.json\")\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "config = args_dict"
   ],
   "id": "d40c54361529c397",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/no_ensemble_24h/params.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''{\"batch_size\":8,\n",
    "\"gnn_hidden\":256,\n",
    "\"gnn_layers\":1,\n",
    "\"heads\":8,\n",
    "\"lr\":0.0001,\n",
    "\"max_dist\":50,\n",
    "\"max_epochs\": 23,\n",
    "\"remove_edges\": \"False\",\n",
    "\"only_summary\": \"True\"}'''"
   ],
   "id": "391e368b08fd18ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:50:54.512580Z",
     "start_time": "2025-05-06T00:50:49.608061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframes = load_dataframes(leadtime=leadtime)\n",
    "dataframes = summary_statistics(dataframes)"
   ],
   "id": "904e840fcebc3b74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for valid\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:50:54.596470Z",
     "start_time": "2025-05-06T00:50:54.582408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_train_valid_data():\n",
    "    train_path = 'exploration/graphs/train-AgalloEg100-a10-o4.pt'\n",
    "    valid_path = 'exploration/graphs/valid-AgalloEg100-a10-o4.pt'\n",
    "    test_rf_path = 'exploration/graphs/test_rf-AgalloEg100-a10-o4.pt'\n",
    "    test_f_path = 'exploration/graphs/test_f-AgalloEg100-a10-o4.pt'\n",
    "\n",
    "    if os.path.exists(train_path) and os.path.exists(valid_path):\n",
    "        print(\"Loading precomputed graph data...\")\n",
    "        try:\n",
    "            train_data = torch.load(train_path)\n",
    "            valid_data = torch.load(valid_path)\n",
    "            test_rf = torch.load(test_rf_path)\n",
    "            test_f = torch.load(test_f_path)\n",
    "            print(\"Successfully loaded precomputed data.\")\n",
    "            return train_data, valid_data, test_rf, test_f\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading precomputed data: {e}\")\n",
    "            print(\"Falling back to data preparation...\")\n",
    "    else:\n",
    "        print(\"Precomputed data not found.\")\n",
    "\n",
    "    print(\"Preparing data from scratch...\")\n",
    "    train_data, valid_data, test_rf, test_f = prepare_data()\n",
    "    return train_data, valid_data, test_rf, test_f\n",
    "\n",
    "def prepare_data():\n",
    "    leadtime = \"24h\"\n",
    "    dataframes = load_dataframes(leadtime=leadtime)\n",
    "    dataframes = summary_statistics(dataframes)\n",
    "    graph_name = \"g3\"\n",
    "    graphs3_train_rf, tests3 = normalize_features_and_create_graphs1(df_train=dataframes['train'],\n",
    "                                                                     df_valid_test=[dataframes['valid'], dataframes['test_rf'], dataframes['test_f']],\n",
    "                                                                     station_df=dataframes['stations'],attributes=[\"geo\", \"alt\", \"lon\", \"lat\",\"alt-orog\"],\n",
    "                                                                     edges=[(\"geo\", 100), (\"alt\", 10), (\"alt-orog\", 4)],\n",
    "                                                                     sum_stats=True)\n",
    "    graphs3_valid_rf, graphs3_test_rf, graphs3_test_f = tests3\n",
    "    os.makedirs('exploration/graphs', exist_ok=True)\n",
    "    torch.save(graphs3_train_rf, 'exploration/graphs/train-AgalloEg100-a10-o4.pt')\n",
    "    torch.save(graphs3_valid_rf, 'exploration/graphs/valid-AgalloEg100-a10-o4.pt')\n",
    "    torch.save(graphs3_test_rf, 'exploration/graphs/test_rf-AgalloEg100-a10-o4.pt')\n",
    "    torch.save(graphs3_test_f, 'exploration/graphs/test_f-AgalloEg100-a10-o4.pt')\n",
    "    return graphs3_train_rf, graphs3_valid_rf, graphs3_test_rf, graphs3_test_f"
   ],
   "id": "ad725891f883386e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:22:45.348794Z",
     "start_time": "2025-05-02T03:22:21.675653Z"
    }
   },
   "cell_type": "code",
   "source": "prepare_data()",
   "id": "bab95043b723d3be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:11<00:00, 220.52it/s]\n",
      "100%|██████████| 836/836 [00:03<00:00, 276.01it/s]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T03:53:31.267304Z",
     "start_time": "2025-05-02T03:53:30.206013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train, valid, testrf, testf = get_train_valid_data()\n",
    "print(valid[0])"
   ],
   "id": "5ea604f2af10f788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading precomputed graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_579418/2227381667.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_data = torch.load(train_path)\n",
      "/tmp/ipykernel_579418/2227381667.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid_data = torch.load(valid_path)\n",
      "/tmp/ipykernel_579418/2227381667.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_rf = torch.load(test_rf_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded precomputed data.\n",
      "Data(x=[120, 65], edge_index=[2, 2626], edge_attr=[2626, 5], y=[120], timestamp=2010-01-01 00:00:00, n_idx=[120])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_579418/2227381667.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_f = torch.load(test_f_path)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph 1",
   "id": "b1276267d753726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:51:28.228431Z",
     "start_time": "2025-05-06T00:50:59.953795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_name = \"g1\"\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}\")\n",
    "\n",
    "graphs1_train_rf, tests1 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['valid'], dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\"], edges=[(\"geo\", 50)], sum_stats = True)\n",
    "graphs1_valid_rf, graphs1_test_rf, graphs1_test_f = tests1\n",
    "\n",
    "g1_train_loader = DataLoader(graphs1_train_rf, batch_size=config['batch_size'], shuffle=True)\n",
    "g1_valid_loader = DataLoader(graphs1_valid_rf, batch_size=config['batch_size'], shuffle=False)\n",
    "g1_test_f_loader = DataLoader(graphs1_test_f, batch_size=config['batch_size'], shuffle=False)\n",
    "g1_test_rf_loader = DataLoader(graphs1_test_rf, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "train_loader = g1_train_loader\n",
    "valid_loader = g1_valid_loader\n",
    "test_f_loader = g1_test_f_loader\n",
    "test_rf_loader = g1_test_rf_loader\n",
    "test_loader = [test_f_loader, test_rf_loader]\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = graphs1_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs1_train_rf[0].num_edge_features\n",
    "num_nodes = graphs1_train_rf[0].num_nodes\n",
    "# max_epochs = max_epoch_list[graph_name]\n",
    "max_epochs = 100\n",
    "\n",
    "\n",
    "# embedding_dim = emb_dim\n",
    "# in_channels = in_channels\n",
    "# hidden_channels_gnn = config['gnn_hidden']\n",
    "# out_channels_gnn = config['gnn_hidden']\n",
    "# num_layers_gnn = config['gnn_hidden']\n",
    "# heads = config['heads']\n",
    "# hidden_channels_deepset = config['gnn_hidden']\n",
    "# optimizer_class = AdamW\n",
    "# optimizer_params = dict(lr=config['lr'])"
   ],
   "id": "70a4cfdc6550457c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "transform 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:11<00:00, 229.52it/s]\n",
      "100%|██████████| 836/836 [00:03<00:00, 255.32it/s]\n",
      "100%|██████████| 732/732 [00:02<00:00, 266.51it/s]\n",
      "100%|██████████| 730/730 [00:02<00:00, 248.84it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:51:28.238974Z",
     "start_time": "2025-05-06T00:51:28.233406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(SAVEPATH)\n",
    "print(RESULTPATH)"
   ],
   "id": "faa795d621c520ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T06:24:06.508251Z",
     "start_time": "2025-05-10T06:24:04.102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "if wandb.run is None:\n",
    "    wandb.init(project=\"your-project-name\")"
   ],
   "id": "1b280df78d82c388",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleachen01\u001B[0m (\u001B[33mleachen01-karlsruhe-institute-of-technology\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/tmp/pycharm_project_408/exploration/gnn/wandb/run-20250510_082405-r47naitt</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/your-project-name/runs/r47naitt' target=\"_blank\">cosmic-flower-1</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/your-project-name' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/your-project-name' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/your-project-name</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/your-project-name/runs/r47naitt' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/your-project-name/runs/r47naitt</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:07:48.086654Z",
     "start_time": "2025-05-04T22:45:49.419022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"gnn_run8\"\n",
    "\n",
    "for i in range(3, 10):\n",
    "    # FILENAME = graph_name + f\"run{i}_\" + leadtime\n",
    "    TRAINNAME = f\"{graph_name}_{leadtime}_train_run{i}\"\n",
    "\n",
    "    with wandb.init(\n",
    "            project=PROJECTNAME, id=TRAINNAME, config=args_dict, tags=[\"final\"], resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=num_nodes, #\n",
    "            embedding_dim=emb_dim,\n",
    "            edge_dim=edge_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config['gnn_hidden'],\n",
    "            out_channels_gnn=config['gnn_hidden'],\n",
    "            num_layers_gnn=config['gnn_layers'],\n",
    "            heads=config['heads'],\n",
    "            hidden_channels_deepset=config['gnn_hidden'],\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config['lr']),\n",
    "        )\n",
    "        torch.compile(multigraph)\n",
    "        batch = next(iter(train_loader))\n",
    "        multigraph.forward(batch)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=0)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "                max_epochs=max_epochs,\n",
    "                log_every_n_steps=1,\n",
    "                accelerator=\"gpu\",\n",
    "                devices = [1],\n",
    "                enable_progress_bar=True,\n",
    "                logger=wandb_logger,\n",
    "                callbacks=[early_stop, progress_bar, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=multigraph, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "id": "c80e0004160cd3e5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleachen01\u001B[0m (\u001B[33mleachen01-karlsruhe-institute-of-technology\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_004550-g1_24h_train_run3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run3' target=\"_blank\">g1_24h_train_run3</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▆▇▃▄▅▄█▄▅▄▅▄▃▅▄▃▅▆▂▅▄▅▄▄▄▅▃▃▃▃▃▃▄▅▁▄▂▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▂▃▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_loss_epoch</td><td>0.55642</td></tr><tr><td>train_loss_step</td><td>0.4572</td></tr><tr><td>trainer/global_step</td><td>12752</td></tr><tr><td>val_loss</td><td>0.65584</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run3</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run3</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_004550-g1_24h_train_run3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_004859-g1_24h_train_run4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run4' target=\"_blank\">g1_24h_train_run4</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▇▅▇▅▇▇▆▅▄▃▅▄▄▄▄▄▃▂▃▃▃▃▃▃▃▄▃▄▁▃▄▃▂▂▃▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▂▂▃▂▁▂▁▂▂▁▁▁▂▁▁▂▂▁▁▁▁▂▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_loss_epoch</td><td>0.56764</td></tr><tr><td>train_loss_step</td><td>0.58707</td></tr><tr><td>trainer/global_step</td><td>11444</td></tr><tr><td>val_loss</td><td>0.65889</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run4</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run4</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_004859-g1_24h_train_run4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_005147-g1_24h_train_run5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run5' target=\"_blank\">g1_24h_train_run5</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▃▅▄▄▅▄▄▃▅▂▃▃▃▂▃▃▃▄▃▃▂▂▂▃▃▂▃▁▂▂▂▁▃▂▂▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▂▁▂▁▂▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_loss_epoch</td><td>0.56943</td></tr><tr><td>train_loss_step</td><td>0.53146</td></tr><tr><td>trainer/global_step</td><td>11117</td></tr><tr><td>val_loss</td><td>0.65399</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run5</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run5</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_005147-g1_24h_train_run5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_005428-g1_24h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run6' target=\"_blank\">g1_24h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▄▆▅▄▃▄▃▃▃▅▂▄▄▂▃▃▃▄▃▃▂▃▃▃▂▂▄▃▃▁▃▃▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▃▂▂▂▂▂▂▂▂▂▁▃▁▁▂▂▂▁▁▁▁▂▁▁▂▁▁▁▁▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>44</td></tr><tr><td>train_loss_epoch</td><td>0.54875</td></tr><tr><td>train_loss_step</td><td>0.41778</td></tr><tr><td>trainer/global_step</td><td>14714</td></tr><tr><td>val_loss</td><td>0.66027</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_005428-g1_24h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_005802-g1_24h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run7' target=\"_blank\">g1_24h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▄▆▄▆▅▃▃▄▂▂▁▃▂▂▂▃▂▃▂▃▃▃▂▃▄▃▃▃▄▂▂▂▁▃▂▂▃▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▃▁▂▁▂▂▂▂▂▁▁▃▂▂▂▂▂▁▁▁▂▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>45</td></tr><tr><td>train_loss_epoch</td><td>0.54721</td></tr><tr><td>train_loss_step</td><td>0.46216</td></tr><tr><td>trainer/global_step</td><td>15041</td></tr><tr><td>val_loss</td><td>0.66592</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_005802-g1_24h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_010145-g1_24h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run8' target=\"_blank\">g1_24h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇██▄▆▃▃▃▅▆▅▂▁▃▇▃▆▃▃▅▅▁▃▄▄▅▂▃▄▂▄▂▃▂▄▄▁▅▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▂▁▃▁▂▂▁▁▁▂▁▁▁▂▁▁▁▂▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_loss_epoch</td><td>0.57732</td></tr><tr><td>train_loss_step</td><td>0.48267</td></tr><tr><td>trainer/global_step</td><td>11117</td></tr><tr><td>val_loss</td><td>0.66059</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010145-g1_24h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_010432-g1_24h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run9' target=\"_blank\">g1_24h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 878 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.317     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇█▃▅▃▃▃▂▂▃▃▄▅▂▃▂▄▄▂▃▄▂▂▂▃▄▃▂▁▂▂▃▂▄▂▃▂▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▂▁▁▁▁▁▁▂▁▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_loss_epoch</td><td>0.55405</td></tr><tr><td>train_loss_step</td><td>0.57807</td></tr><tr><td>trainer/global_step</td><td>13406</td></tr><tr><td>val_loss</td><td>0.66599</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_24h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g1_24h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_010432-g1_24h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:54:20.771319Z",
     "start_time": "2025-05-06T00:54:07.421567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [\"f\", \"rf\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            # Load Model from checkpoint\n",
    "\n",
    "            multigraph = Multigraph.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                num_nodes=num_nodes,\n",
    "                embedding_dim=emb_dim,\n",
    "                edge_dim=edge_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels_gnn=config['gnn_hidden'],\n",
    "                out_channels_gnn=config['gnn_hidden'],\n",
    "                num_layers_gnn=config['gnn_layers'],\n",
    "                heads=config['heads'],\n",
    "                hidden_channels_deepset=config['gnn_hidden'],\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=config['lr']),\n",
    "            )\n",
    "            multigraph.eval()\n",
    "            batch = next(iter(train_loader))\n",
    "            batch = batch.to(\"cuda\")\n",
    "            multigraph.to(\"cuda\")\n",
    "            multigraph.forward(batch)\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "            ####################################################################################################\n",
    "            preds = trainer.predict(model=multigraph, dataloaders=[tl])\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            preds_list.append(preds)\n",
    "            print()\n",
    "            print(preds.shape)\n",
    "\n",
    "    targets = dataframes[f\"test_{data}\"][1]\n",
    "    targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "    res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_{graph_name}_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")\n"
   ],
   "id": "93829b3c4384375e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g1_24h_train_run7.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 162.05it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 176.56it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 166.80it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 159.20it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 177.13it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 185.24it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 159.88it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 163.81it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 142.15it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g1_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 175.77it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6149606924107296\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g1_24h_train_run7.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 187.91it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 164.69it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 176.73it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 163.03it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 150.49it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 191.06it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 170.65it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 186.23it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 166.75it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g1_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 164.54it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6189686617958247\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T00:56:00.231140Z",
     "start_time": "2025-05-06T00:56:00.221201Z"
    }
   },
   "cell_type": "code",
   "source": "RESULTPATH",
   "id": "28688a60f3ebc5c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g1_24h'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph 2",
   "id": "f240ee08cf901f4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:15:37.187238Z",
     "start_time": "2025-05-04T23:15:01.276162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_name = \"g2\"\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}\")\n",
    "\n",
    "graphs2_train_rf, tests2 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['valid'], dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\", \"alt\", \"lon\", \"lat\", \"alt-orog\"], edges=[(\"geo\", 50)], sum_stats = True)\n",
    "graphs2_valid_rf, graphs2_test_rf, graphs2_test_f = tests2\n",
    "\n",
    "g2_train_loader = DataLoader(graphs2_train_rf, batch_size=config['batch_size'], shuffle=True)\n",
    "g2_valid_loader = DataLoader(graphs2_valid_rf, batch_size=config['batch_size'], shuffle=False)\n",
    "g2_test_f_loader = DataLoader(graphs2_test_f, batch_size=config['batch_size'], shuffle=False)\n",
    "g2_test_rf_loader = DataLoader(graphs2_test_rf, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "train_loader = g2_train_loader\n",
    "valid_loader = g2_valid_loader\n",
    "test_f_loader = g2_test_f_loader\n",
    "test_rf_loader = g2_test_rf_loader\n",
    "test_loader = [test_f_loader, test_rf_loader]\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = graphs2_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs2_train_rf[0].num_edge_features\n",
    "num_nodes = graphs2_train_rf[0].num_nodes\n",
    "# max_epochs = max_epoch_list[graph_name]\n",
    "max_epochs = 100\n",
    "\n",
    "# embedding_dim = emb_dim\n",
    "# in_channels = in_channels\n",
    "# hidden_channels_gnn = config['gnn_hidden']\n",
    "# out_channels_gnn = config['gnn_hidden']\n",
    "# num_layers_gnn = config['gnn_hidden']\n",
    "# heads = config['heads']\n",
    "# hidden_channels_deepset = config['gnn_hidden']\n",
    "# optimizer_class = AdamW\n",
    "# optimizer_params = dict(lr=config['lr'])"
   ],
   "id": "acb5cfc01177b7cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "transform 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:11<00:00, 222.63it/s]\n",
      "100%|██████████| 836/836 [00:03<00:00, 271.77it/s]\n",
      "100%|██████████| 732/732 [00:02<00:00, 264.64it/s]\n",
      "100%|██████████| 730/730 [00:02<00:00, 264.70it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:38:31.024103Z",
     "start_time": "2025-05-04T23:15:52.447130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"gnn_run8\"\n",
    "\n",
    "for i in range(3, 10):\n",
    "    TRAINNAME = f\"{graph_name}_{leadtime}_train_run{i}\"\n",
    "\n",
    "    with wandb.init(\n",
    "            project=PROJECTNAME, id=TRAINNAME, config=args_dict, tags=[\"final\"], resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=num_nodes,  #\n",
    "            embedding_dim=emb_dim,\n",
    "            edge_dim=edge_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config['gnn_hidden'],\n",
    "            out_channels_gnn=config['gnn_hidden'],\n",
    "            num_layers_gnn=config['gnn_layers'],\n",
    "            heads=config['heads'],\n",
    "            hidden_channels_deepset=config['gnn_hidden'],\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config['lr']),\n",
    "        )\n",
    "        torch.compile(multigraph)\n",
    "        batch = next(iter(train_loader))\n",
    "        multigraph.forward(batch)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=0)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=1,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[1],\n",
    "            enable_progress_bar=True,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[early_stop, progress_bar, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=multigraph, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "id": "349aad85fbe50166",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_011552-g2_24h_train_run3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run3' target=\"_blank\">g2_24h_train_run3</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▂▃▂▂▃▂▂▁▃▃▂▂▂▁▂▂▁▂▂▂▂▂▁▁▂▁▂▁▂▁▂▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▃▄▁▁▁▁▂▂▁▁▂▁▁▁▁▂▁▂▁▁▁▁▂▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_loss_epoch</td><td>0.55585</td></tr><tr><td>train_loss_step</td><td>0.55841</td></tr><tr><td>trainer/global_step</td><td>13733</td></tr><tr><td>val_loss</td><td>0.66641</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run3</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run3</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_011552-g2_24h_train_run3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_011920-g2_24h_train_run4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run4' target=\"_blank\">g2_24h_train_run4</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▃▃▂▂▂▂▂▂▂▂▁▂▂▂▃▂▁▂▂▂▁▁▁▂▁▁▂▂▂▁▂▂▂▁▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂▁▂▁▁▂▂▁▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_loss_epoch</td><td>0.55261</td></tr><tr><td>train_loss_step</td><td>0.59485</td></tr><tr><td>trainer/global_step</td><td>13733</td></tr><tr><td>val_loss</td><td>0.66614</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run4</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run4</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_011920-g2_24h_train_run4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_012247-g2_24h_train_run5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run5' target=\"_blank\">g2_24h_train_run5</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇▇▅▄▄▄▂▄▅▃▄▇▆▅▃█▃▃▄▃▂▃▅▄▃▄▂▄▁▃▂▁▁▃▁▄▄▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▃▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_loss_epoch</td><td>0.569</td></tr><tr><td>train_loss_step</td><td>0.60744</td></tr><tr><td>trainer/global_step</td><td>11771</td></tr><tr><td>val_loss</td><td>0.66909</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run5</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run5</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_012247-g2_24h_train_run5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_012545-g2_24h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run6' target=\"_blank\">g2_24h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▄▃▃▂▄▃▃▂▃▂▃▃▂▃▃▄▁▄▂▂▃▂▄▃▄▃▂▄▂▁▂▂▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▂▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>42</td></tr><tr><td>train_loss_epoch</td><td>0.54812</td></tr><tr><td>train_loss_step</td><td>0.50321</td></tr><tr><td>trainer/global_step</td><td>14060</td></tr><tr><td>val_loss</td><td>0.66395</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_012545-g2_24h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_012916-g2_24h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run7' target=\"_blank\">g2_24h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆█▅▆▃▂█▅▃▁▅▂▃▅▄▄▃▄▃▂▅▃▃▄▄▂▆▅▄▄▂▃▅▂▄▄▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▅▃▃▂▂▂▂▂▂▂▁▁▁▁▂▁▁▂▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_loss_epoch</td><td>0.5662</td></tr><tr><td>train_loss_step</td><td>0.48256</td></tr><tr><td>trainer/global_step</td><td>12098</td></tr><tr><td>val_loss</td><td>0.65238</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_012916-g2_24h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_013220-g2_24h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run8' target=\"_blank\">g2_24h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅█▅▅▅▃▂▆▄▅▆▄▄▄▃▃▄▂▂▄▆▃▄▅▄▂▃▄▆▁▂▄▃▃▃▄▄▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▃▂▂▂▃▂▃▂▃▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>31</td></tr><tr><td>train_loss_epoch</td><td>0.57556</td></tr><tr><td>train_loss_step</td><td>0.54007</td></tr><tr><td>trainer/global_step</td><td>10463</td></tr><tr><td>val_loss</td><td>0.65901</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_013220-g2_24h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_013459-g2_24h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run9' target=\"_blank\">g2_24h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 887 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 197 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.349     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▂▂▃▃▂▁▂▁▂▂▁▁▁▂▂▁▂▂▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▃▂▂▂▂▃▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>42</td></tr><tr><td>train_loss_epoch</td><td>0.5504</td></tr><tr><td>train_loss_step</td><td>0.54106</td></tr><tr><td>trainer/global_step</td><td>14060</td></tr><tr><td>val_loss</td><td>0.66237</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_24h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g2_24h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_013459-g2_24h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T23:39:38.596893Z",
     "start_time": "2025-05-04T23:39:24.865250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [\"f\", \"rf\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            # Load Model from checkpoint\n",
    "\n",
    "            multigraph = Multigraph.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                num_nodes=num_nodes,\n",
    "                embedding_dim=emb_dim,\n",
    "                edge_dim=edge_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels_gnn=config['gnn_hidden'],\n",
    "                out_channels_gnn=config['gnn_hidden'],\n",
    "                num_layers_gnn=config['gnn_layers'],\n",
    "                heads=config['heads'],\n",
    "                hidden_channels_deepset=config['gnn_hidden'],\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=config['lr']),\n",
    "            )\n",
    "            multigraph.eval()\n",
    "            batch = next(iter(train_loader))\n",
    "            batch = batch.to(\"cuda\")\n",
    "            multigraph.to(\"cuda\")\n",
    "            multigraph.forward(batch)\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "            ####################################################################################################\n",
    "            preds = trainer.predict(model=multigraph, dataloaders=[tl])\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            preds_list.append(preds)\n",
    "            print()\n",
    "            print(preds.shape)\n",
    "\n",
    "    targets = dataframes[f\"test_{data}\"][1]\n",
    "    targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "    res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_{graph_name}_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")"
   ],
   "id": "c7687ca3eef6cfa8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g2_24h_train_run1.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 173.98it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 184.12it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 164.77it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 165.32it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 157.18it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 153.78it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 171.07it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 158.34it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 144.33it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g2_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 160.20it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6126827844698789\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g2_24h_train_run1.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 153.28it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 153.28it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 170.70it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 160.78it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 168.57it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 162.60it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 147.87it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 177.91it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 197.10it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g2_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 168.35it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6193296252351883\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g2_24h\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph 3",
   "id": "c71f6c830162c808"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:42:53.622896Z",
     "start_time": "2025-05-05T01:42:53.614216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_name = \"g3\"\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}\")\n",
    "\n",
    "PARAMS = os.path.join(RESULTPATH, \"params.json\")\n",
    "with open(PARAMS, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {PARAMS}\")\n",
    "    args_dict = json.load(f)\n",
    "config_g3 = args_dict"
   ],
   "id": "41cbc5ab5126cb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/params.json\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T01:44:55.363532Z",
     "start_time": "2025-05-05T01:44:19.197874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# same number of edges for each attribute as Graph 1 (350 x 3)\n",
    "graphs3_train_rf, tests3 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['valid'], dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\", \"alt\", \"lon\", \"lat\", \"alt-orog\"], edges=[(\"geo\", 50), (\"alt\", 4),(\"alt-orog\", 1.5)], sum_stats = True)\n",
    "graphs3_valid_rf, graphs3_test_rf, graphs3_test_f = tests3\n",
    "\n",
    "g3_train_loader = DataLoader(graphs3_train_rf, batch_size=config_g3['batch_size'], shuffle=True)\n",
    "g3_valid_loader = DataLoader(graphs3_valid_rf, batch_size=config_g3['batch_size'], shuffle=False)\n",
    "g3_test_f_loader = DataLoader(graphs3_test_f, batch_size=config_g3['batch_size'], shuffle=False)\n",
    "g3_test_rf_loader = DataLoader(graphs3_test_rf, batch_size=config_g3['batch_size'], shuffle=False)\n",
    "\n",
    "train_loader = g3_train_loader\n",
    "valid_loader = g3_valid_loader\n",
    "test_f_loader = g3_test_f_loader\n",
    "test_rf_loader = g3_test_rf_loader\n",
    "test_loader = [test_f_loader, test_rf_loader]\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = graphs3_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs3_train_rf[0].num_edge_features\n",
    "num_nodes = graphs3_train_rf[0].num_nodes\n",
    "# max_epochs = max_epoch_list[graph_name]\n",
    "max_epochs = 100\n",
    "\n",
    "facts_about(graphs3_train_rf[0])"
   ],
   "id": "27cecb3a69ed133e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "transform 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:12<00:00, 215.22it/s]\n",
      "100%|██████████| 836/836 [00:03<00:00, 277.40it/s]\n",
      "100%|██████████| 732/732 [00:02<00:00, 275.84it/s]\n",
      "100%|██████████| 730/730 [00:02<00:00, 271.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 120 with feature dimension of x: 65\n",
      "Number of isolated nodes: 4\n",
      "Number of edges: 1054 with edge dimension: 5\n",
      "Average node degree: 8.783333778381348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T02:16:05.437252Z",
     "start_time": "2025-05-05T01:45:45.304523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"gnn_run8\"\n",
    "\n",
    "for i in range(0, 10):\n",
    "    TRAINNAME = f\"{graph_name}_{leadtime}_train_run{i}\"\n",
    "\n",
    "    with wandb.init(\n",
    "            project=PROJECTNAME, id=TRAINNAME, config=args_dict, tags=[\"final\"], resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=num_nodes,  #\n",
    "            embedding_dim=emb_dim,\n",
    "            edge_dim=edge_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config['gnn_hidden'],\n",
    "            out_channels_gnn=config['gnn_hidden'],\n",
    "            num_layers_gnn=config['gnn_layers'],\n",
    "            heads=config['heads'],\n",
    "            hidden_channels_deepset=config['gnn_hidden'],\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config['lr']),\n",
    "        )\n",
    "        torch.compile(multigraph)\n",
    "        batch = next(iter(train_loader))\n",
    "        multigraph.forward(batch)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=0)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=1,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[1],\n",
    "            enable_progress_bar=True,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[early_stop, progress_bar, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=multigraph, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "id": "5118ea254885505b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_034545-g3_24h_train_run0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run0' target=\"_blank\">g3_24h_train_run0</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇██▄▆▄▃▅▆▃▃▂▄▄▄▅▅▂▄▅▄▂▂▃▃▃▁▅▄▂▆▃▄▂▄▂▃▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇█</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▃▂▂▃▂▁▃▁▃▁▁▁▂▂▃▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_loss_epoch</td><td>0.58041</td></tr><tr><td>train_loss_step</td><td>0.49402</td></tr><tr><td>trainer/global_step</td><td>12752</td></tr><tr><td>val_loss</td><td>0.66308</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run0</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run0</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_034545-g3_24h_train_run0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_034858-g3_24h_train_run1</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run1' target=\"_blank\">g3_24h_train_run1</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▇█▅▇▅▆▅▅▆▆▃▃▃▄▂▃▂▃▄▃▂▄▄▂▄▂▂▂▃▃▂▃▂▂▁▃▅▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▂▂▂▁▂▂▁▁▁▁▁▁▁▂▁▁▂▁▁▂▁▂▃▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>44</td></tr><tr><td>train_loss_epoch</td><td>0.56889</td></tr><tr><td>train_loss_step</td><td>0.53033</td></tr><tr><td>trainer/global_step</td><td>14714</td></tr><tr><td>val_loss</td><td>0.6685</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run1</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run1</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_034858-g3_24h_train_run1/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_035240-g3_24h_train_run2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run2' target=\"_blank\">g3_24h_train_run2</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▆▅▄▅▅▅█▃▆▅▄▃▄▃▃▃▄▅▃▂▃▃▃▃▂▄▃▁▃▂▂▇▂▃▄▂▃▃▂▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▄▂▂▂▂▂▂▁▂▂▂▂▃▃▄▁▂▂▁▁▁▂▁▂▁▂▁▁▁▁▁▁▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_loss_epoch</td><td>0.58041</td></tr><tr><td>train_loss_step</td><td>0.62528</td></tr><tr><td>trainer/global_step</td><td>12425</td></tr><tr><td>val_loss</td><td>0.7054</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run2</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run2</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_035240-g3_24h_train_run2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_035548-g3_24h_train_run3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run3' target=\"_blank\">g3_24h_train_run3</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▂▂▁▁▁▄▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.60244</td></tr><tr><td>train_loss_step</td><td>0.62495</td></tr><tr><td>trainer/global_step</td><td>8828</td></tr><tr><td>val_loss</td><td>0.66424</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run3</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run3</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_035548-g3_24h_train_run3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_035800-g3_24h_train_run4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run4' target=\"_blank\">g3_24h_train_run4</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▄▆▇▅▄▆▃▅▃▄▅▆▃▂▄▂▂▂▂▃▂▁▂▃▃▂▂▄▂▃▃▂▁▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▄▄▂▃▃▂▂▂▃▁▁▁▂▂▂▂▂▁▂▂▁▁▁▂▁▃▁▂▂▁▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_loss_epoch</td><td>0.57957</td></tr><tr><td>train_loss_step</td><td>0.52747</td></tr><tr><td>trainer/global_step</td><td>12752</td></tr><tr><td>val_loss</td><td>0.6613</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run4</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run4</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_035800-g3_24h_train_run4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_040111-g3_24h_train_run5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run5' target=\"_blank\">g3_24h_train_run5</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▆█▄▄▅▄▄▄▆▃▃▅▄▄▂▄▃▂▂▃▃▂▃▃▂▂▄▃▃▂▃▂▁▂▃▃▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▃▂▂▂▃▂▃▂▃▂▂▁▂▂▂▂▁▃▁▂▃▂▁▂▁▁▁▂▂▂▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>42</td></tr><tr><td>train_loss_epoch</td><td>0.57455</td></tr><tr><td>train_loss_step</td><td>0.56505</td></tr><tr><td>trainer/global_step</td><td>14060</td></tr><tr><td>val_loss</td><td>0.65699</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run5</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run5</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_040111-g3_24h_train_run5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_040433-g3_24h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run6' target=\"_blank\">g3_24h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▄▂▂▂▂▁▁▃▂▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>train_loss_epoch</td><td>0.59028</td></tr><tr><td>train_loss_step</td><td>0.61536</td></tr><tr><td>trainer/global_step</td><td>10790</td></tr><tr><td>val_loss</td><td>0.66128</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_040433-g3_24h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_040705-g3_24h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run7' target=\"_blank\">g3_24h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▇▆▆▄▄▃▄▅▄▄▃▅▂▇▂▅▄▂▅▄▂▄▂▂▂▄▄▂▄▂▂▁▄▃▂▂▃▄</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>██▅▃▃▃▂▂▃▂▁▁▁▂▁▁▂▂▄▂▁▁▂▃▂▁▂▁▂▁▁▁▁▂▂▁▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_loss_epoch</td><td>0.57462</td></tr><tr><td>train_loss_step</td><td>0.60774</td></tr><tr><td>trainer/global_step</td><td>13733</td></tr><tr><td>val_loss</td><td>0.69146</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_040705-g3_24h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_041019-g3_24h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run8' target=\"_blank\">g3_24h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▄▄▄▄▅▃▃▂▃▃▃▂▄▂▂▄▄▃▁▃▃▃▂▂▁▂▂▂▂▂▂▄▂▃▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▄▃▄▂▂▂▂▃▂▂▂▂▂▁▂▃▂▁▁▁▁▁▁▁▁▁▁▂▂▁▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_loss_epoch</td><td>0.59029</td></tr><tr><td>train_loss_step</td><td>0.51909</td></tr><tr><td>trainer/global_step</td><td>11117</td></tr><tr><td>val_loss</td><td>0.76348</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_041019-g3_24h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_041255-g3_24h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run9' target=\"_blank\">g3_24h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 123 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 12.6 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.554     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>███▅▄▃▃▃▇▄▄▆▃▃▆▄▂▄▇▅▄▅▁▅▃▁▂▃▃▆▆▂▂▂▃▅▃▃▄▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▄▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁▂▂▁▂▂▁▁▁▁▂▁▂▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_loss_epoch</td><td>0.58065</td></tr><tr><td>train_loss_step</td><td>0.6358</td></tr><tr><td>trainer/global_step</td><td>13406</td></tr><tr><td>val_loss</td><td>0.65679</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_24h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g3_24h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_041255-g3_24h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T02:23:58.333454Z",
     "start_time": "2025-05-05T02:23:46.704191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [\"f\", \"rf\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            # Load Model from checkpoint\n",
    "\n",
    "            multigraph = Multigraph.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                num_nodes=num_nodes,\n",
    "                embedding_dim=emb_dim,\n",
    "                edge_dim=edge_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels_gnn=config_g3['gnn_hidden'],\n",
    "                out_channels_gnn=config_g3['gnn_hidden'],\n",
    "                num_layers_gnn=config_g3['gnn_layers'],\n",
    "                heads=config_g3['heads'],\n",
    "                hidden_channels_deepset=config_g3['gnn_hidden'],\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=config_g3['lr']),\n",
    "            )\n",
    "            multigraph.eval()\n",
    "            batch = next(iter(train_loader))\n",
    "            batch = batch.to(\"cuda\")\n",
    "            multigraph.to(\"cuda\")\n",
    "            multigraph.forward(batch)\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "            ####################################################################################################\n",
    "            preds = trainer.predict(model=multigraph, dataloaders=[tl])\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            preds_list.append(preds)\n",
    "            print()\n",
    "            print(preds.shape)\n",
    "\n",
    "    targets = dataframes[f\"test_{data}\"][1]\n",
    "    targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "    res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_{graph_name}_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")"
   ],
   "id": "7a28b553a6b43b56",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g3_24h_train_run5.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 202.35it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 149.86it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 169.47it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 236.60it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 202.15it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 173.69it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 241.58it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 203.09it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 215.91it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g3_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 195.53it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6095958010435606\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g3_24h_train_run5.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 226.96it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 182.14it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 217.41it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 248.94it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 204.32it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 183.44it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 199.54it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 189.43it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 160.28it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g3_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 184.70it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6172392730752438\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g3_24h\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph 4",
   "id": "3812a3b8b1b8abff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:21:04.999556Z",
     "start_time": "2025-05-06T01:21:04.990110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_name = \"g4\"\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}\")\n",
    "\n",
    "PARAMS = os.path.join(RESULTPATH, \"params.json\")\n",
    "with open(PARAMS, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {PARAMS}\")\n",
    "    args_dict = json.load(f)\n",
    "config_g4 = args_dict"
   ],
   "id": "4f1ba8ba211c47b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/params.json\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:21:27.927645Z",
     "start_time": "2025-05-06T01:21:06.723683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_name = \"g4\"\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}\")\n",
    "\n",
    "graphs4_train_rf, tests4 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['valid'], dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"dist2\", \"dist3\"],edges=[(\"dist2\", 0.003), (\"dist3\", 0.0074)], sum_stats = True)\n",
    "graphs4_valid_rf, graphs4_test_rf, graphs4_test_f = tests4\n",
    "\n",
    "g4_train_loader = DataLoader(graphs4_train_rf, batch_size=config_g4['batch_size'], shuffle=True)\n",
    "g4_valid_loader = DataLoader(graphs4_valid_rf, batch_size=config_g4['batch_size'], shuffle=False)\n",
    "g4_test_f_loader = DataLoader(graphs4_test_f, batch_size=config_g4['batch_size'], shuffle=False)\n",
    "g4_test_rf_loader = DataLoader(graphs4_test_rf, batch_size=config_g4['batch_size'], shuffle=False)\n",
    "\n",
    "train_loader = g4_train_loader\n",
    "valid_loader = g4_valid_loader\n",
    "test_f_loader = g4_test_f_loader\n",
    "test_rf_loader = g4_test_rf_loader\n",
    "test_loader = [test_f_loader, test_rf_loader]\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = graphs4_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs4_train_rf[0].num_edge_features\n",
    "num_nodes = graphs4_train_rf[0].num_nodes\n",
    "# max_epochs = max_epoch_list[graph_name]\n",
    "max_epochs = 100\n",
    "\n",
    "facts_about(graphs4_train_rf[0])\n"
   ],
   "id": "2d76f9f2b85f8ef1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "transform 3\n",
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:11<00:00, 220.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 836/836 [00:03<00:00, 252.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 732/732 [00:02<00:00, 270.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:02<00:00, 279.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 120 with feature dimension of x: 65\n",
      "Number of isolated nodes: 13\n",
      "Number of edges: 764 with edge dimension: 2\n",
      "Average node degree: 6.366666793823242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T04:19:09.410580Z",
     "start_time": "2025-05-05T03:43:57.983570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"gnn_run8\"\n",
    "\n",
    "for i in range(0, 10):\n",
    "    TRAINNAME = f\"{graph_name}_{leadtime}_train_run{i}\"\n",
    "\n",
    "    with wandb.init(\n",
    "            project=PROJECTNAME, id=TRAINNAME, config=args_dict, tags=[\"final\"], resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=num_nodes,  #\n",
    "            embedding_dim=emb_dim,\n",
    "            edge_dim=edge_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config['gnn_hidden'],\n",
    "            out_channels_gnn=config['gnn_hidden'],\n",
    "            num_layers_gnn=config['gnn_layers'],\n",
    "            heads=config['heads'],\n",
    "            hidden_channels_deepset=config['gnn_hidden'],\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config['lr']),\n",
    "        )\n",
    "        torch.compile(multigraph)\n",
    "        batch = next(iter(train_loader))\n",
    "        multigraph.forward(batch)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=0)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=1,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[1],\n",
    "            enable_progress_bar=True,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[early_stop, progress_bar, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=multigraph, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "id": "75be23a8a4fcc4a2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_054358-g4_24h_train_run0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run0' target=\"_blank\">g4_24h_train_run0</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▂▂▁▁▂▁▂▂▁▁▁▁▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▃▂▃▂▂▁▁▂▂▂▁▂▁▂▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.60099</td></tr><tr><td>train_loss_step</td><td>0.79984</td></tr><tr><td>trainer/global_step</td><td>8828</td></tr><tr><td>val_loss</td><td>0.67027</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run0</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run0</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_054358-g4_24h_train_run0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_054612-g4_24h_train_run1</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run1' target=\"_blank\">g4_24h_train_run1</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▇▅▆▅▆▆▄▃▇▄▃▃▂▅▅▆▄▃▃▃▃▂▃▄▄▅▃▄▄▂▃▅▃▂▂▄▄▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▃▅▂▂▅▂▂▁▂▁▁▂▁▂▂▁▂▂▂▁▁▁▂▁▁▁▁▁▂▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>45</td></tr><tr><td>train_loss_epoch</td><td>0.55924</td></tr><tr><td>train_loss_step</td><td>0.45763</td></tr><tr><td>trainer/global_step</td><td>15041</td></tr><tr><td>val_loss</td><td>0.68508</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run1</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run1</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_054612-g4_24h_train_run1/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_054955-g4_24h_train_run2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run2' target=\"_blank\">g4_24h_train_run2</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▇▆▆█▃▅▆▅▆▄▆▇▇▄▄▇▆█▆▅▇▃▄▆▄▅▆▅▄▁▃▃▂▄▂▁▃▅▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▆▆▆▇▇▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▄▄▄▃▃▂▂▂▂▂▁▂▁▁▂▁▂▂▂▁▂▁▃▁▁▁▁▁▂▂▁▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>42</td></tr><tr><td>train_loss_epoch</td><td>0.56601</td></tr><tr><td>train_loss_step</td><td>0.56625</td></tr><tr><td>trainer/global_step</td><td>14060</td></tr><tr><td>val_loss</td><td>0.67638</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run2</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run2</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_054955-g4_24h_train_run2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_055326-g4_24h_train_run3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run3' target=\"_blank\">g4_24h_train_run3</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▃▄▃▃▄▄▃▂▂▃▂▂▃▂▃▃▂▂▃▃▂▂▂▂▂▂▁▂▁▁▃▁▃▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss_epoch</td><td>0.57053</td></tr><tr><td>train_loss_step</td><td>0.56717</td></tr><tr><td>trainer/global_step</td><td>13079</td></tr><tr><td>val_loss</td><td>0.67754</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run3</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run3</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_055326-g4_24h_train_run3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_055642-g4_24h_train_run4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run4' target=\"_blank\">g4_24h_train_run4</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▁▁▁▁▂▁▁▂▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▃▂▂▂▂▂▁▁▂▂▁▂▂▂▂▂▁▁▂▁▂▂▁▃▁▁▁▂▁▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>train_loss_epoch</td><td>0.55848</td></tr><tr><td>train_loss_step</td><td>0.52301</td></tr><tr><td>trainer/global_step</td><td>15695</td></tr><tr><td>val_loss</td><td>0.68118</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run4</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run4</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_055642-g4_24h_train_run4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_060037-g4_24h_train_run5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run5' target=\"_blank\">g4_24h_train_run5</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▂▂▁▂▂▁▂▁▂▁▁▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▂▂▂▂▃▂▁▂▃▂▂▁▁▁▁▁▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_loss_epoch</td><td>0.57145</td></tr><tr><td>train_loss_step</td><td>0.53239</td></tr><tr><td>trainer/global_step</td><td>13733</td></tr><tr><td>val_loss</td><td>0.6708</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run5</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run5</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_060037-g4_24h_train_run5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_060404-g4_24h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run6' target=\"_blank\">g4_24h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▅▅▆█▄▂▅▆▃▃▄▃▄▄▂▃▅▅▅▃▃▃▃▃▄▃▂▁▂▄▄▃▁▃▃▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▃▂▂▂▂▂▂▁▁▂▁▂▂▁▁▂▂▁▂▁▂▁▁▁▁▁▁▂▁▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>train_loss_epoch</td><td>0.55444</td></tr><tr><td>train_loss_step</td><td>0.63022</td></tr><tr><td>trainer/global_step</td><td>16349</td></tr><tr><td>val_loss</td><td>0.6735</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_060404-g4_24h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_060810-g4_24h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run7' target=\"_blank\">g4_24h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▇▅▃▃▄▃▃▃▃▃▃▄▃▂▃▃▂▁▃▂▂▃▃▂▂▄▃▆▁▂▇▂▂▁▂▃▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▂▂▂▂▂▁▃▁▁▂▂▁▁▁▁▁▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>47</td></tr><tr><td>train_loss_epoch</td><td>0.55876</td></tr><tr><td>train_loss_step</td><td>0.61911</td></tr><tr><td>trainer/global_step</td><td>15695</td></tr><tr><td>val_loss</td><td>0.67838</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_060810-g4_24h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_061205-g4_24h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run8' target=\"_blank\">g4_24h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▃▃▃▃▃▃▃▂▂▂▄▃▂▄▂▂▁▂▂▂▂▂▂▂▂▃▂▂▂▂▃▁▃▂▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇████</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▂▂▁▁▁▂▁▁▂▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss_epoch</td><td>0.5724</td></tr><tr><td>train_loss_step</td><td>0.56513</td></tr><tr><td>trainer/global_step</td><td>13079</td></tr><tr><td>val_loss</td><td>0.67773</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_061205-g4_24h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250505_061520-g4_24h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run9' target=\"_blank\">g4_24h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 309 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 49.8 K | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "361 K     Trainable params\n",
      "0         Non-trainable params\n",
      "361 K     Total params\n",
      "1.446     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▄▄▂▂▂▃▂▁▂▂▂▂▃▂▁▂▂▂▂▁▂▂▁▂▂▁▂▁▃▁▁▁▂▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▅▄▅▃▃▃▂▂▂▂▂▁▂▂▂▂▁▁▁▁▂▁▁▂▁▃▁▁▁▁▁▂▁▂▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>46</td></tr><tr><td>train_loss_epoch</td><td>0.56273</td></tr><tr><td>train_loss_step</td><td>0.52407</td></tr><tr><td>trainer/global_step</td><td>15368</td></tr><tr><td>val_loss</td><td>0.70427</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_24h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g4_24h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250505_061520-g4_24h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:22:42.121874Z",
     "start_time": "2025-05-06T01:22:42.117088Z"
    }
   },
   "cell_type": "code",
   "source": "SAVEPATH",
   "id": "38578b3b4a9f4530",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h/models'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T01:26:29.883506Z",
     "start_time": "2025-05-06T01:26:16.851054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [\"f\", \"rf\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            # Load Model from checkpoint\n",
    "\n",
    "            multigraph = Multigraph.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                num_nodes=num_nodes,\n",
    "                embedding_dim=emb_dim,\n",
    "                edge_dim=edge_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels_gnn=config_g4['gnn_hidden'],\n",
    "                out_channels_gnn=config_g4['gnn_hidden'],\n",
    "                num_layers_gnn=config_g4['gnn_layers'],\n",
    "                heads=config_g4['heads'],\n",
    "                hidden_channels_deepset=config_g4['gnn_hidden'],\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=config_g4['lr']),\n",
    "            )\n",
    "            multigraph.eval()\n",
    "            batch = next(iter(train_loader))\n",
    "            batch = batch.to(\"cuda\")\n",
    "            multigraph.to(\"cuda\")\n",
    "            multigraph.forward(batch)\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "            ####################################################################################################\n",
    "            preds = trainer.predict(model=multigraph, dataloaders=[tl])\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            preds_list.append(preds)\n",
    "            print()\n",
    "            print(preds.shape)\n",
    "\n",
    "    targets = dataframes[f\"test_{data}\"][1]\n",
    "    targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "    res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_{graph_name}_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")"
   ],
   "id": "676d43ce6d937037",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g4_24h_train_run7.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 183.76it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 169.07it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 185.53it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 198.52it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 163.32it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 167.73it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 184.72it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 152.29it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 176.54it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g4_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 161.01it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.610728339804973\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g4_24h_train_run7.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 158.52it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 178.71it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 168.88it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 183.87it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 149.82it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 162.22it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 182.34it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 174.19it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 162.70it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g4_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 179.16it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6233347052191083\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g4_24h\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Graph 5",
   "id": "ea2ff98f48a57828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:56:11.459636Z",
     "start_time": "2025-05-06T04:56:11.450616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_name = \"g5\"\n",
    "SAVEPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}/models\")\n",
    "RESULTPATH = os.path.join(DIRECTORY, f\"leas_trained_models/sum_stats_{leadtime}/{graph_name}_{leadtime}\")\n",
    "\n",
    "PARAMS = os.path.join(RESULTPATH, \"params.json\")\n",
    "with open(PARAMS, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {PARAMS}\")\n",
    "    args_dict = json.load(f)\n",
    "config_g5 = args_dict"
   ],
   "id": "e277ba2bd4a45526",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/params.json\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T04:57:50.062056Z",
     "start_time": "2025-05-06T04:57:14.061336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graphs5_train_rf, tests5 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['valid'], dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\", \"alt\", \"lon\", \"lat\", \"alt-orog\", \"dist2\", \"dist3\"], edges=[(\"geo\", 50),(\"alt\", 4), (\"alt-orog\", 1.5), (\"dist2\", 0.003), (\"dist3\", 0.0074)], sum_stats = True)\n",
    "graphs5_valid_rf, graphs5_test_rf, graphs5_test_f = tests5\n",
    "\n",
    "g5_train_loader = DataLoader(graphs5_train_rf, batch_size=config_g5['batch_size'], shuffle=True)\n",
    "g5_valid_loader = DataLoader(graphs5_valid_rf, batch_size=config_g5['batch_size'], shuffle=False)\n",
    "g5_test_f_loader = DataLoader(graphs5_test_f, batch_size=config_g5['batch_size'], shuffle=False)\n",
    "g5_test_rf_loader = DataLoader(graphs5_test_rf, batch_size=config_g5['batch_size'], shuffle=False)\n",
    "\n",
    "train_loader = g5_train_loader\n",
    "valid_loader = g5_valid_loader\n",
    "test_f_loader = g5_test_f_loader\n",
    "test_rf_loader = g5_test_rf_loader\n",
    "test_loader = [test_f_loader, test_rf_loader]\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = graphs5_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs5_train_rf[0].num_edge_features\n",
    "num_nodes = graphs5_train_rf[0].num_nodes\n",
    "max_epochs = 100\n"
   ],
   "id": "ae59ba2a46388475",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "transform 3\n",
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:11<00:00, 221.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 836/836 [00:03<00:00, 271.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 732/732 [00:02<00:00, 255.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:02<00:00, 267.08it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T05:33:31.347166Z",
     "start_time": "2025-05-06T04:57:50.069206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"gnn_run8\"\n",
    "\n",
    "for i in range(0, 10):\n",
    "    TRAINNAME = f\"{graph_name}_{leadtime}_train_run{i}\"\n",
    "\n",
    "    with wandb.init(\n",
    "            project=PROJECTNAME, id=TRAINNAME, config=args_dict, tags=[\"final\"], resume=\"never\"\n",
    "    ):\n",
    "        config = wandb.config\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=num_nodes,  #\n",
    "            embedding_dim=emb_dim,\n",
    "            edge_dim=edge_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config['gnn_hidden'],\n",
    "            out_channels_gnn=config['gnn_hidden'],\n",
    "            num_layers_gnn=config['gnn_layers'],\n",
    "            heads=config['heads'],\n",
    "            hidden_channels_deepset=config['gnn_hidden'],\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config['lr']),\n",
    "        )\n",
    "        # torch.compile(multigraph)\n",
    "        batch = next(iter(train_loader))\n",
    "        multigraph.forward(batch)\n",
    "\n",
    "        wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "        early_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "        progress_bar = TQDMProgressBar(refresh_rate=0)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            dirpath=SAVEPATH, filename=TRAINNAME, monitor=\"val_loss\", mode=\"min\", save_top_k=1\n",
    "        )\n",
    "\n",
    "        trainer = L.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            log_every_n_steps=1,\n",
    "            accelerator=\"gpu\",\n",
    "            devices=[1],\n",
    "            enable_progress_bar=True,\n",
    "            logger=wandb_logger,\n",
    "            callbacks=[early_stop, progress_bar, checkpoint_callback],\n",
    "        )\n",
    "\n",
    "        trainer.fit(model=multigraph, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ],
   "id": "589031dadc004a3e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleachen01\u001B[0m (\u001B[33mleachen01-karlsruhe-institute-of-technology\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_065750-g5_24h_train_run0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run0' target=\"_blank\">g5_24h_train_run0</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▃▄▅▄▄▃▃▆▃▄▅▄▅▂▃▃▂▄▃▂▂▁▄▄▁▄▄▁▁▃▃▃▂▁▂▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>val_loss</td><td>█▅▄▄▃▂▃▂▄▁▁▂▂▁▃▁▁▂▁▁▁▂▂▁▁▂▁▁▂▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_loss_epoch</td><td>0.5518</td></tr><tr><td>train_loss_step</td><td>0.55819</td></tr><tr><td>trainer/global_step</td><td>11444</td></tr><tr><td>val_loss</td><td>0.66167</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run0</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run0' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run0</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_065750-g5_24h_train_run0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_070132-g5_24h_train_run1</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run1' target=\"_blank\">g5_24h_train_run1</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run1</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▃▃▄▃▄▅▂▂▃▃▃▃▃▂▂▂▂▄▃▃▃▂▃▂▂▂▂▂▃▃▂▃▁▃▃▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇███████</td></tr><tr><td>val_loss</td><td>█▅▄▃▃▂▂▂▂▅▂▂▂▂▁▁▁▁▁▁▂▁▁▂▁▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_loss_epoch</td><td>0.56258</td></tr><tr><td>train_loss_step</td><td>0.49346</td></tr><tr><td>trainer/global_step</td><td>9155</td></tr><tr><td>val_loss</td><td>0.69096</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run1</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run1' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run1</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_070132-g5_24h_train_run1/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_070429-g5_24h_train_run2</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run2' target=\"_blank\">g5_24h_train_run2</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run2</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▃▄▃█▅▄▄▂▃▃▂▄▄▃▄▃▃▃▄▄▄▃▄▃▁▃▂▄▃▃▁▁▂▂▃▃▂▁▃▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▆▅▄▃▃▃▂▂▁▃▃▁▂▁▂▁▂▂▁▂▁▂▃▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_loss_epoch</td><td>0.57418</td></tr><tr><td>train_loss_step</td><td>0.56671</td></tr><tr><td>trainer/global_step</td><td>8828</td></tr><tr><td>val_loss</td><td>0.66624</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run2</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run2' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run2</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_070429-g5_24h_train_run2/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_070719-g5_24h_train_run3</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run3' target=\"_blank\">g5_24h_train_run3</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▃▁▂▂▂▂▂▁▂▂▁▁▂▁▂▁▁▂▂▁▁▁▂▁▂▂▂▁▁▁▁▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▆▄▃▃▂▄▂▂▂▂▁▂▁▂▂▁▂▂▁▂▁▁▂▃▁▁▁▂▁▂▁▁▂▁▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_loss_epoch</td><td>0.55127</td></tr><tr><td>train_loss_step</td><td>0.56262</td></tr><tr><td>trainer/global_step</td><td>12098</td></tr><tr><td>val_loss</td><td>0.73384</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run3</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run3' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run3</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_070719-g5_24h_train_run3/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_071112-g5_24h_train_run4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run4' target=\"_blank\">g5_24h_train_run4</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▇▇▇▇▇▇███</td></tr><tr><td>val_loss</td><td>█▆▅▃▃▂▃▃▃▂▂▂▂▂▁▂▁▁▂▂▃▁▁▁▁▃▂▁▂▄▂▁▂▂▄▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>44</td></tr><tr><td>train_loss_epoch</td><td>0.51849</td></tr><tr><td>train_loss_step</td><td>0.48004</td></tr><tr><td>trainer/global_step</td><td>14714</td></tr><tr><td>val_loss</td><td>0.67844</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run4</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run4' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run4</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_071112-g5_24h_train_run4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_071554-g5_24h_train_run5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run5' target=\"_blank\">g5_24h_train_run5</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▆▄█▅▃▅▇▃▅▄▄▃▆▄▆▃▄▃▆▅▄▃▁▃▃▁▃▂▃▄▂▃▂▂▂▂▂▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▆▄▄▂▃▃▂▃▁▁▃▁▃▁▂▁▂▂▂▁▃▂▁▂▁▁▁▁▁▂▁▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_loss_epoch</td><td>0.53804</td></tr><tr><td>train_loss_step</td><td>0.53304</td></tr><tr><td>trainer/global_step</td><td>12098</td></tr><tr><td>val_loss</td><td>0.67865</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run5</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run5</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_071554-g5_24h_train_run5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_071944-g5_24h_train_run6</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run6' target=\"_blank\">g5_24h_train_run6</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run6</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▃▂▂▃▃▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▃▂▁▃▁▂▂▁▁▂▁▂▁▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>val_loss</td><td>█▄▃▃▂▂▄▂▂▂▁▁▂▁▁▂▂▁▂▁▂▂▁▁▂▁▁▁▂▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>train_loss_epoch</td><td>0.55429</td></tr><tr><td>train_loss_step</td><td>0.5722</td></tr><tr><td>trainer/global_step</td><td>10790</td></tr><tr><td>val_loss</td><td>0.67192</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run6</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run6' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run6</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_071944-g5_24h_train_run6/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_072310-g5_24h_train_run7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run7' target=\"_blank\">g5_24h_train_run7</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇████</td></tr><tr><td>train_loss_epoch</td><td>█▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▇▄▃▃▃▃▂▂▃▂▂▄▂▂▁▄▁▁▁▂▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_loss_epoch</td><td>0.58486</td></tr><tr><td>train_loss_step</td><td>0.66497</td></tr><tr><td>trainer/global_step</td><td>8501</td></tr><tr><td>val_loss</td><td>0.66489</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run7</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run7' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run7</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_072310-g5_24h_train_run7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_072554-g5_24h_train_run8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run8' target=\"_blank\">g5_24h_train_run8</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆█▇▄▅▇▅▇▅▅▅▄▃▄▄▂▄▄▆▂▄▂▅▄▁▄▃▄▃▃▃▃▃▂▃▃▄▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▅▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▃▂▁▆▁▁▂▁▂▁▂▂▁▃▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_loss_epoch</td><td>0.53587</td></tr><tr><td>train_loss_step</td><td>0.56088</td></tr><tr><td>trainer/global_step</td><td>12425</td></tr><tr><td>val_loss</td><td>0.65445</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run8</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run8</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_072554-g5_24h_train_run8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250506_072955-g5_24h_train_run9</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run9' target=\"_blank\">g5_24h_train_run9</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run9</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 941 K  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.624     Total estimated model params size (MB)\n",
      "19        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▄▆▅▅▆▃▄▃▄▄▃▃▁▂▂▄▄▃▂▁▃▄▂▁▃▄▃▄▃▂▂▃▁▁▂▃▃▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▄▃▄▂▂▃▂▃▂▂▃▁▂▅▁▃▁▁▁▁▂▁▂▂▁▂▁▂▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_loss_epoch</td><td>0.55087</td></tr><tr><td>train_loss_step</td><td>0.48892</td></tr><tr><td>trainer/global_step</td><td>11117</td></tr><tr><td>val_loss</td><td>0.67377</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_24h_train_run9</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run9' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8/runs/g5_24h_train_run9</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/gnn_run8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250506_072955-g5_24h_train_run9/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T05:33:50.580603Z",
     "start_time": "2025-05-06T05:33:31.601062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_list = [\"f\", \"rf\"]\n",
    "for data, tl in zip(data_list, test_loader):\n",
    "    preds_list = []\n",
    "    for path in os.listdir(SAVEPATH):\n",
    "        if path.endswith(\".ckpt\"):\n",
    "            print(f\"[INFO] Loading model from {path}\")\n",
    "            # Load Model from checkpoint\n",
    "\n",
    "            multigraph = Multigraph.load_from_checkpoint(\n",
    "                os.path.join(SAVEPATH, path),\n",
    "                num_nodes=num_nodes,\n",
    "                embedding_dim=emb_dim,\n",
    "                edge_dim=edge_dim,\n",
    "                in_channels=in_channels,\n",
    "                hidden_channels_gnn=config_g5['gnn_hidden'],\n",
    "                out_channels_gnn=config_g5['gnn_hidden'],\n",
    "                num_layers_gnn=config_g5['gnn_layers'],\n",
    "                heads=config_g5['heads'],\n",
    "                hidden_channels_deepset=config_g5['gnn_hidden'],\n",
    "                optimizer_class=AdamW,\n",
    "                optimizer_params=dict(lr=config_g5['lr']),\n",
    "            )\n",
    "            multigraph.eval()\n",
    "            batch = next(iter(train_loader))\n",
    "            batch = batch.to(\"cuda\")\n",
    "            multigraph.to(\"cuda\")\n",
    "            multigraph.forward(batch)\n",
    "\n",
    "            trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "            ####################################################################################################\n",
    "            preds = trainer.predict(model=multigraph, dataloaders=[tl])\n",
    "            preds = torch.cat(preds, dim=0)\n",
    "            preds_list.append(preds)\n",
    "            print()\n",
    "            print(preds.shape)\n",
    "\n",
    "    targets = dataframes[f\"test_{data}\"][1]\n",
    "    targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "    stacked = torch.stack(preds_list)\n",
    "    final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "    res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"final crps: {res.item()}\")\n",
    "    print(\"#############################################\")\n",
    "    print(\"#############################################\")\n",
    "\n",
    "    ####################################################################################################\n",
    "    os.makedirs(RESULTPATH, exist_ok=True)\n",
    "    print(RESULTPATH)\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "    df.to_csv(os.path.join(RESULTPATH, f\"{data}_{graph_name}_{leadtime}_results.csv\"), index=False)\n",
    "\n",
    "    # Create Log File ###############################################################\n",
    "    log_file = os.path.join(RESULTPATH, f\"{data}.txt\")\n",
    "    with open(log_file, \"w\") as f:\n",
    "        f.write(f\"Data: {data}\\n\")\n",
    "        f.write(f\"Leadtime: {leadtime}\\n\")\n",
    "        f.write(f\"Final crps: {res.item()}\")"
   ],
   "id": "4c16422d086e3c82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g5_24h_train_run5.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 109.43it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 113.67it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 118.43it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 117.82it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 113.40it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 110.69it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 120.62it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 119.70it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 120.00it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "[INFO] Loading model from g5_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 121.08it/s]\n",
      "\n",
      "torch.Size([87600, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6051888017527292\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model from g5_24h_train_run5.ckpt\n",
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 118.29it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run0.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 118.66it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run7.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 114.12it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run4.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 119.16it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 111.61it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run6.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 117.69it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 110.13it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run8.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 107.01it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 119.95it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "[INFO] Loading model from g5_24h_train_run3.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:00<00:00, 119.67it/s]\n",
      "\n",
      "torch.Size([87840, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6175431825760571\n",
      "#############################################\n",
      "#############################################\n",
      "/home/ltchen/gnnpp/leas_trained_models/sum_stats_24h/g5_24h\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
