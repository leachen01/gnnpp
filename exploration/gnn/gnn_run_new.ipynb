{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T03:28:34.648032Z",
     "start_time": "2025-04-16T03:28:34.633781Z"
    }
   },
   "source": [
    "%cd /home/ltchen/gnnpp\n",
    "import sys\n",
    "import os\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch_geometric\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "from typing import Tuple\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import scatter\n",
    "from torch.nn import Linear, ModuleList, ReLU\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from models.loss import NormalCRPS\n",
    "from models.model_utils import MakePositive, EmbedStations\n",
    "from utils.data import (\n",
    "    load_dataframes,\n",
    "    load_distances,\n",
    "    normalize_features_and_create_graphs,\n",
    "    rm_edges,\n",
    "    summary_statistics,\n",
    ")\n",
    "from exploration.graph_creation import *\n",
    "from models.graphensemble.multigraph import *"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:28:37.123511Z",
     "start_time": "2025-04-16T03:28:37.116523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "DIRECTORY = os.getcwd()\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"explored_models/gnn_new_attr_24h/models\")\n",
    "JSONPATH = os.path.join(DIRECTORY, \"trained_models/best_24h/params.json\")"
   ],
   "id": "75ec733d044d00ba",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:28:42.791602Z",
     "start_time": "2025-04-16T03:28:42.778645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "config = args_dict\n",
    "print(config)\n",
    "print(config['lr'])\n",
    "print(config['max_dist'])\n",
    "print(type(config))\n",
    "print(type(config['lr']))\n",
    "print(type(config['gnn_hidden']))\n",
    "print(config['gnn_hidden'])\n",
    "'''{\"batch_size\":8,\n",
    "\"gnn_hidden\":265,\n",
    "\"gnn_layers\":2,\n",
    "\"heads\":8,\n",
    "\"lr\":0.0002,\n",
    "\"max_dist\":100,\n",
    "\"max_epochs\": 31}'''"
   ],
   "id": "1a46aa1396e2032b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/best_24h/params.json\n",
      "{'batch_size': 8, 'gnn_hidden': 265, 'gnn_layers': 2, 'heads': 8, 'lr': 0.0002, 'max_dist': 100, 'max_epochs': 31}\n",
      "0.0002\n",
      "100\n",
      "<class 'dict'>\n",
      "<class 'float'>\n",
      "<class 'int'>\n",
      "265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"batch_size\":8,\\n\"gnn_hidden\":265,\\n\"gnn_layers\":2,\\n\"heads\":8,\\n\"lr\":0.0002,\\n\"max_dist\":100,\\n\"max_epochs\": 31}'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load Graphs",
   "id": "39f9b06d4d02a511"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:28:59.891214Z",
     "start_time": "2025-04-16T03:28:54.712097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframes = load_dataframes(mode=\"eval\", leadtime= \"24h\") # load newly created dataframes\n",
    "dataframes = summary_statistics(dataframes)"
   ],
   "id": "7c9b6f3060caf0db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:46:58.462816Z",
     "start_time": "2025-04-15T14:46:24.262612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train = dataframes[\"train\"][0]\n",
    "# train_target = dataframes[\"train\"][1]\n",
    "# test_rf = dataframes[\"test_rf\"][0]\n",
    "# test_rf_target = dataframes[\"test_rf\"][1]\n",
    "# test_f = dataframes[\"test_f\"][0]\n",
    "# test_f_target = dataframes[\"test_f\"][1]\n",
    "\n",
    "# self-created\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\"], edges=[(\"geo\", 100)], sum_stats = True)\n",
    "\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "graphs_test = graphs_test_rf"
   ],
   "id": "77f4db90bb612146",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3448/3448 [00:16<00:00, 204.66it/s]\n",
      "100%|██████████| 732/732 [00:02<00:00, 273.74it/s]\n",
      "100%|██████████| 730/730 [00:02<00:00, 257.81it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graph 1: reproduction",
   "id": "b14be158a3f74b21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:52:34.691351Z",
     "start_time": "2025-04-15T17:52:34.675942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "g1_train_loader = DataLoader(graphs_train_rf, batch_size=batch_size, shuffle=True)\n",
    "g1_test_f_loader = DataLoader(graphs_test_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim=20\n",
    "\n",
    "in_channels = graphs_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "\n",
    "edge_dim = graphs_train_rf[0].num_edge_features\n",
    "embedding_dim=emb_dim\n",
    "in_channels=in_channels\n",
    "hidden_channels_gnn=config['gnn_hidden']\n",
    "out_channels_gnn=config['gnn_hidden']\n",
    "num_layers_gnn=config['gnn_hidden']\n",
    "heads=config['heads']\n",
    "hidden_channels_deepset=config['gnn_hidden']\n",
    "optimizer_class=AdamW\n",
    "optimizer_params=dict(lr=config['lr'])"
   ],
   "id": "9d45bfcff7733bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T14:47:22.575209Z",
     "start_time": "2025-04-15T14:47:21.042814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wandb_run = None\n",
    "\n",
    "try:\n",
    "    wandb_run = wandb.init(project=\"my-project\", name=\"safe_run\")\n",
    "except Exception as e:\n",
    "    print(f\"W&B failed to start: {e}\")"
   ],
   "id": "9e296d0509c5aeb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleachen01\u001B[0m (\u001B[33mleachen01-karlsruhe-institute-of-technology\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_164721-rwnzzqt5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/my-project/runs/rwnzzqt5' target=\"_blank\">safe_run</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/my-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/my-project' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/my-project</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/my-project/runs/rwnzzqt5' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/my-project/runs/rwnzzqt5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:01:05.352539Z",
     "start_time": "2025-04-15T17:52:47.260562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"new_attr_graphs\"\n",
    "FILENAME = \"g1_train_run_24h\"\n",
    "train_loader = g1_train_loader\n",
    "with wandb.init(\n",
    "        project=PROJECTNAME, id=FILENAME, config=args_dict, tags=[\"reproduction\"], reinit=True\n",
    "):\n",
    "    config = wandb.config\n",
    "\n",
    "    multigraph = Multigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "    batch = next(iter(g1_train_loader))\n",
    "    # batch = batch  # .to(\"cuda\")\n",
    "    # multigraph  # .to(\"cuda\")\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=FILENAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    # print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "862cbff3c23d84aa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_195247-g1_train_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g1_train_run_24h' target=\"_blank\">g1_train_run_24h</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g1_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g1_train_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/gnn_new_attr_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.9 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 M    Total params\n",
      "40.571    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:15<00:00, 27.55it/s, v_num=_24h, train_loss_step=0.441, train_loss_epoch=0.504]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:15<00:00, 27.54it/s, v_num=_24h, train_loss_step=0.441, train_loss_epoch=0.504]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▆█▄▄▄▄▃▃▃▃▃▂▄▃▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▃▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.50399</td></tr><tr><td>train_loss_step</td><td>0.441</td></tr><tr><td>trainer/global_step</td><td>13360</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g1_train_run_24h</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g1_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g1_train_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_195247-g1_train_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T18:05:31.878438Z",
     "start_time": "2025-04-15T18:05:30.394174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# g1_test_rf_loader = DataLoader(graphs_test_rf, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "preds_list = []\n",
    "preds = trainer.predict(model=multigraph, dataloaders=[g1_test_f_loader]) # 92 x 976 x 2 forecasts with mu and sigma of 122 stations\n",
    "preds = torch.cat(preds, dim=0)\n",
    "preds_list.append(preds)\n",
    "\n",
    "# targets = dataframes[\"test_rf\"][1]\n",
    "targets = dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "bb4403ba555cc96e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:01<00:00, 65.17it/s]\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6454867613345044\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graph2: same edges, more attributes",
   "id": "593ef78340f1ab5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:29:40.000232Z",
     "start_time": "2025-04-16T03:29:05.598266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graphs2_train_rf, tests2 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\", \"alt\", \"lon\", \"lat\", \"alt-orog\"], edges=[(\"geo\", 100)], sum_stats = True)\n",
    "\n",
    "graphs2_test_rf, graphs2_test_f = tests2\n",
    "graphs2_test = graphs2_test_rf\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "g2_train_loader = DataLoader(graphs2_train_rf, batch_size=batch_size, shuffle=True)\n",
    "g2_test_f_loader = DataLoader(graphs2_test_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim = 20\n",
    "\n",
    "in_channels = graphs2_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "\n",
    "edge_dim = graphs2_train_rf[0].num_edge_features\n",
    "embedding_dim = emb_dim\n",
    "in_channels = in_channels\n",
    "hidden_channels_gnn = config['gnn_hidden']\n",
    "out_channels_gnn = config['gnn_hidden']\n",
    "num_layers_gnn = config['gnn_hidden']\n",
    "heads = config['heads']\n",
    "hidden_channels_deepset = config['gnn_hidden']\n",
    "optimizer_class = AdamW\n",
    "optimizer_params = dict(lr=config['lr'])"
   ],
   "id": "29d3c199e72137d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3448/3448 [00:16<00:00, 204.21it/s]\n",
      "100%|██████████| 732/732 [00:02<00:00, 282.93it/s]\n",
      "100%|██████████| 730/730 [00:02<00:00, 276.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:33:25.723403Z",
     "start_time": "2025-04-16T03:33:25.718739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"new_attr_graphs\"\n",
    "FILENAME = \"g2_train_run_24h\""
   ],
   "id": "41616ac119e63107",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:05:14.863578Z",
     "start_time": "2025-04-15T14:56:57.999772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_loader = g2_train_loader\n",
    "with wandb.init(\n",
    "        project=PROJECTNAME, id=FILENAME, config=args_dict, tags=[\"reproduction\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "\n",
    "    multigraph = Multigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "    batch = next(iter(train_loader))\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=FILENAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    # print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "a89112da4781b0fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_165658-g2_train_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g2_train_run_24h' target=\"_blank\">g2_train_run_24h</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g2_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g2_train_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/gnn_new_attr_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.9 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.2 M    Total params\n",
      "40.639    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:15<00:00, 27.55it/s, v_num=_24h, train_loss_step=0.548, train_loss_epoch=0.499]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:15<00:00, 27.05it/s, v_num=_24h, train_loss_step=0.548, train_loss_epoch=0.499]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇████████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▅▇▄▆▅▄▅▃▄▃▃▄▅▄▅▅▃▃▆▄▃▂▄▂▃▂▁▂▃▂▁▃▂▃▃▃▂▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.49887</td></tr><tr><td>train_loss_step</td><td>0.5477</td></tr><tr><td>trainer/global_step</td><td>13360</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g2_train_run_24h</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g2_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g2_train_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_165658-g2_train_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:42:19.536609Z",
     "start_time": "2025-04-16T03:42:18.027706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = g2_test_f_loader\n",
    "\n",
    "CKPT_PATH = os.path.join(SAVEPATH, FILENAME+'.ckpt')\n",
    "\n",
    "multigraph = Multigraph.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    embedding_dim=emb_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels_gnn=config['gnn_hidden'],\n",
    "    out_channels_gnn=config['gnn_hidden'],\n",
    "    num_layers_gnn=config['gnn_layers'],\n",
    "    heads=config['heads'],\n",
    "    hidden_channels_deepset=config['gnn_hidden'],\n",
    "    optimizer_class=AdamW,\n",
    "    optimizer_params=dict(lr=config['lr']),\n",
    ")\n",
    "\n",
    "multigraph.eval()\n",
    "trainer = L.Trainer()\n",
    "# trainer.fit(multigraph, ckpt_path=CKPT_PATH)\n",
    "\n",
    "preds_list = []\n",
    "preds = trainer.predict(model=multigraph, dataloaders=[test_loader]) # 92 x 976 x 2 forecasts with mu and sigma of 122 stations\n",
    "# print(preds)\n",
    "print(preds[0].shape)\n",
    "# preds = [prediction.reshape(1, 122, 2).mean(axis=0) for prediction in\n",
    "#          preds]\n",
    "#ACHTUNG - reshape(1, 122, 2) mit 1 statt 5!\n",
    "preds = torch.cat(preds, dim=0)\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "d6c429c821c3f37a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:01<00:00, 67.75it/s]\n",
      "torch.Size([976, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6623683693510217\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Graph3: more edges, more attributes",
   "id": "a0c1744f85b8f397"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:43:50.058620Z",
     "start_time": "2025-04-16T03:43:14.684804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graphs3_train_rf, tests3 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"geo\", \"alt\", \"lon\", \"lat\", \"alt-orog\"], edges=[(\"geo\", 55), (\"alt\", 6.5), (\"alt-orog\", 2.5)], sum_stats = True)\n",
    "\n",
    "graphs3_test_rf, graphs3_test_f = tests3\n",
    "graphs3_test = graphs3_test_rf\n",
    "\n",
    "facts_about(graphs3_train_rf[0])"
   ],
   "id": "75caf5171f131f40",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3448/3448 [00:17<00:00, 200.95it/s]\n",
      "100%|██████████| 732/732 [00:02<00:00, 263.88it/s]\n",
      "100%|██████████| 730/730 [00:02<00:00, 274.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 122 with feature dimension of x: 65\n",
      "Number of isolated nodes: 2\n",
      "Number of edges: 1482 with edge dimension: 5\n",
      "Average node degree: 12.147541046142578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:45:23.524876Z",
     "start_time": "2025-04-16T03:45:23.516353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "g3_train_loader = DataLoader(graphs3_train_rf, batch_size=batch_size, shuffle=True)\n",
    "g3_test_f_loader = DataLoader(graphs3_test_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim = 20\n",
    "in_channels = graphs3_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs3_train_rf[0].num_edge_features"
   ],
   "id": "a0f25b681ac46e90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:44:49.690336Z",
     "start_time": "2025-04-16T03:44:49.686547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"new_attr_graphs\"\n",
    "FILENAME = \"g3_train_run_24h\""
   ],
   "id": "7fdb19abd7365f3",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T15:17:49.649688Z",
     "start_time": "2025-04-15T15:09:24.948771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_loader = g3_train_loader\n",
    "with wandb.init(\n",
    "        project=PROJECTNAME, id=FILENAME, config=args_dict, tags=[\"reproduction\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "\n",
    "    multigraph = Multigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "    batch = next(iter(train_loader))\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=FILENAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    # print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "cd9efdd82eeac091",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_170924-g3_train_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g3_train_run_24h' target=\"_blank\">g3_train_run_24h</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g3_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g3_train_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/gnn_new_attr_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.9 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.2 M    Total params\n",
      "40.639    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:16<00:00, 26.86it/s, v_num=_24h, train_loss_step=0.521, train_loss_epoch=0.494]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:16<00:00, 26.33it/s, v_num=_24h, train_loss_step=0.521, train_loss_epoch=0.494]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▆▄▅▆▃▇▄▄▄▄▄▄▃▄▅▄▃▃▃▄▁▃▄▅▄▄▃▃▂▃▃▂▂▁▃▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.49358</td></tr><tr><td>train_loss_step</td><td>0.52067</td></tr><tr><td>trainer/global_step</td><td>13360</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g3_train_run_24h</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g3_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g3_train_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_170924-g3_train_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:45:32.933239Z",
     "start_time": "2025-04-16T03:45:32.803071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = g3_test_f_loader\n",
    "\n",
    "CKPT_PATH = os.path.join(SAVEPATH, FILENAME+'.ckpt')\n",
    "\n",
    "multigraph = Multigraph.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    embedding_dim=emb_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels_gnn=config['gnn_hidden'],\n",
    "    out_channels_gnn=config['gnn_hidden'],\n",
    "    num_layers_gnn=config['gnn_layers'],\n",
    "    heads=config['heads'],\n",
    "    hidden_channels_deepset=config['gnn_hidden'],\n",
    "    optimizer_class=AdamW,\n",
    "    optimizer_params=dict(lr=config['lr']),\n",
    ")\n",
    "\n",
    "multigraph.eval()\n",
    "trainer = L.Trainer()"
   ],
   "id": "b6974ec0fd52db80",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:45:37.380569Z",
     "start_time": "2025-04-16T03:45:35.983624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds_list = []\n",
    "preds = trainer.predict(model=multigraph, dataloaders=[test_loader]) # 92 x 976 x 2 forecasts with mu and sigma of 122 stations\n",
    "# print(preds)\n",
    "print(preds[0].shape)\n",
    "# preds = [prediction.reshape(1, 122, 2).mean(axis=0) for prediction in\n",
    "#          preds]\n",
    "#ACHTUNG - reshape(1, 122, 2) mit 1 statt 5!\n",
    "preds = torch.cat(preds, dim=0)\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "aeb6148e0e23d3db",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:01<00:00, 68.95it/s]\n",
      "torch.Size([976, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6568804218453907\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:46:18.835662Z",
     "start_time": "2025-04-16T03:45:55.410965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graphs4_train_rf, tests4 = normalize_features_and_create_graphs1(df_train=dataframes['train'], df_valid_test=[dataframes['test_rf'], dataframes['test_f']], station_df=dataframes['stations'], attributes=[\"dist2\"], edges=[(\"dist2\", 0.005)], sum_stats = True)\n",
    "\n",
    "graphs4_test_rf, graphs4_test_f = tests4\n",
    "graphs4_test = graphs4_test_rf\n",
    "\n",
    "facts_about(graphs4_train_rf[0])"
   ],
   "id": "e8dd9a60273d6744",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3448/3448 [00:17<00:00, 202.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 732/732 [00:03<00:00, 236.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:02<00:00, 276.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 122 with feature dimension of x: 65\n",
      "Number of isolated nodes: 9\n",
      "Number of edges: 1356 with edge dimension: 1\n",
      "Average node degree: 11.114753723144531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:46:18.878351Z",
     "start_time": "2025-04-16T03:46:18.842379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "g4_train_loader = DataLoader(graphs4_train_rf, batch_size=batch_size, shuffle=True)\n",
    "g4_test_f_loader = DataLoader(graphs4_test_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim = 20\n",
    "in_channels = graphs4_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs4_train_rf[0].num_edge_features"
   ],
   "id": "e96b7f610aa38536",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:46:47.169433Z",
     "start_time": "2025-04-16T03:46:47.164722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"new_attr_graphs\"\n",
    "FILENAME = \"g4_train_run_24h\""
   ],
   "id": "b5fd6a4ce0971849",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:24:05.136014Z",
     "start_time": "2025-04-15T16:15:47.897728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = g4_train_loader\n",
    "with wandb.init(\n",
    "        project=PROJECTNAME, id=FILENAME, config=args_dict, tags=[\"reproduction\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "\n",
    "    multigraph = Multigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "    batch = next(iter(train_loader))\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=FILENAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    # print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "2311bf0016af252f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_181547-g4_train_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g4_train_run_24h' target=\"_blank\">g4_train_run_24h</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g4_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g4_train_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/gnn_new_attr_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.9 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 M    Total params\n",
      "40.571    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:15<00:00, 27.85it/s, v_num=_24h, train_loss_step=0.468, train_loss_epoch=0.522]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:15<00:00, 27.33it/s, v_num=_24h, train_loss_step=0.468, train_loss_epoch=0.522]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▂▂▁▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.52206</td></tr><tr><td>train_loss_step</td><td>0.46805</td></tr><tr><td>trainer/global_step</td><td>13360</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g4_train_run_24h</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g4_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g4_train_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_181547-g4_train_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:46:52.129315Z",
     "start_time": "2025-04-16T03:46:52.014126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_loader = g4_test_f_loader\n",
    "\n",
    "CKPT_PATH = os.path.join(SAVEPATH, FILENAME+'.ckpt')\n",
    "\n",
    "multigraph = Multigraph.load_from_checkpoint(\n",
    "    CKPT_PATH,\n",
    "    embedding_dim=emb_dim,\n",
    "    edge_dim=edge_dim,\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels_gnn=config['gnn_hidden'],\n",
    "    out_channels_gnn=config['gnn_hidden'],\n",
    "    num_layers_gnn=config['gnn_layers'],\n",
    "    heads=config['heads'],\n",
    "    hidden_channels_deepset=config['gnn_hidden'],\n",
    "    optimizer_class=AdamW,\n",
    "    optimizer_params=dict(lr=config['lr']),\n",
    ")\n",
    "\n",
    "multigraph.eval()\n",
    "trainer = L.Trainer()"
   ],
   "id": "8480a90f13de420e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T03:46:57.209744Z",
     "start_time": "2025-04-16T03:46:55.828100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds_list = []\n",
    "preds = trainer.predict(model=multigraph, dataloaders=[test_loader]) # 92 x 976 x 2 forecasts with mu and sigma of 122 stations\n",
    "# print(preds)\n",
    "print(preds[0].shape)\n",
    "# preds = [prediction.reshape(1, 122, 2).mean(axis=0) for prediction in\n",
    "#          preds]\n",
    "#ACHTUNG - reshape(1, 122, 2) mit 1 statt 5!\n",
    "preds = torch.cat(preds, dim=0)\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "37e5ec2881d6b99a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:01<00:00, 69.49it/s]\n",
      "torch.Size([976, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6382404991725519\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:34:30.496274Z",
     "start_time": "2025-04-15T16:33:53.369591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graphs5_train_rf, tests5 = normalize_features_and_create_graphs1(df_train=dataframes['train'],\n",
    "                                                                 df_valid_test=[dataframes['test_rf'],\n",
    "                                                                                dataframes['test_f']],\n",
    "                                                                 station_df=dataframes['stations'],\n",
    "                                                                 attributes=[\"geo\", \"alt\", \"lon\", \"lat\", \"alt-orog\"],\n",
    "                                                                 edges=[(\"geo\", 100), (\"alt\", 10), (\"alt-orog\", 5)],\n",
    "                                                                 sum_stats=True)\n",
    "\n",
    "graphs5_test_rf, graphs5_test_f = tests5\n",
    "graphs5_test = graphs5_test_rf\n",
    "\n",
    "facts_about(graphs5_train_rf[0])\n"
   ],
   "id": "2d49b43bfb605235",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3448 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 17/3448 [00:00<00:20, 166.12it/s]\u001B[A\n",
      "  1%|          | 39/3448 [00:00<00:17, 193.99it/s]\u001B[A\n",
      "  2%|▏         | 59/3448 [00:00<00:17, 190.98it/s]\u001B[A\n",
      "  2%|▏         | 79/3448 [00:00<00:17, 189.93it/s]\u001B[A\n",
      "  3%|▎         | 101/3448 [00:00<00:16, 199.17it/s]\u001B[A\n",
      "  4%|▎         | 122/3448 [00:00<00:16, 202.37it/s]\u001B[A\n",
      "  4%|▍         | 144/3448 [00:00<00:16, 205.60it/s]\u001B[A\n",
      "  5%|▍         | 166/3448 [00:00<00:15, 209.34it/s]\u001B[A\n",
      "  5%|▌         | 188/3448 [00:00<00:15, 211.69it/s]\u001B[A\n",
      "  6%|▌         | 210/3448 [00:01<00:15, 208.22it/s]\u001B[A\n",
      "  7%|▋         | 231/3448 [00:01<00:16, 195.82it/s]\u001B[A\n",
      "  7%|▋         | 252/3448 [00:01<00:16, 198.49it/s]\u001B[A\n",
      "  8%|▊         | 272/3448 [00:01<00:17, 183.98it/s]\u001B[A\n",
      "  9%|▊         | 294/3448 [00:01<00:16, 191.63it/s]\u001B[A\n",
      "  9%|▉         | 314/3448 [00:01<00:16, 188.01it/s]\u001B[A\n",
      " 10%|▉         | 333/3448 [00:01<00:16, 184.89it/s]\u001B[A\n",
      " 10%|█         | 352/3448 [00:01<00:17, 176.39it/s]\u001B[A\n",
      " 11%|█         | 373/3448 [00:01<00:16, 185.22it/s]\u001B[A\n",
      " 11%|█▏        | 392/3448 [00:02<00:16, 180.34it/s]\u001B[A\n",
      " 12%|█▏        | 413/3448 [00:02<00:16, 186.78it/s]\u001B[A\n",
      " 13%|█▎        | 432/3448 [00:02<00:16, 183.67it/s]\u001B[A\n",
      " 13%|█▎        | 451/3448 [00:02<00:17, 176.25it/s]\u001B[A\n",
      " 14%|█▎        | 471/3448 [00:02<00:16, 182.29it/s]\u001B[A\n",
      " 14%|█▍        | 490/3448 [00:02<00:16, 179.51it/s]\u001B[A\n",
      " 15%|█▍        | 512/3448 [00:02<00:15, 189.34it/s]\u001B[A\n",
      " 15%|█▌        | 534/3448 [00:02<00:14, 196.82it/s]\u001B[A\n",
      " 16%|█▌        | 556/3448 [00:02<00:14, 201.19it/s]\u001B[A\n",
      " 17%|█▋        | 577/3448 [00:03<00:14, 199.53it/s]\u001B[A\n",
      " 17%|█▋        | 598/3448 [00:03<00:14, 200.17it/s]\u001B[A\n",
      " 18%|█▊        | 619/3448 [00:03<00:14, 201.71it/s]\u001B[A\n",
      " 19%|█▊        | 640/3448 [00:03<00:13, 203.62it/s]\u001B[A\n",
      " 19%|█▉        | 661/3448 [00:03<00:14, 189.75it/s]\u001B[A\n",
      " 20%|█▉        | 681/3448 [00:03<00:15, 179.54it/s]\u001B[A\n",
      " 20%|██        | 700/3448 [00:03<00:15, 179.61it/s]\u001B[A\n",
      " 21%|██        | 719/3448 [00:03<00:15, 179.62it/s]\u001B[A\n",
      " 21%|██▏       | 740/3448 [00:03<00:14, 186.46it/s]\u001B[A\n",
      " 22%|██▏       | 762/3448 [00:03<00:13, 193.58it/s]\u001B[A\n",
      " 23%|██▎       | 783/3448 [00:04<00:13, 197.66it/s]\u001B[A\n",
      " 23%|██▎       | 803/3448 [00:04<00:14, 188.73it/s]\u001B[A\n",
      " 24%|██▍       | 822/3448 [00:04<00:14, 175.89it/s]\u001B[A\n",
      " 24%|██▍       | 843/3448 [00:04<00:14, 184.68it/s]\u001B[A\n",
      " 25%|██▌       | 864/3448 [00:04<00:13, 190.14it/s]\u001B[A\n",
      " 26%|██▌       | 884/3448 [00:04<00:14, 180.27it/s]\u001B[A\n",
      " 26%|██▌       | 904/3448 [00:04<00:13, 185.66it/s]\u001B[A\n",
      " 27%|██▋       | 923/3448 [00:04<00:13, 181.87it/s]\u001B[A\n",
      " 27%|██▋       | 944/3448 [00:04<00:13, 188.98it/s]\u001B[A\n",
      " 28%|██▊       | 964/3448 [00:05<00:13, 185.84it/s]\u001B[A\n",
      " 29%|██▊       | 983/3448 [00:05<00:26, 92.29it/s] \u001B[A\n",
      " 29%|██▉       | 1003/3448 [00:05<00:22, 109.53it/s]\u001B[A\n",
      " 30%|██▉       | 1024/3448 [00:05<00:18, 127.90it/s]\u001B[A\n",
      " 30%|███       | 1042/3448 [00:05<00:17, 138.84it/s]\u001B[A\n",
      " 31%|███       | 1063/3448 [00:05<00:15, 154.80it/s]\u001B[A\n",
      " 31%|███▏      | 1084/3448 [00:06<00:14, 167.62it/s]\u001B[A\n",
      " 32%|███▏      | 1104/3448 [00:06<00:13, 167.72it/s]\u001B[A\n",
      " 33%|███▎      | 1123/3448 [00:06<00:14, 160.54it/s]\u001B[A\n",
      " 33%|███▎      | 1142/3448 [00:06<00:13, 167.20it/s]\u001B[A\n",
      " 34%|███▎      | 1163/3448 [00:06<00:12, 177.19it/s]\u001B[A\n",
      " 34%|███▍      | 1184/3448 [00:06<00:12, 185.30it/s]\u001B[A\n",
      " 35%|███▍      | 1205/3448 [00:06<00:11, 191.71it/s]\u001B[A\n",
      " 36%|███▌      | 1227/3448 [00:06<00:11, 197.44it/s]\u001B[A\n",
      " 36%|███▌      | 1249/3448 [00:06<00:10, 201.47it/s]\u001B[A\n",
      " 37%|███▋      | 1270/3448 [00:07<00:10, 202.08it/s]\u001B[A\n",
      " 37%|███▋      | 1291/3448 [00:07<00:11, 193.06it/s]\u001B[A\n",
      " 38%|███▊      | 1312/3448 [00:07<00:10, 196.28it/s]\u001B[A\n",
      " 39%|███▊      | 1332/3448 [00:07<00:11, 183.23it/s]\u001B[A\n",
      " 39%|███▉      | 1353/3448 [00:07<00:11, 190.01it/s]\u001B[A\n",
      " 40%|███▉      | 1373/3448 [00:07<00:11, 185.48it/s]\u001B[A\n",
      " 40%|████      | 1394/3448 [00:07<00:10, 191.20it/s]\u001B[A\n",
      " 41%|████      | 1415/3448 [00:07<00:10, 196.05it/s]\u001B[A\n",
      " 42%|████▏     | 1437/3448 [00:07<00:10, 200.88it/s]\u001B[A\n",
      " 42%|████▏     | 1458/3448 [00:08<00:10, 194.08it/s]\u001B[A\n",
      " 43%|████▎     | 1479/3448 [00:08<00:09, 198.46it/s]\u001B[A\n",
      " 44%|████▎     | 1500/3448 [00:08<00:09, 201.68it/s]\u001B[A\n",
      " 44%|████▍     | 1521/3448 [00:08<00:09, 203.56it/s]\u001B[A\n",
      " 45%|████▍     | 1542/3448 [00:08<00:09, 204.33it/s]\u001B[A\n",
      " 45%|████▌     | 1563/3448 [00:08<00:09, 204.47it/s]\u001B[A\n",
      " 46%|████▌     | 1584/3448 [00:08<00:09, 206.07it/s]\u001B[A\n",
      " 47%|████▋     | 1606/3448 [00:08<00:08, 207.82it/s]\u001B[A\n",
      " 47%|████▋     | 1628/3448 [00:08<00:08, 208.72it/s]\u001B[A\n",
      " 48%|████▊     | 1649/3448 [00:08<00:08, 209.05it/s]\u001B[A\n",
      " 48%|████▊     | 1670/3448 [00:09<00:08, 208.55it/s]\u001B[A\n",
      " 49%|████▉     | 1691/3448 [00:09<00:08, 199.15it/s]\u001B[A\n",
      " 50%|████▉     | 1712/3448 [00:09<00:08, 196.18it/s]\u001B[A\n",
      " 50%|█████     | 1732/3448 [00:09<00:09, 188.32it/s]\u001B[A\n",
      " 51%|█████     | 1753/3448 [00:09<00:08, 192.83it/s]\u001B[A\n",
      " 51%|█████▏    | 1773/3448 [00:09<00:08, 190.23it/s]\u001B[A\n",
      " 52%|█████▏    | 1793/3448 [00:09<00:08, 192.94it/s]\u001B[A\n",
      " 53%|█████▎    | 1814/3448 [00:09<00:08, 196.94it/s]\u001B[A\n",
      " 53%|█████▎    | 1834/3448 [00:09<00:08, 191.32it/s]\u001B[A\n",
      " 54%|█████▍    | 1854/3448 [00:10<00:08, 186.42it/s]\u001B[A\n",
      " 54%|█████▍    | 1875/3448 [00:10<00:08, 191.91it/s]\u001B[A\n",
      " 55%|█████▍    | 1895/3448 [00:10<00:08, 189.48it/s]\u001B[A\n",
      " 56%|█████▌    | 1915/3448 [00:10<00:08, 189.11it/s]\u001B[A\n",
      " 56%|█████▌    | 1934/3448 [00:10<00:08, 185.57it/s]\u001B[A\n",
      " 57%|█████▋    | 1953/3448 [00:10<00:08, 184.63it/s]\u001B[A\n",
      " 57%|█████▋    | 1972/3448 [00:10<00:08, 176.26it/s]\u001B[A\n",
      " 58%|█████▊    | 1994/3448 [00:10<00:07, 188.30it/s]\u001B[A\n",
      " 58%|█████▊    | 2013/3448 [00:10<00:07, 183.70it/s]\u001B[A\n",
      " 59%|█████▉    | 2035/3448 [00:10<00:07, 192.30it/s]\u001B[A\n",
      " 60%|█████▉    | 2055/3448 [00:11<00:07, 181.26it/s]\u001B[A\n",
      " 60%|██████    | 2074/3448 [00:11<00:07, 175.28it/s]\u001B[A\n",
      " 61%|██████    | 2096/3448 [00:11<00:07, 186.46it/s]\u001B[A\n",
      " 61%|██████▏   | 2115/3448 [00:11<00:07, 184.43it/s]\u001B[A\n",
      " 62%|██████▏   | 2136/3448 [00:11<00:06, 190.94it/s]\u001B[A\n",
      " 63%|██████▎   | 2156/3448 [00:11<00:07, 180.02it/s]\u001B[A\n",
      " 63%|██████▎   | 2175/3448 [00:11<00:07, 174.22it/s]\u001B[A\n",
      " 64%|██████▎   | 2193/3448 [00:11<00:07, 174.58it/s]\u001B[A\n",
      " 64%|██████▍   | 2215/3448 [00:11<00:06, 186.06it/s]\u001B[A\n",
      " 65%|██████▍   | 2237/3448 [00:12<00:06, 193.48it/s]\u001B[A\n",
      " 65%|██████▌   | 2258/3448 [00:12<00:06, 196.18it/s]\u001B[A\n",
      " 66%|██████▌   | 2280/3448 [00:12<00:05, 201.30it/s]\u001B[A\n",
      " 67%|██████▋   | 2302/3448 [00:12<00:05, 204.57it/s]\u001B[A\n",
      " 67%|██████▋   | 2323/3448 [00:12<00:05, 204.49it/s]\u001B[A\n",
      " 68%|██████▊   | 2344/3448 [00:12<00:05, 193.71it/s]\u001B[A\n",
      " 69%|██████▊   | 2364/3448 [00:12<00:05, 188.42it/s]\u001B[A\n",
      " 69%|██████▉   | 2383/3448 [00:12<00:05, 184.08it/s]\u001B[A\n",
      " 70%|██████▉   | 2405/3448 [00:12<00:05, 193.13it/s]\u001B[A\n",
      " 70%|███████   | 2425/3448 [00:13<00:05, 181.93it/s]\u001B[A\n",
      " 71%|███████   | 2446/3448 [00:13<00:05, 189.22it/s]\u001B[A\n",
      " 72%|███████▏  | 2468/3448 [00:13<00:04, 196.14it/s]\u001B[A\n",
      " 72%|███████▏  | 2488/3448 [00:13<00:05, 186.61it/s]\u001B[A\n",
      " 73%|███████▎  | 2510/3448 [00:13<00:04, 194.92it/s]\u001B[A\n",
      " 73%|███████▎  | 2532/3448 [00:13<00:04, 200.50it/s]\u001B[A\n",
      " 74%|███████▍  | 2553/3448 [00:13<00:04, 193.56it/s]\u001B[A\n",
      " 75%|███████▍  | 2573/3448 [00:13<00:04, 192.71it/s]\u001B[A\n",
      " 75%|███████▌  | 2593/3448 [00:13<00:04, 189.10it/s]\u001B[A\n",
      " 76%|███████▌  | 2615/3448 [00:14<00:04, 196.81it/s]\u001B[A\n",
      " 76%|███████▋  | 2635/3448 [00:14<00:04, 193.51it/s]\u001B[A\n",
      " 77%|███████▋  | 2657/3448 [00:14<00:03, 199.03it/s]\u001B[A\n",
      " 78%|███████▊  | 2679/3448 [00:14<00:03, 199.54it/s]\u001B[A\n",
      " 78%|███████▊  | 2699/3448 [00:14<00:03, 187.84it/s]\u001B[A\n",
      " 79%|███████▉  | 2718/3448 [00:14<00:04, 176.65it/s]\u001B[A\n",
      " 79%|███████▉  | 2736/3448 [00:14<00:04, 177.17it/s]\u001B[A\n",
      " 80%|███████▉  | 2758/3448 [00:14<00:03, 188.27it/s]\u001B[A\n",
      " 81%|████████  | 2780/3448 [00:14<00:03, 196.71it/s]\u001B[A\n",
      " 81%|████████  | 2800/3448 [00:15<00:03, 188.22it/s]\u001B[A\n",
      " 82%|████████▏ | 2821/3448 [00:15<00:03, 193.77it/s]\u001B[A\n",
      " 82%|████████▏ | 2843/3448 [00:15<00:03, 199.64it/s]\u001B[A\n",
      " 83%|████████▎ | 2864/3448 [00:15<00:02, 200.76it/s]\u001B[A\n",
      " 84%|████████▎ | 2886/3448 [00:15<00:02, 204.16it/s]\u001B[A\n",
      " 84%|████████▍ | 2908/3448 [00:15<00:02, 207.48it/s]\u001B[A\n",
      " 85%|████████▍ | 2930/3448 [00:15<00:02, 209.33it/s]\u001B[A\n",
      " 86%|████████▌ | 2951/3448 [00:15<00:02, 198.17it/s]\u001B[A\n",
      " 86%|████████▌ | 2973/3448 [00:15<00:02, 202.32it/s]\u001B[A\n",
      " 87%|████████▋ | 2995/3448 [00:15<00:02, 206.70it/s]\u001B[A\n",
      " 87%|████████▋ | 3016/3448 [00:16<00:02, 197.85it/s]\u001B[A\n",
      " 88%|████████▊ | 3036/3448 [00:16<00:02, 187.50it/s]\u001B[A\n",
      " 89%|████████▊ | 3055/3448 [00:16<00:02, 185.14it/s]\u001B[A\n",
      " 89%|████████▉ | 3077/3448 [00:16<00:01, 193.72it/s]\u001B[A\n",
      " 90%|████████▉ | 3097/3448 [00:16<00:01, 189.47it/s]\u001B[A\n",
      " 90%|█████████ | 3117/3448 [00:16<00:01, 188.95it/s]\u001B[A\n",
      " 91%|█████████ | 3136/3448 [00:16<00:01, 180.01it/s]\u001B[A\n",
      " 92%|█████████▏| 3158/3448 [00:16<00:01, 190.10it/s]\u001B[A\n",
      " 92%|█████████▏| 3178/3448 [00:16<00:01, 188.27it/s]\u001B[A\n",
      " 93%|█████████▎| 3200/3448 [00:17<00:01, 194.95it/s]\u001B[A\n",
      " 93%|█████████▎| 3220/3448 [00:17<00:01, 191.60it/s]\u001B[A\n",
      " 94%|█████████▍| 3243/3448 [00:17<00:01, 200.39it/s]\u001B[A\n",
      " 95%|█████████▍| 3266/3448 [00:17<00:00, 206.59it/s]\u001B[A\n",
      " 95%|█████████▌| 3289/3448 [00:17<00:00, 210.97it/s]\u001B[A\n",
      " 96%|█████████▌| 3311/3448 [00:17<00:00, 203.08it/s]\u001B[A\n",
      " 97%|█████████▋| 3333/3448 [00:17<00:00, 207.09it/s]\u001B[A\n",
      " 97%|█████████▋| 3355/3448 [00:17<00:00, 210.38it/s]\u001B[A\n",
      " 98%|█████████▊| 3377/3448 [00:17<00:00, 212.93it/s]\u001B[A\n",
      " 99%|█████████▊| 3399/3448 [00:18<00:00, 204.92it/s]\u001B[A\n",
      " 99%|█████████▉| 3420/3448 [00:18<00:00, 184.89it/s]\u001B[A\n",
      "100%|██████████| 3448/3448 [00:18<00:00, 188.20it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/732 [00:00<?, ?it/s]\u001B[A\n",
      "  4%|▍         | 28/732 [00:00<00:02, 278.13it/s]\u001B[A\n",
      "  8%|▊         | 57/732 [00:00<00:02, 279.47it/s]\u001B[A\n",
      " 12%|█▏        | 85/732 [00:00<00:02, 260.69it/s]\u001B[A\n",
      " 16%|█▌        | 114/732 [00:00<00:02, 268.45it/s]\u001B[A\n",
      " 19%|█▉        | 141/732 [00:00<00:02, 259.55it/s]\u001B[A\n",
      " 23%|██▎       | 168/732 [00:00<00:02, 250.72it/s]\u001B[A\n",
      " 27%|██▋       | 196/732 [00:00<00:02, 259.51it/s]\u001B[A\n",
      " 31%|███       | 224/732 [00:00<00:01, 265.70it/s]\u001B[A\n",
      " 35%|███▍      | 253/732 [00:00<00:01, 270.19it/s]\u001B[A\n",
      " 38%|███▊      | 281/732 [00:01<00:01, 273.05it/s]\u001B[A\n",
      " 42%|████▏     | 309/732 [00:01<00:01, 274.12it/s]\u001B[A\n",
      " 46%|████▌     | 337/732 [00:01<00:01, 259.14it/s]\u001B[A\n",
      " 50%|████▉     | 365/732 [00:01<00:01, 264.38it/s]\u001B[A\n",
      " 54%|█████▎    | 392/732 [00:01<00:01, 261.43it/s]\u001B[A\n",
      " 58%|█████▊    | 421/732 [00:01<00:01, 269.41it/s]\u001B[A\n",
      " 61%|██████▏   | 449/732 [00:01<00:01, 261.46it/s]\u001B[A\n",
      " 65%|██████▌   | 478/732 [00:01<00:00, 267.07it/s]\u001B[A\n",
      " 69%|██████▉   | 506/732 [00:01<00:00, 270.45it/s]\u001B[A\n",
      " 73%|███████▎  | 534/732 [00:02<00:00, 262.40it/s]\u001B[A\n",
      " 77%|███████▋  | 561/732 [00:02<00:00, 258.01it/s]\u001B[A\n",
      " 80%|████████  | 589/732 [00:02<00:00, 264.19it/s]\u001B[A\n",
      " 84%|████████▍ | 618/732 [00:02<00:00, 269.88it/s]\u001B[A\n",
      " 88%|████████▊ | 646/732 [00:02<00:00, 272.24it/s]\u001B[A\n",
      " 92%|█████████▏| 675/732 [00:02<00:00, 276.34it/s]\u001B[A\n",
      " 96%|█████████▌| 703/732 [00:02<00:00, 263.78it/s]\u001B[A\n",
      "100%|██████████| 732/732 [00:03<00:00, 233.99it/s]\u001B[A\n",
      "\n",
      "  0%|          | 0/730 [00:00<?, ?it/s]\u001B[A\n",
      "  4%|▎         | 26/730 [00:00<00:02, 253.11it/s]\u001B[A\n",
      "  7%|▋         | 53/730 [00:00<00:02, 258.66it/s]\u001B[A\n",
      " 11%|█         | 79/730 [00:00<00:02, 258.63it/s]\u001B[A\n",
      " 14%|█▍        | 105/730 [00:00<00:02, 251.53it/s]\u001B[A\n",
      " 18%|█▊        | 131/730 [00:00<00:02, 248.44it/s]\u001B[A\n",
      " 21%|██▏       | 156/730 [00:00<00:02, 247.89it/s]\u001B[A\n",
      " 25%|██▍       | 181/730 [00:00<00:02, 244.59it/s]\u001B[A\n",
      " 29%|██▉       | 210/730 [00:00<00:02, 257.54it/s]\u001B[A\n",
      " 32%|███▏      | 237/730 [00:00<00:01, 260.73it/s]\u001B[A\n",
      " 36%|███▌      | 264/730 [00:01<00:01, 255.79it/s]\u001B[A\n",
      " 40%|████      | 293/730 [00:01<00:01, 263.82it/s]\u001B[A\n",
      " 44%|████▍     | 320/730 [00:01<00:01, 258.60it/s]\u001B[A\n",
      " 48%|████▊     | 348/730 [00:01<00:01, 264.53it/s]\u001B[A\n",
      " 52%|█████▏    | 377/730 [00:01<00:01, 269.48it/s]\u001B[A\n",
      " 55%|█████▌    | 404/730 [00:01<00:01, 249.95it/s]\u001B[A\n",
      " 59%|█████▉    | 430/730 [00:01<00:01, 246.94it/s]\u001B[A\n",
      " 62%|██████▏   | 455/730 [00:01<00:01, 237.82it/s]\u001B[A\n",
      " 66%|██████▌   | 479/730 [00:01<00:01, 235.56it/s]\u001B[A\n",
      " 70%|██████▉   | 508/730 [00:02<00:00, 249.57it/s]\u001B[A\n",
      " 74%|███████▎  | 537/730 [00:02<00:00, 260.01it/s]\u001B[A\n",
      " 78%|███████▊  | 566/730 [00:02<00:00, 267.57it/s]\u001B[A\n",
      " 81%|████████  | 593/730 [00:02<00:00, 250.35it/s]\u001B[A\n",
      " 85%|████████▌ | 622/730 [00:02<00:00, 259.76it/s]\u001B[A\n",
      " 89%|████████▉ | 649/730 [00:02<00:00, 248.93it/s]\u001B[A\n",
      " 92%|█████████▏| 675/730 [00:02<00:00, 239.12it/s]\u001B[A\n",
      " 96%|█████████▋| 703/730 [00:02<00:00, 249.30it/s]\u001B[A\n",
      "100%|██████████| 730/730 [00:02<00:00, 251.44it/s]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 122 with feature dimension of x: 65\n",
      "Number of isolated nodes: 1\n",
      "Number of edges: 2804 with edge dimension: 5\n",
      "Average node degree: 22.983606338500977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:34:30.508549Z",
     "start_time": "2025-04-15T16:34:30.502422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "g5_train_loader = DataLoader(graphs5_train_rf, batch_size=batch_size, shuffle=True)\n",
    "g5_test_f_loader = DataLoader(graphs5_test_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim = 20\n",
    "in_channels = graphs5_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs5_train_rf[0].num_edge_features\n"
   ],
   "id": "bddb4f7ef699d832",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T16:46:50.092722Z",
     "start_time": "2025-04-15T16:35:26.418431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"new_attr_graphs\"\n",
    "FILENAME = \"g5_train_run_24h\"\n",
    "train_loader = g5_train_loader\n",
    "with wandb.init(\n",
    "        project=PROJECTNAME, id=FILENAME, config=args_dict, tags=[\"reproduction\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "\n",
    "    multigraph = Multigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "    batch = next(iter(train_loader))\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=FILENAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    # print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "217cadbf04b2a496",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_183526-g5_train_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g5_train_run_24h' target=\"_blank\">g5_train_run_24h</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g5_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g5_train_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/gnn_new_attr_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.9 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.2 M    Total params\n",
      "40.639    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:21<00:00, 19.89it/s, v_num=_24h, train_loss_step=0.610, train_loss_epoch=0.487]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:21<00:00, 19.62it/s, v_num=_24h, train_loss_step=0.610, train_loss_epoch=0.487]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇█████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.48651</td></tr><tr><td>train_loss_step</td><td>0.61032</td></tr><tr><td>trainer/global_step</td><td>13360</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g5_train_run_24h</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g5_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g5_train_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_183526-g5_train_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:15:03.021176Z",
     "start_time": "2025-04-15T17:15:00.893229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds_list = []\n",
    "preds = trainer.predict(model=multigraph, dataloaders=[g5_test_f_loader]) # 92 x 976 x 2 forecasts with mu and sigma of 122 stations\n",
    "# print(preds)\n",
    "print(preds[0].shape)\n",
    "# preds = [prediction.reshape(1, 122, 2).mean(axis=0) for prediction in\n",
    "#          preds]\n",
    "#ACHTUNG - reshape(1, 122, 2) mit 1 statt 5!\n",
    "preds = torch.cat(preds, dim=0)\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "8498b63441dceca5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:02<00:00, 44.41it/s]\n",
      "torch.Size([976, 2])\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6466054917523435\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:41:50.259866Z",
     "start_time": "2025-04-15T17:41:27.211685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graphs6_train_rf, tests6 = normalize_features_and_create_graphs1(df_train=dataframes['train'],\n",
    "                                                                 df_valid_test=[dataframes['test_rf'],\n",
    "                                                                                dataframes['test_f']],\n",
    "                                                                 station_df=dataframes['stations'],\n",
    "                                                                 attributes=[\"dist3\"],\n",
    "                                                                 edges=[(\"dist3\", 0.015)],\n",
    "                                                                 sum_stats=True)\n",
    "\n",
    "graphs6_test_rf, graphs6_test_f = tests6\n",
    "graphs6_test = graphs6_test_rf\n",
    "\n",
    "facts_about(graphs6_train_rf[0])"
   ],
   "id": "a942a32dfd942af0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "fit_transform\n",
      "transform 1\n",
      "transform 2\n",
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3448/3448 [00:16<00:00, 204.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 732/732 [00:02<00:00, 273.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading distances from file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [00:02<00:00, 263.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 122 with feature dimension of x: 65\n",
      "Number of isolated nodes: 26\n",
      "Number of edges: 1514 with edge dimension: 1\n",
      "Average node degree: 12.409835815429688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:42:11.915952Z",
     "start_time": "2025-04-15T17:42:11.909627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = config['batch_size']\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "g6_train_loader = DataLoader(graphs6_train_rf, batch_size=batch_size, shuffle=True)\n",
    "g6_test_f_loader = DataLoader(graphs6_test_f, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim = 20\n",
    "in_channels = graphs6_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "edge_dim = graphs6_train_rf[0].num_edge_features"
   ],
   "id": "6a65d980a9ec5cab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:50:49.710881Z",
     "start_time": "2025-04-15T17:42:21.761173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "PROJECTNAME = \"new_attr_graphs\"\n",
    "FILENAME = \"g6_train_run_24h\"\n",
    "train_loader = g6_train_loader\n",
    "with wandb.init(\n",
    "        project=PROJECTNAME, id=FILENAME, config=args_dict, tags=[\"reproduction\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "\n",
    "    multigraph = Multigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "    batch = next(iter(train_loader))\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=PROJECTNAME)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=FILENAME, monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    # print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "4c343dcc9c3a2d74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250415_194221-g6_train_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g6_train_run_24h' target=\"_blank\">g6_train_run_24h</a></strong> to <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g6_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g6_train_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/ltchen/gnnpp/explored_models/gnn_new_attr_24h/models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.9 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.1 M    Total params\n",
      "40.571    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:16<00:00, 26.74it/s, v_num=_24h, train_loss_step=0.556, train_loss_epoch=0.523]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 431/431 [00:16<00:00, 26.22it/s, v_num=_24h, train_loss_step=0.556, train_loss_epoch=0.523]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇███████</td></tr><tr><td>train_loss_epoch</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>██▆▅▆▅▃▆▅▅▅▄▄▅▃▃▂▂▅▃▃▅▅▄▃▂▂▃▃▄▂▂▂▃▃▂▂▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_loss_epoch</td><td>0.52288</td></tr><tr><td>train_loss_step</td><td>0.55647</td></tr><tr><td>trainer/global_step</td><td>13360</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">g6_train_run_24h</strong> at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g6_train_run_24h' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs/runs/g6_train_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs' target=\"_blank\">https://wandb.ai/leachen01-karlsruhe-institute-of-technology/new_attr_graphs</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_194221-g6_train_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:52:03.201292Z",
     "start_time": "2025-04-15T17:52:01.638509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds_list = []\n",
    "preds = trainer.predict(model=multigraph, dataloaders=[g6_test_f_loader]) # 92 x 976 x 2 forecasts with mu and sigma of 122 stations\n",
    "preds = torch.cat(preds, dim=0)\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "1f3eedac62da94c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 92/92 [00:01<00:00, 61.93it/s]\n",
      "#############################################\n",
      "#############################################\n",
      "final crps: 0.6487236425125072\n",
      "#############################################\n",
      "#############################################\n"
     ]
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
