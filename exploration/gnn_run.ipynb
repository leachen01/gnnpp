{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:11:28.012115Z",
     "start_time": "2025-03-26T00:11:24.932669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /home/ltchen/gnnpp\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch_geometric\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "from typing import Tuple\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import scatter\n",
    "from torch.nn import Linear, ModuleList, ReLU\n",
    "#from torch.utils.data import DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from models.loss import NormalCRPS\n",
    "from models.model_utils import MakePositive, EmbedStations\n",
    "from utils.data import (\n",
    "    load_dataframes,\n",
    "    load_distances,\n",
    "    normalize_features_and_create_graphs,\n",
    "    rm_edges,\n",
    "    summary_statistics,\n",
    ")"
   ],
   "id": "7a72cc47a3f90c32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:11:28.023087Z",
     "start_time": "2025-03-26T00:11:28.019442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DIRECTORY = os.getcwd()\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"explored_models/gnn_24h/models\")\n",
    "JSONPATH = os.path.join(DIRECTORY, \"trained_models/best_24h/params.json\")"
   ],
   "id": "f9b587b1ca8953b6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:11:28.789435Z",
     "start_time": "2025-03-26T00:11:28.772594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "config = args_dict\n",
    "print(config)\n",
    "print(config['lr'])\n",
    "print(config['max_dist'])\n",
    "print(type(config))\n",
    "'''{\"batch_size\":8,\n",
    "\"gnn_hidden\":265,\n",
    "\"gnn_layers\":2,\n",
    "\"heads\":8,\n",
    "\"lr\":0.0002,\n",
    "\"max_dist\":100,\n",
    "\"max_epochs\": 31}'''"
   ],
   "id": "a21cd6960e3d25fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/best_24h/params.json\n",
      "{'batch_size': 8, 'gnn_hidden': 265, 'gnn_layers': 2, 'heads': 8, 'lr': 0.0002, 'max_dist': 100, 'max_epochs': 31}\n",
      "0.0002\n",
      "100\n",
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"batch_size\":8,\\n\"gnn_hidden\":265,\\n\"gnn_layers\":2,\\n\"heads\":8,\\n\"lr\":0.0002,\\n\"max_dist\":100,\\n\"max_epochs\": 31}'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:11:33.909225Z",
     "start_time": "2025-03-26T00:11:31.345043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load graph only for rf\n",
    "dataframes = load_dataframes(mode=\"eval\", leadtime= \"24h\")\n",
    "dist = load_distances(dataframes[\"stations\"])\n"
   ],
   "id": "c9e52c29f737905a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Computing distances...\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T23:43:10.281162Z",
     "start_time": "2025-03-25T23:43:10.274291Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"dist.shape: {dist.shape}\")",
   "id": "3fcc41fb67d441c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist.shape: (122, 122)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:31:21.909746Z",
     "start_time": "2025-03-26T00:31:21.898808Z"
    }
   },
   "cell_type": "code",
   "source": "print(next(iter(train_loader)))\n",
   "id": "8caf207afba1a1d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[10736, 36], edge_index=[2, 232320], edge_attr=[232320, 1], y=[976], timestamp=[8], n_idx=[10736], batch=[10736], ptr=[9])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T23:56:53.197605Z",
     "start_time": "2025-03-25T23:56:48.219266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# just for review to understand the difference between gnn and drn\n",
    "\n",
    "dataframes = load_dataframes(mode=\"train\", leadtime=\"24h\")\n",
    "dist = load_distances(dataframes[\"stations\"])\n",
    "\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"],\n",
    "    valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "    mat=dist,\n",
    "    max_dist=config['max_dist'],\n",
    ")\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "# wo ist alles was hinter diesem Code kommt : pop stations, reset_index, dropnans in normalize_features_and_create_graphs\n",
    "dataframes.pop(\"stations\") # .pop(\"stations\") => entfernt den df mit stations, wofuer brauche ich die dann Ã¼berhaupt? Grafik?\n",
    "\n",
    "# test\n",
    "for X, y in dataframes.values(): # wofuer?\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "##\n",
    "train, valid_test = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    ")\n",
    "\n",
    "train = drop_nans(train)\n",
    "(test_rf, test_f) = valid_test\n",
    "test_rf = drop_nans(test_rf)\n",
    "test_f = drop_nans(test_f)\n",
    "\n",
    "DIRECTORY = os.getcwd()\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"explored_models/drn_24h/models\")"
   ],
   "id": "de0f8f7cd13ca828",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Computing distances...\n",
      "[INFO] Normalizing features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m dataframes \u001B[38;5;241m=\u001B[39m load_dataframes(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, leadtime\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m24h\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m dist \u001B[38;5;241m=\u001B[39m load_distances(dataframes[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstations\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m----> 6\u001B[0m graphs_train_rf, tests \u001B[38;5;241m=\u001B[39m \u001B[43mnormalize_features_and_create_graphs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataframes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalid_test_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mdataframes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest_rf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataframes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest_f\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdist\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_dist\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax_dist\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m graphs_test_rf, graphs_test_f \u001B[38;5;241m=\u001B[39m tests\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# wo ist alles was hinter diesem Code kommt : pop stations, reset_index, dropnans in normalize_features_and_create_graphs\u001B[39;00m\n",
      "File \u001B[0;32m/tmp/pycharm_project_413/utils/data.py:669\u001B[0m, in \u001B[0;36mnormalize_features_and_create_graphs\u001B[0;34m(training_data, valid_test_data, mat, max_dist)\u001B[0m\n\u001B[1;32m    666\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler()\n\u001B[1;32m    668\u001B[0m \u001B[38;5;66;03m# Fit and transform the selected features\u001B[39;00m\n\u001B[0;32m--> 669\u001B[0m train_rf\u001B[38;5;241m.\u001B[39mloc[:, features_to_normalize] \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_rf\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfeatures_to_normalize\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat32\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    671\u001B[0m train_rf\u001B[38;5;241m.\u001B[39mloc[:, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcos_doy\u001B[39m\u001B[38;5;124m\"\u001B[39m]] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mcos(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m*\u001B[39m train_rf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdayofyear \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m365\u001B[39m)\n\u001B[1;32m    672\u001B[0m train_rf\u001B[38;5;241m.\u001B[39mloc[:, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msin_doy\u001B[39m\u001B[38;5;124m\"\u001B[39m]] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msin(\u001B[38;5;241m2\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mpi \u001B[38;5;241m*\u001B[39m train_rf[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdayofyear \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m365\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    163\u001B[0m         )\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/base.py:916\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[1;32m    914\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    915\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[0;32m--> 916\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    918\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/utils/_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    163\u001B[0m         )\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:1006\u001B[0m, in \u001B[0;36mStandardScaler.transform\u001B[0;34m(self, X, copy)\u001B[0m\n\u001B[1;32m   1003\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1005\u001B[0m copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[0;32m-> 1006\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1013\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[1;32m   1016\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/base.py:605\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    603\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    604\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 605\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[1;32m    607\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/utils/validation.py:915\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    913\u001B[0m         array \u001B[38;5;241m=\u001B[39m xp\u001B[38;5;241m.\u001B[39mastype(array, dtype, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 915\u001B[0m         array \u001B[38;5;241m=\u001B[39m \u001B[43m_asarray_with_order\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[1;32m    917\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    918\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(array)\n\u001B[1;32m    919\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcomplex_warning\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/sklearn/utils/_array_api.py:380\u001B[0m, in \u001B[0;36m_asarray_with_order\u001B[0;34m(array, dtype, order, copy, xp)\u001B[0m\n\u001B[1;32m    378\u001B[0m     array \u001B[38;5;241m=\u001B[39m numpy\u001B[38;5;241m.\u001B[39marray(array, order\u001B[38;5;241m=\u001B[39morder, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 380\u001B[0m     array \u001B[38;5;241m=\u001B[39m \u001B[43mnumpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;66;03m# container that is consistent with the input's namespace.\u001B[39;00m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m xp\u001B[38;5;241m.\u001B[39masarray(array)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/generic.py:1997\u001B[0m, in \u001B[0;36mNDFrame.__array__\u001B[0;34m(self, dtype)\u001B[0m\n\u001B[1;32m   1996\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__array__\u001B[39m(\u001B[38;5;28mself\u001B[39m, dtype: npt\u001B[38;5;241m.\u001B[39mDTypeLike \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m-> 1997\u001B[0m     values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\n\u001B[1;32m   1998\u001B[0m     arr \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(values, dtype\u001B[38;5;241m=\u001B[39mdtype)\n\u001B[1;32m   1999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2000\u001B[0m         astype_is_view(values\u001B[38;5;241m.\u001B[39mdtype, arr\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m   2001\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m using_copy_on_write()\n\u001B[1;32m   2002\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mgr\u001B[38;5;241m.\u001B[39mis_single_block\n\u001B[1;32m   2003\u001B[0m     ):\n\u001B[1;32m   2004\u001B[0m         \u001B[38;5;66;03m# Check if both conversions can be done without a copy\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/frame.py:1001\u001B[0m, in \u001B[0;36mDataFrame._values\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    999\u001B[0m blocks \u001B[38;5;241m=\u001B[39m mgr\u001B[38;5;241m.\u001B[39mblocks\n\u001B[1;32m   1000\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(blocks) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m-> 1001\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ensure_wrapped_if_datetimelike(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m)\n\u001B[1;32m   1003\u001B[0m arr \u001B[38;5;241m=\u001B[39m blocks[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[1;32m   1004\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1005\u001B[0m     \u001B[38;5;66;03m# non-2D ExtensionArray\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/frame.py:11360\u001B[0m, in \u001B[0;36mDataFrame.values\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m  11286\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m  11287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mvalues\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[1;32m  11288\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m  11289\u001B[0m \u001B[38;5;124;03m    Return a Numpy representation of the DataFrame.\u001B[39;00m\n\u001B[1;32m  11290\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m  11358\u001B[0m \u001B[38;5;124;03m           ['monkey', nan, None]], dtype=object)\u001B[39;00m\n\u001B[1;32m  11359\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m> 11360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mgr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/internals/managers.py:1732\u001B[0m, in \u001B[0;36mBlockManager.as_array\u001B[0;34m(self, dtype, copy, na_value)\u001B[0m\n\u001B[1;32m   1730\u001B[0m         arr\u001B[38;5;241m.\u001B[39mflags\u001B[38;5;241m.\u001B[39mwriteable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_interleave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1733\u001B[0m     \u001B[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001B[39;00m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;66;03m# to further copy if copy=True or setting na_value\u001B[39;00m\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mno_default:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/internals/managers.py:1795\u001B[0m, in \u001B[0;36mBlockManager._interleave\u001B[0;34m(self, dtype, na_value)\u001B[0m\n\u001B[1;32m   1793\u001B[0m         arr \u001B[38;5;241m=\u001B[39m blk\u001B[38;5;241m.\u001B[39mget_values(dtype)\n\u001B[1;32m   1794\u001B[0m     result[rl\u001B[38;5;241m.\u001B[39mindexer] \u001B[38;5;241m=\u001B[39m arr\n\u001B[0;32m-> 1795\u001B[0m     itemmask[rl\u001B[38;5;241m.\u001B[39mindexer] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1797\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m itemmask\u001B[38;5;241m.\u001B[39mall():\n\u001B[1;32m   1798\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome items were not contained in blocks\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GNN Architecture",
   "id": "102eda0f19a9c8e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:11:48.755570Z",
     "start_time": "2025-03-26T00:11:48.715234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gnn architecture\n",
    "class DeepSetAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(DeepSetAggregator, self).__init__()\n",
    "\n",
    "        self.input = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.hidden1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.output = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, index):\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Input: {x.shape}\")\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Hidden1: {x.shape}\")\n",
    "        x = scatter(x, index, dim=0, reduce=\"mean\")\n",
    "        print(f\"scatter: {x.shape}\")\n",
    "        print(f\"index: {index}\")\n",
    "        self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Hidden2: {x.shape}\")\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResGnn(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, num_layers: int, hidden_channels: int, heads: int):\n",
    "        super(ResGnn, self).__init__()\n",
    "        assert num_layers > 0, \"num_layers must be > 0.\"\n",
    "\n",
    "        # Create Layers\n",
    "        self.convolutions = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convolutions.append(\n",
    "                GATv2Conv(-1, hidden_channels, heads=heads, edge_dim=1, add_self_loops=True, fill_value=0.01)\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels * heads, out_channels)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "        for i, conv in enumerate(self.convolutions):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "                x = self.relu(x)\n",
    "            else:\n",
    "                x = x + self.relu(conv(x, edge_index, edge_attr))  # Residual Layers\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention(\n",
    "        self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Runs a forward Pass for the given graph only though the ResGNN layer.\n",
    "        NOTE: the data that is given to this method must first pass through the layers before this layer in the Graph\n",
    "\n",
    "        :param torch.Tensor x: Tensor of Node Features (NxD)\n",
    "        :param torch.Tensor edge_index: Tensor of Edges (2xE)\n",
    "        :param torch.Tensor edge_attr: Edge Attributes (ExNum_Attr)\n",
    "        :return x, edge_index_attention, attention_weights: Tensor of Node Features (NxD), Tensor of Edges with\n",
    "        self loops (2xE), Tensor of Attention per edge (ExNum_Heads)\n",
    "        \"\"\"\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "\n",
    "        # Pass Data though Layer to get the Attention\n",
    "        attention_list = []\n",
    "        # Note: edge_index_attention has to be added since we have self loops now\n",
    "        edge_index_attention, attention_weights = None, None\n",
    "\n",
    "        for i, conv in enumerate(\n",
    "            self.convolutions,\n",
    "        ):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x, (edge_index_attention, attention_weights) = conv(\n",
    "                    x, edge_index, edge_attr, return_attention_weights=True\n",
    "                )\n",
    "                print(\"attention_weights:\")\n",
    "                print(attention_weights)\n",
    "                print(\"edge_index_attention\")\n",
    "                print(edge_index_attention)\n",
    "                print(f\"attention_weights.shape{attention_weights.shape}\")\n",
    "                print(f\"type(attention_weights){type(attention_weights)}\")\n",
    "                attention_list.append(attention_weights)\n",
    "                x = self.relu(x)\n",
    "                x = self.norm(x)\n",
    "            else:\n",
    "                x_conv, (edge_index_attention, attention_weights) = conv(\n",
    "                    x, edge_index, edge_attr, return_attention_weights=True\n",
    "                )\n",
    "                attention_list.append(attention_weights)\n",
    "                x = x + self.relu(x_conv)  # Residual Layers\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Attention weights of first layer\n",
    "        attention_weights = attention_weights.mean(dim=1)\n",
    "        print(\"attention_weights.mean(dim=1)\")\n",
    "        print(attention_weights)\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        return x, edge_index_attention, attention_weights, attention_list\n",
    "\n",
    "# gnn architecture\n",
    "class ThisMultigraph(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        in_channels,\n",
    "        hidden_channels_gnn,\n",
    "        out_channels_gnn,\n",
    "        num_layers_gnn,\n",
    "        heads,\n",
    "        hidden_channels_deepset,\n",
    "        optimizer_class,\n",
    "        optimizer_params,\n",
    "    ):\n",
    "        super(ThisMultigraph, self).__init__()\n",
    "\n",
    "        self.encoder = EmbedStations(num_stations_max=122, embedding_dim=embedding_dim)\n",
    "\n",
    "        self.conv = ResGnn(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels_gnn,\n",
    "            out_channels=out_channels_gnn,\n",
    "            num_layers=num_layers_gnn,\n",
    "            heads=heads,\n",
    "        )\n",
    "\n",
    "        self.aggr = DeepSetAggregator(\n",
    "            in_channels=out_channels_gnn, hidden_channels=hidden_channels_deepset, out_channels=2\n",
    "        )\n",
    "\n",
    "        self.postprocess = MakePositive()\n",
    "        self.loss_fn = NormalCRPS()\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch_id, node_idx = data.x, data.edge_index, data.edge_attr, data.batch, data.n_idx\n",
    "        node_idx = node_idx + batch_id * 122  # add batch_id to node_idx to get unique node indices\n",
    "        x = self.encoder(x)\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        x = self.aggr(x, node_idx)\n",
    "        x = self.postprocess(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=1\n",
    "        )  # The batch size is not actually 1 but the loss is already averaged over the batch\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        return loss\n",
    "\n",
    "    def initialize(self, dataloader):\n",
    "        batch = next(iter(dataloader))\n",
    "        self.validation_step(batch, 0)"
   ],
   "id": "4b0c09236490cd26",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check DeepSetAggregator and ResGNN outside of Multigraph",
   "id": "b25f56cbcc8e7dcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T23:49:36.263507Z",
     "start_time": "2025-03-25T23:48:43.281500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DeepSetAggregator\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"],\n",
    "    valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "    mat=dist,\n",
    "    max_dist=config['max_dist'],\n",
    ")\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "graphs_test = graphs_test_rf\n",
    "\n",
    "# print(graphs_test_rf[0].x.shape) (1342, 36)\n",
    "\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "train_loader = DataLoader(graphs_train_rf, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim=20\n",
    "in_channels = graphs_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "\n",
    "embedding_dim=emb_dim,\n",
    "in_channels=in_channels,\n",
    "hidden_channels_gnn=config['gnn_hidden'],\n",
    "out_channels_gnn=config['gnn_hidden'],\n",
    "num_layers_gnn=config['gnn_hidden'],\n",
    "heads=config['heads'],\n",
    "hidden_channels_deepset=config['gnn_hidden'],\n",
    "optimizer_class=AdamW,\n",
    "optimizer_params=dict(lr=config['lr']),\n",
    "\n",
    "ds = DeepSetAggregator(\n",
    "    in_channels=out_channels_gnn, hidden_channels=hidden_channels_deepset, out_channels=2\n",
    ")"
   ],
   "id": "eb207df0bf6978e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "[INFO] Creating graph data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# DeepSetAggregator\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m graphs_train_rf, tests \u001B[38;5;241m=\u001B[39m \u001B[43mnormalize_features_and_create_graphs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtraining_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataframes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalid_test_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mdataframes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest_rf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataframes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtest_f\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdist\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_dist\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmax_dist\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m graphs_test_rf, graphs_test_f \u001B[38;5;241m=\u001B[39m tests\n\u001B[1;32m     10\u001B[0m graphs_test \u001B[38;5;241m=\u001B[39m graphs_test_rf\n",
      "File \u001B[0;32m/tmp/pycharm_project_413/utils/data.py:686\u001B[0m, in \u001B[0;36mnormalize_features_and_create_graphs\u001B[0;34m(training_data, valid_test_data, mat, max_dist)\u001B[0m\n\u001B[1;32m    684\u001B[0m test_valid \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    685\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m features, targets \u001B[38;5;129;01min\u001B[39;00m valid_test_data:\n\u001B[0;32m--> 686\u001B[0m     graphs_valid_test \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_multigraph\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_target\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdistances\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_dist\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_dist\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    687\u001B[0m     test_valid\u001B[38;5;241m.\u001B[39mappend(graphs_valid_test)\n\u001B[1;32m    689\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m graphs_train_rf, test_valid\n",
      "File \u001B[0;32m/tmp/pycharm_project_413/utils/data.py:908\u001B[0m, in \u001B[0;36mcreate_multigraph\u001B[0;34m(df, df_target, distances, max_dist)\u001B[0m\n\u001B[1;32m    906\u001B[0m graphs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    907\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m time \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mtime\u001B[38;5;241m.\u001B[39munique():\n\u001B[0;32m--> 908\u001B[0m     day \u001B[38;5;241m=\u001B[39m df[\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtime\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtime\u001B[49m]  \u001B[38;5;66;03m# get all forecasts for one day\u001B[39;00m\n\u001B[1;32m    909\u001B[0m     day \u001B[38;5;241m=\u001B[39m day\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    910\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(day\u001B[38;5;241m.\u001B[39mto_numpy(dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mfloat32))\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/ops/common.py:81\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     79\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001B[0m, in \u001B[0;36mOpsMixin.__eq__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__eq__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__eq__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cmp_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meq\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/series.py:6096\u001B[0m, in \u001B[0;36mSeries._cmp_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m   6093\u001B[0m rvalues \u001B[38;5;241m=\u001B[39m extract_array(other, extract_numpy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, extract_range\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m   6095\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39merrstate(\u001B[38;5;28mall\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 6096\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcomparison_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6098\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_result(res_values, name\u001B[38;5;241m=\u001B[39mres_name)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:279\u001B[0m, in \u001B[0;36mcomparison_op\u001B[0;34m(left, right, op)\u001B[0m\n\u001B[1;32m    270\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    271\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLengths must match to compare\u001B[39m\u001B[38;5;124m\"\u001B[39m, lvalues\u001B[38;5;241m.\u001B[39mshape, rvalues\u001B[38;5;241m.\u001B[39mshape\n\u001B[1;32m    272\u001B[0m         )\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_extension_dispatch(lvalues, rvalues) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[1;32m    275\u001B[0m     (\u001B[38;5;28misinstance\u001B[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001B[38;5;129;01mor\u001B[39;00m right \u001B[38;5;129;01mis\u001B[39;00m NaT)\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_object_dtype(lvalues\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    277\u001B[0m ):\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;66;03m# Call the method on lvalues\u001B[39;00m\n\u001B[0;32m--> 279\u001B[0m     res_values \u001B[38;5;241m=\u001B[39m \u001B[43mop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrvalues\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_scalar(rvalues) \u001B[38;5;129;01mand\u001B[39;00m isna(rvalues):  \u001B[38;5;66;03m# TODO: but not pd.NA?\u001B[39;00m\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;66;03m# numpy does not like comparisons vs None\u001B[39;00m\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m op \u001B[38;5;129;01mis\u001B[39;00m operator\u001B[38;5;241m.\u001B[39mne:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/ops/common.py:81\u001B[0m, in \u001B[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     77\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m\n\u001B[1;32m     79\u001B[0m other \u001B[38;5;241m=\u001B[39m item_from_zerodim(other)\n\u001B[0;32m---> 81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/arraylike.py:40\u001B[0m, in \u001B[0;36mOpsMixin.__eq__\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;129m@unpack_zerodim_and_defer\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__eq__\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__eq__\u001B[39m(\u001B[38;5;28mself\u001B[39m, other):\n\u001B[0;32m---> 40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cmp_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moperator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meq\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pandas/core/arrays/datetimelike.py:981\u001B[0m, in \u001B[0;36mDatetimeLikeArrayMixin._cmp_method\u001B[0;34m(self, other, op)\u001B[0m\n\u001B[1;32m    979\u001B[0m o_mask \u001B[38;5;241m=\u001B[39m isna(other)\n\u001B[1;32m    980\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_isnan \u001B[38;5;241m|\u001B[39m o_mask\n\u001B[0;32m--> 981\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mmask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    982\u001B[0m     nat_result \u001B[38;5;241m=\u001B[39m op \u001B[38;5;129;01mis\u001B[39;00m operator\u001B[38;5;241m.\u001B[39mne\n\u001B[1;32m    983\u001B[0m     np\u001B[38;5;241m.\u001B[39mputmask(result, mask, nat_result)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/numpy/core/_methods.py:55\u001B[0m, in \u001B[0;36m_any\u001B[0;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_prod\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     52\u001B[0m           initial\u001B[38;5;241m=\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m umr_prod(a, axis, dtype, out, keepdims, initial, where)\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_any\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m# Parsing keyword arguments is currently fairly slow, so avoid it for now\u001B[39;00m\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m where \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     58\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m umr_any(a, axis, dtype, out, keepdims)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train GNN",
   "id": "d2a71ee480a1bedb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T00:13:04.896031Z",
     "start_time": "2025-03-26T00:11:54.574268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train gnn\n",
    "# build a graph with wandb => create multigraph - without summmary_statistics and no edges removed\n",
    "with wandb.init(\n",
    "    project=\"multigraph\", id=f\"training_run_24h\", config=args_dict, tags=[\"final_training\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "    #print(\"[INFO] Starting sweep with config: \", config)\n",
    "\n",
    "    graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "        training_data=dataframes[\"train\"],\n",
    "        valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "        mat=dist,\n",
    "        max_dist=config['max_dist'],\n",
    "    )\n",
    "    graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "    graphs_test = graphs_test_rf\n",
    "\n",
    "    # print(graphs_test_rf[0].x.shape) (1342, 36)\n",
    "\n",
    "\n",
    "    print(\"[INFO] Creating data loaders...\")\n",
    "    train_loader = DataLoader(graphs_train_rf, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    print(\"[INFO] Creating model...\")\n",
    "    emb_dim=20\n",
    "    in_channels = graphs_train_rf[0].x.shape[1] + emb_dim - 1 #(36 + 20 - 1) = 55\n",
    "\n",
    "    multigraph = ThisMultigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config['gnn_hidden'],\n",
    "        out_channels_gnn=config['gnn_hidden'],\n",
    "        num_layers_gnn=config['gnn_layers'],\n",
    "        heads=config['heads'],\n",
    "        hidden_channels_deepset=config['gnn_hidden'],\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config['lr']),\n",
    "    )\n",
    "    torch.compile(multigraph)\n",
    "\n",
    "    # understand what this is\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = batch  # .to(\"cuda\")\n",
    "    multigraph  # .to(\"cuda\")\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"multigraph\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config['max_epochs'],\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "38ba57bc26ec34e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mleachen\u001B[0m (\u001B[33mleachen_thesis\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250326_011155-training_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">training_run_24h</a></strong> to <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "[INFO] Creating graph data...\n",
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n",
      "Input: torch.Size([10736, 265])\n",
      "Hidden1: torch.Size([10736, 265])\n",
      "scatter: torch.Size([976, 265])\n",
      "index: tensor([  0,   1,   2,  ..., 973, 974, 975])\n",
      "Hidden2: torch.Size([976, 265])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name        | Type              | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | encoder     | EmbedStations     | 2.4 K  | train\n",
      "1 | conv        | ResGnn            | 9.8 M  | train\n",
      "2 | aggr        | DeepSetAggregator | 212 K  | train\n",
      "3 | postprocess | MakePositive      | 0      | train\n",
      "4 | loss_fn     | NormalCRPS        | 0      | train\n",
      "----------------------------------------------------------\n",
      "10.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.0 M    Total params\n",
      "40.079    Total estimated model params size (MB)\n",
      "24        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training model...\n",
      "Epoch 0:   0%|          | 0/431 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3183839/4149401971.py\", line 64, in <module>\n",
      "    trainer.fit(model=multigraph, train_dataloaders=train_loader)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 250, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 190, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 268, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 167, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/core/module.py\", line 1306, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py\", line 153, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 238, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 122, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/optim/adamw.py\", line 204, in step\n",
      "    loss = closure()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 108, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 144, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 129, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 317, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 390, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3183839/3248446311.py\", line 158, in training_step\n",
      "    y_hat = self.forward(batch)\n",
      "  File \"/tmp/ipykernel_3183839/3248446311.py\", line 152, in forward\n",
      "    x = self.conv(x, edge_index, edge_attr)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3183839/3248446311.py\", line 49, in forward\n",
      "    x = conv(x, edge_index, edge_attr)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/gatv2_conv.py\", line 325, in forward\n",
      "    alpha = self.edge_updater(edge_index, x=(x_l, x_r),\n",
      "  File \"/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_518bb09a.py\", line 145, in edge_updater\n",
      "    kwargs = self.edge_collect(\n",
      "  File \"/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_518bb09a.py\", line 91, in edge_collect\n",
      "    x_j = self._index_select(_x_0, edge_index_j)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py\", line 267, in _index_select\n",
      "    return self._index_select_safe(src, index)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py\", line 290, in _index_select_safe\n",
      "    raise e\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py\", line 271, in _index_select_safe\n",
      "    return src.index_select(self.node_dim, index)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.92 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.00 GiB is free. Process 3119012 has 22.17 GiB memory in use. Including non-PyTorch memory, this process has 504.00 MiB memory in use. Of the allocated memory 234.84 MiB is allocated by PyTorch, and 23.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_24h</strong> at: <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250326_011155-training_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.92 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.00 GiB is free. Process 3119012 has 22.17 GiB memory in use. Including non-PyTorch memory, this process has 504.00 MiB memory in use. Of the allocated memory 234.84 MiB is allocated by PyTorch, and 23.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 64\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[INFO] Training model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     54\u001B[0m trainer \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[1;32m     55\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_epochs\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m     56\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     61\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mcheckpoint_callback,\n\u001B[1;32m     62\u001B[0m )\n\u001B[0;32m---> 64\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmultigraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    570\u001B[0m     ckpt_path,\n\u001B[1;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m )\n\u001B[0;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1025\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m    249\u001B[0m         \u001B[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001B[39;00m\n\u001B[0;32m--> 250\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautomatic_optimization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    252\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_optimization\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001B[0m, in \u001B[0;36m_AutomaticOptimization.run\u001B[0;34m(self, optimizer, batch_idx, kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m         closure()\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001B[0m, in \u001B[0;36m_AutomaticOptimization._optimizer_step\u001B[0;34m(self, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[1;32m    267\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:167\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 167\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    170\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1306\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001B[0m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1277\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     optimizer_closure: Optional[Callable[[], Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1281\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;124;03m    the optimizer.\u001B[39;00m\n\u001B[1;32m   1284\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1304\u001B[0m \n\u001B[1;32m   1305\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1306\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:153\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:238\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001B[39;00m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001B[0m, in \u001B[0;36mPrecision.optimizer_step\u001B[0;34m(self, optimizer, model, closure, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001B[39;00m\n\u001B[1;32m    121\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, closure)\n\u001B[0;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    480\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    481\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    482\u001B[0m             )\n\u001B[0;32m--> 484\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    487\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     88\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 89\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     91\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/optim/adamw.py:204\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    203\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 204\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    207\u001B[0m     params_with_grad: List[Tensor] \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001B[0m, in \u001B[0;36mPrecision._wrap_closure\u001B[0;34m(self, model, optimizer, closure)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     97\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     98\u001B[0m     optimizer: Steppable,\n\u001B[1;32m     99\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m    100\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    101\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    hook is called.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    106\u001B[0m \n\u001B[1;32m    107\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer)\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad()\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 129\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001B[0m, in \u001B[0;36m_AutomaticOptimization._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001B[39;00m\n\u001B[1;32m    307\u001B[0m \n\u001B[1;32m    308\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    313\u001B[0m \n\u001B[1;32m    314\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    315\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m--> 317\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()  \u001B[38;5;66;03m# unused hook - call anyway for backward compatibility\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training_step_output \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mworld_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 158\u001B[0m, in \u001B[0;36mThisMultigraph.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, batch_idx):\n\u001B[0;32m--> 158\u001B[0m     y_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    159\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_fn\u001B[38;5;241m.\u001B[39mcrps(mu_sigma\u001B[38;5;241m=\u001B[39my_hat, y\u001B[38;5;241m=\u001B[39mbatch\u001B[38;5;241m.\u001B[39my)\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m, loss, on_step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, prog_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    162\u001B[0m     )  \u001B[38;5;66;03m# The batch size is not actually 1 but the loss is already averaged over the batch\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[5], line 152\u001B[0m, in \u001B[0;36mThisMultigraph.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    150\u001B[0m node_idx \u001B[38;5;241m=\u001B[39m node_idx \u001B[38;5;241m+\u001B[39m batch_id \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m122\u001B[39m  \u001B[38;5;66;03m# add batch_id to node_idx to get unique node indices\u001B[39;00m\n\u001B[1;32m    151\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x)\n\u001B[0;32m--> 152\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    153\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggr(x, node_idx)\n\u001B[1;32m    154\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(x)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[5], line 49\u001B[0m, in \u001B[0;36mResGnn.forward\u001B[0;34m(self, x, edge_index, edge_attr)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, conv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvolutions):\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     48\u001B[0m         \u001B[38;5;66;03m# First Layer\u001B[39;00m\n\u001B[0;32m---> 49\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/gatv2_conv.py:325\u001B[0m, in \u001B[0;36mGATv2Conv.forward\u001B[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001B[0m\n\u001B[1;32m    319\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    320\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe usage of \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medge_attr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madd_self_loops\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    321\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msimultaneously is currently not yet supported for \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    322\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medge_index\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in a \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparseTensor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m form\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    324\u001B[0m \u001B[38;5;66;03m# edge_updater_type: (x: PairTensor, edge_attr: OptTensor)\u001B[39;00m\n\u001B[0;32m--> 325\u001B[0m alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_updater\u001B[49m\u001B[43m(\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mx_l\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_r\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m                          \u001B[49m\u001B[43medge_attr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: PairTensor, alpha: Tensor)\u001B[39;00m\n\u001B[1;32m    329\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpropagate(edge_index, x\u001B[38;5;241m=\u001B[39m(x_l, x_r), alpha\u001B[38;5;241m=\u001B[39malpha)\n",
      "File \u001B[0;32m/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_518bb09a.py:145\u001B[0m, in \u001B[0;36medge_updater\u001B[0;34m(self, edge_index, x, edge_attr, size)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21medge_updater\u001B[39m(\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    137\u001B[0m     edge_index: Union[Tensor, SparseTensor],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m     size: Size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    141\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    143\u001B[0m     mutable_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_input(edge_index, size)\n\u001B[0;32m--> 145\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_collect\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[43m        \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43medge_attr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmutable_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;66;03m# Begin Edge Update Forward Pre Hook #######################################\u001B[39;00m\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_compiling():\n",
      "File \u001B[0;32m/tmp/torch_geometric.nn.conv.gatv2_conv_GATv2Conv_edge_updater_518bb09a.py:91\u001B[0m, in \u001B[0;36medge_collect\u001B[0;34m(self, edge_index, x, edge_attr, size)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(_x_0, Tensor):\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_size(size, \u001B[38;5;241m0\u001B[39m, _x_0)\n\u001B[0;32m---> 91\u001B[0m     x_j \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_index_select\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_x_0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index_j\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     93\u001B[0m     x_j \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:267\u001B[0m, in \u001B[0;36mMessagePassing._index_select\u001B[0;34m(self, src, index)\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m src\u001B[38;5;241m.\u001B[39mindex_select(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim, index)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_index_select_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:290\u001B[0m, in \u001B[0;36mMessagePassing._index_select_safe\u001B[0;34m(self, src, index)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (index\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m index\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim)):\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound indices in \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medge_index\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m that are larger \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthan \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msrc\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min the interval [0, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msrc\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour node feature matrix and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 290\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env3/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:271\u001B[0m, in \u001B[0;36mMessagePassing._index_select_safe\u001B[0;34m(self, src, index)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_index_select_safe\u001B[39m(\u001B[38;5;28mself\u001B[39m, src: Tensor, index: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mIndexError\u001B[39;00m, \u001B[38;5;167;01mRuntimeError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m index\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m index\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 1.92 GiB. GPU 0 has a total capacity of 23.68 GiB of which 1.00 GiB is free. Process 3119012 has 22.17 GiB memory in use. Including non-PyTorch memory, this process has 504.00 MiB memory in use. Of the allocated memory 234.84 MiB is allocated by PyTorch, and 23.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# evaluate gnn\n",
   "id": "649e4aadb19512f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:00:22.392550Z",
     "start_time": "2025-03-11T03:00:22.105868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "3315937a70662a07",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
