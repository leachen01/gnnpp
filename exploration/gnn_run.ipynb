{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:23:37.544073Z",
     "start_time": "2025-03-11T02:23:37.528271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /home/ltchen/gnnpp\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch_geometric\n",
    "import json\n",
    "import wandb\n",
    "\n",
    "from typing import Tuple\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import scatter\n",
    "from torch.nn import Linear, ModuleList, ReLU\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from models.loss import NormalCRPS\n",
    "from models.model_utils import MakePositive, EmbedStations\n",
    "from utils.data import (\n",
    "    load_dataframes,\n",
    "    load_distances,\n",
    "    normalize_features_and_create_graphs,\n",
    "    rm_edges,\n",
    "    summary_statistics,\n",
    ")"
   ],
   "id": "7a72cc47a3f90c32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:13:22.804452Z",
     "start_time": "2025-03-11T02:13:22.798221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DIRECTORY = os.getcwd()\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"explored_models/gnn_24h/models\")\n",
    "JSONPATH = os.path.join(DIRECTORY, \"trained_models/best_24h/params.json\")"
   ],
   "id": "f9b587b1ca8953b6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:13:24.494818Z",
     "start_time": "2025-03-11T02:13:24.484059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "config = args_dict\n",
    "\n",
    "'''{\"batch_size\":8,\n",
    "\"gnn_hidden\":265,\n",
    "\"gnn_layers\":2,\n",
    "\"heads\":8,\n",
    "\"lr\":0.0002,\n",
    "\"max_dist\":100,\n",
    "\"max_epochs\": 31}'''"
   ],
   "id": "a21cd6960e3d25fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading /home/ltchen/gnnpp/trained_models/best_24h/params.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"batch_size\":8,\\n\"gnn_hidden\":265,\\n\"gnn_layers\":2,\\n\"heads\":8,\\n\"lr\":0.0002,\\n\"max_dist\":100,\\n\"max_epochs\": 31}'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:13:32.913237Z",
     "start_time": "2025-03-11T02:13:27.397549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load graph only for rf\n",
    "dataframes = load_dataframes(mode=\"eval\", leadtime= \"24h\")\n",
    "dist = load_distances(dataframes[\"stations\"])\n"
   ],
   "id": "c9e52c29f737905a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Computing distances...\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:14:10.147122Z",
     "start_time": "2025-03-11T02:14:10.140745Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"dist.shape: {dist.shape}\")",
   "id": "3fcc41fb67d441c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist.shape: (122, 122)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# just for review to understand the difference between gnn and drn\n",
    "\n",
    "dataframes = load_dataframes(mode=\"train\", leadtime=\"24h\")\n",
    "dist = load_distances(dataframes[\"stations\"])\n",
    "\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"],\n",
    "    valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "    mat=dist,\n",
    "    max_dist=config.max_dist,\n",
    ")\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "# wo ist alles was hinter diesem Code kommt : pop stations, reset_index, dropnans in normalize_features_and_create_graphs\n",
    "dataframes.pop(\"stations\") # .pop(\"stations\") => entfernt den df mit stations, wofuer brauche ich die dann Ã¼berhaupt? Grafik?\n",
    "\n",
    "# test\n",
    "for X, y in dataframes.values(): # wofuer?\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "\n",
    "##\n",
    "train, valid_test = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"], valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]]\n",
    ")\n",
    "\n",
    "train = drop_nans(train)\n",
    "(test_rf, test_f) = valid_test\n",
    "test_rf = drop_nans(test_rf)\n",
    "test_f = drop_nans(test_f)\n",
    "\n",
    "DIRECTORY = os.getcwd()\n",
    "SAVEPATH = os.path.join(DIRECTORY, \"explored_models/drn_24h/models\")"
   ],
   "id": "de0f8f7cd13ca828"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GNN Architecture",
   "id": "102eda0f19a9c8e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:14:18.183221Z",
     "start_time": "2025-03-11T02:14:18.150828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# gnn architecture\n",
    "class DeepSetAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(DeepSetAggregator, self).__init__()\n",
    "\n",
    "        self.input = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.hidden1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.output = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, index):\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Input: {x.shape}\")\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Hidden1: {x.shape}\")\n",
    "        x = scatter(x, index, dim=0, reduce=\"mean\")\n",
    "        print(f\"scatter: {x.shape}\")\n",
    "        print(f\"index: {index}\")\n",
    "        self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Hidden2: {x.shape}\")\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResGnn(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, num_layers: int, hidden_channels: int, heads: int):\n",
    "        super(ResGnn, self).__init__()\n",
    "        assert num_layers > 0, \"num_layers must be > 0.\"\n",
    "\n",
    "        # Create Layers\n",
    "        self.convolutions = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convolutions.append(\n",
    "                GATv2Conv(-1, hidden_channels, heads=heads, edge_dim=1, add_self_loops=True, fill_value=0.01)\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels * heads, out_channels)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "        for i, conv in enumerate(self.convolutions):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "                x = self.relu(x)\n",
    "            else:\n",
    "                x = x + self.relu(conv(x, edge_index, edge_attr))  # Residual Layers\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention(\n",
    "        self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Runs a forward Pass for the given graph only though the ResGNN layer.\n",
    "        NOTE: the data that is given to this method must first pass through the layers before this layer in the Graph\n",
    "\n",
    "        :param torch.Tensor x: Tensor of Node Features (NxD)\n",
    "        :param torch.Tensor edge_index: Tensor of Edges (2xE)\n",
    "        :param torch.Tensor edge_attr: Edge Attributes (ExNum_Attr)\n",
    "        :return x, edge_index_attention, attention_weights: Tensor of Node Features (NxD), Tensor of Edges with\n",
    "        self loops (2xE), Tensor of Attention per edge (ExNum_Heads)\n",
    "        \"\"\"\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "\n",
    "        # Pass Data though Layer to get the Attention\n",
    "        attention_list = []\n",
    "        # Note: edge_index_attention has to be added since we have self loops now\n",
    "        edge_index_attention, attention_weights = None, None\n",
    "\n",
    "        for i, conv in enumerate(\n",
    "            self.convolutions,\n",
    "        ):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x, (edge_index_attention, attention_weights) = conv(\n",
    "                    x, edge_index, edge_attr, return_attention_weights=True\n",
    "                )\n",
    "                print(\"attention_weights:\")\n",
    "                print(attention_weights)\n",
    "                print(\"edge_index_attention\")\n",
    "                print(edge_index_attention)\n",
    "                print(f\"attention_weights.shape{attention_weights.shape}\")\n",
    "                print(f\"type(attention_weights){type(attention_weights)}\")\n",
    "                attention_list.append(attention_weights)\n",
    "                x = self.relu(x)\n",
    "                x = self.norm(x)\n",
    "            else:\n",
    "                x_conv, (edge_index_attention, attention_weights) = conv(\n",
    "                    x, edge_index, edge_attr, return_attention_weights=True\n",
    "                )\n",
    "                attention_list.append(attention_weights)\n",
    "                x = x + self.relu(x_conv)  # Residual Layers\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Attention weights of first layer\n",
    "        attention_weights = attention_weights.mean(dim=1)\n",
    "        print(\"attention_weights.mean(dim=1)\")\n",
    "        print(attention_weights)\n",
    "        print(attention_weights.shape)\n",
    "\n",
    "        return x, edge_index_attention, attention_weights, attention_list\n",
    "\n",
    "# gnn architecture\n",
    "class ThisMultigraph(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        in_channels,\n",
    "        hidden_channels_gnn,\n",
    "        out_channels_gnn,\n",
    "        num_layers_gnn,\n",
    "        heads,\n",
    "        hidden_channels_deepset,\n",
    "        optimizer_class,\n",
    "        optimizer_params,\n",
    "    ):\n",
    "        super(ThisMultigraph, self).__init__()\n",
    "\n",
    "        self.encoder = EmbedStations(num_stations_max=122, embedding_dim=embedding_dim)\n",
    "\n",
    "        self.conv = ResGnn(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels_gnn,\n",
    "            out_channels=out_channels_gnn,\n",
    "            num_layers=num_layers_gnn,\n",
    "            heads=heads,\n",
    "        )\n",
    "\n",
    "        self.aggr = DeepSetAggregator(\n",
    "            in_channels=out_channels_gnn, hidden_channels=hidden_channels_deepset, out_channels=2\n",
    "        )\n",
    "\n",
    "        self.postprocess = MakePositive()\n",
    "        self.loss_fn = NormalCRPS()\n",
    "\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch_id, node_idx = data.x, data.edge_index, data.edge_attr, data.batch, data.n_idx\n",
    "        node_idx = node_idx + batch_id * 122  # add batch_id to node_idx to get unique node indices\n",
    "        x = self.encoder(x)\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        x = self.aggr(x, node_idx)\n",
    "        x = self.postprocess(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=1\n",
    "        )  # The batch size is not actually 1 but the loss is already averaged over the batch\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        return loss\n",
    "\n",
    "    def initialize(self, dataloader):\n",
    "        batch = next(iter(dataloader))\n",
    "        self.validation_step(batch, 0)"
   ],
   "id": "4b0c09236490cd26",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Check DeepSetAggregator and ResGNN outside of Multigraph",
   "id": "b25f56cbcc8e7dcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:20:36.344773Z",
     "start_time": "2025-03-11T02:19:32.123535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DeepSetAggregator\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"],\n",
    "    valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "    mat=dist,\n",
    "    max_dist=config.max_dist,\n",
    ")\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "graphs_test = graphs_test_rf\n",
    "\n",
    "# print(graphs_test_rf[0].x.shape) (1342, 36)\n",
    "\n",
    "\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "train_loader = DataLoader(graphs_train_rf, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "print(\"[INFO] Creating model...\")\n",
    "emb_dim=20\n",
    "in_channels = graphs_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "\n",
    "embedding_dim=emb_dim,\n",
    "in_channels=in_channels,\n",
    "hidden_channels_gnn=config.gnn_hidden,\n",
    "out_channels_gnn=config.gnn_hidden,\n",
    "num_layers_gnn=config.gnn_layers,\n",
    "heads=config.heads,\n",
    "hidden_channels_deepset=config.gnn_hidden,\n",
    "optimizer_class=AdamW,\n",
    "optimizer_params=dict(lr=config.lr),\n",
    "\n",
    "ds = DeepSetAggregator(\n",
    "    in_channels=out_channels_gnn, hidden_channels=hidden_channels_deepset, out_channels=2\n",
    ")"
   ],
   "id": "eb207df0bf6978e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "[INFO] Creating graph data...\n",
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 32\u001B[0m\n\u001B[1;32m     29\u001B[0m optimizer_class\u001B[38;5;241m=\u001B[39mAdamW,\n\u001B[1;32m     30\u001B[0m optimizer_params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mdict\u001B[39m(lr\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mlr),\n\u001B[0;32m---> 32\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mDeepSetAggregator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43min_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout_channels_gnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_channels_deepset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\n\u001B[1;32m     34\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[13], line 8\u001B[0m, in \u001B[0;36mDeepSetAggregator.__init__\u001B[0;34m(self, in_channels, hidden_channels, out_channels)\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, in_channels, hidden_channels, out_channels):\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28msuper\u001B[39m(DeepSetAggregator, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43min_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_channels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mLinear(hidden_channels, hidden_channels)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhidden2 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mLinear(hidden_channels, hidden_channels)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/nn/modules/linear.py:99\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_features \u001B[38;5;241m=\u001B[39m in_features\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_features \u001B[38;5;241m=\u001B[39m out_features\n\u001B[0;32m---> 99\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m Parameter(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_features\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfactory_kwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bias:\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m Parameter(torch\u001B[38;5;241m.\u001B[39mempty(out_features, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfactory_kwargs))\n",
      "\u001B[0;31mTypeError\u001B[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train GNN",
   "id": "d2a71ee480a1bedb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:15:33.515025Z",
     "start_time": "2025-03-11T02:14:24.996260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train gnn\n",
    "# build a graph with wandb => create multigraph - without summmary_statistics and no edges removed\n",
    "with wandb.init(\n",
    "    project=\"multigraph\", id=f\"training_run_24h\", config=args_dict, tags=[\"final_training\"]\n",
    "):\n",
    "    config = wandb.config\n",
    "    #print(\"[INFO] Starting sweep with config: \", config)\n",
    "\n",
    "    graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "        training_data=dataframes[\"train\"],\n",
    "        valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "        mat=dist,\n",
    "        max_dist=config.max_dist,\n",
    "    )\n",
    "    graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "    graphs_test = graphs_test_rf\n",
    "\n",
    "    # print(graphs_test_rf[0].x.shape) (1342, 36)\n",
    "\n",
    "\n",
    "    print(\"[INFO] Creating data loaders...\")\n",
    "    train_loader = DataLoader(graphs_train_rf, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    print(\"[INFO] Creating model...\")\n",
    "    emb_dim=20\n",
    "    in_channels = graphs_train_rf[0].x.shape[1] + emb_dim - 1 #(36 + 20 - 1) = 55\n",
    "\n",
    "    multigraph = ThisMultigraph(\n",
    "        embedding_dim=emb_dim,\n",
    "        in_channels=in_channels,\n",
    "        hidden_channels_gnn=config.gnn_hidden,\n",
    "        out_channels_gnn=config.gnn_hidden,\n",
    "        num_layers_gnn=config.gnn_layers,\n",
    "        heads=config.heads,\n",
    "        hidden_channels_deepset=config.gnn_hidden,\n",
    "        optimizer_class=AdamW,\n",
    "        optimizer_params=dict(lr=config.lr),\n",
    "    )\n",
    "    torch_geometric.compile(multigraph)\n",
    "\n",
    "    # understand what this is\n",
    "    batch = next(iter(train_loader))\n",
    "    batch = batch  # .to(\"cuda\")\n",
    "    multigraph  # .to(\"cuda\")\n",
    "    multigraph.forward(batch)\n",
    "\n",
    "    wandb_logger = WandbLogger(project=\"multigraph\")\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=SAVEPATH, filename=f\"run_24h\", monitor=\"train_loss\", mode=\"min\", save_top_k=1\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Training model...\")\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config.max_epochs,\n",
    "        log_every_n_steps=1,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        enable_progress_bar=True,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=checkpoint_callback,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model=multigraph, train_dataloaders=train_loader)"
   ],
   "id": "38ba57bc26ec34e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250311_031425-training_run_24h</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">training_run_24h</a></strong> to <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "[INFO] Creating graph data...\n",
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch_geometric/_compile.py:34: UserWarning: 'torch_geometric.compile' is deprecated in favor of 'torch.compile'\n",
      "  warnings.warn(\"'torch_geometric.compile' is deprecated in favor of \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3387260/3049178394.py\", line 43, in <module>\n",
      "    batch = next(iter(train_loader))\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 317, in default_collate\n",
      "    return collate(batch, collate_fn_map=default_collate_fn_map)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py\", line 192, in collate\n",
      "    raise TypeError(default_collate_err_msg_format.format(elem_type))\n",
      "TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training_run_24h</strong> at: <a href='https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph/runs/training_run_24h</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/multigraph' target=\"_blank\">https://wandb.ai/leachen_thesis/multigraph</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250311_031425-training_run_24h/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 43\u001B[0m\n\u001B[1;32m     40\u001B[0m torch_geometric\u001B[38;5;241m.\u001B[39mcompile(multigraph)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# understand what this is\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m batch \u001B[38;5;241m=\u001B[39m batch  \u001B[38;5;66;03m# .to(\"cuda\")\u001B[39;00m\n\u001B[1;32m     45\u001B[0m multigraph  \u001B[38;5;66;03m# .to(\"cuda\")\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:55\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[0;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:317\u001B[0m, in \u001B[0;36mdefault_collate\u001B[0;34m(batch)\u001B[0m\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[1;32m    257\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 317\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:192\u001B[0m, in \u001B[0;36mcollate\u001B[0;34m(batch, collate_fn_map)\u001B[0m\n\u001B[1;32m    187\u001B[0m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    188\u001B[0m             \u001B[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001B[39;00m\n\u001B[1;32m    189\u001B[0m             \u001B[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001B[39;00m\n\u001B[1;32m    190\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m [collate(samples, collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map) \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed]\n\u001B[0;32m--> 192\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem_type))\n",
      "\u001B[0;31mTypeError\u001B[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# evaluate gnn\n",
   "id": "649e4aadb19512f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T03:00:22.392550Z",
     "start_time": "2025-03-11T03:00:22.105868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "3315937a70662a07",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
