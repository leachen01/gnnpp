{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GNN Explore\n",
    "- exploring SimpleConv, GAT, DeepSetAggregator"
   ],
   "id": "108ffe08228478eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:44:07.024675Z",
     "start_time": "2025-04-04T14:44:07.004265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%cd /home/ltchen/gnnpp\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as L\n",
    "import wandb\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# PyTorch geometric\n",
    "import torch_geometric\n",
    "import torch_geometric.data as geom_data\n",
    "import torch_geometric.nn as geom_nn\n",
    "\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv\n",
    "from torch_geometric.nn.aggr import MeanAggregation\n",
    "from torch_geometric.utils import scatter\n",
    "from torch.nn import Linear, ModuleList, ReLU\n",
    "from torch_geometric.loader import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.optim import AdamW\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from models.loss import NormalCRPS\n",
    "from models.model_utils import MakePositive, EmbedStations\n",
    "from utils.data import *\n",
    "from torch_geometric.utils import to_networkx\n",
    "from utils.data import *\n",
    "import matplotlib as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from utils.plot import plot_map\n",
    "import networkx as nx"
   ],
   "id": "ad2c4ad78d6b3a09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:36:59.774892Z",
     "start_time": "2025-04-04T14:36:54.746613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframes = load_dataframes(mode=\"train\", leadtime=\"24h\")\n",
    "dist = load_distances(dataframes[\"stations\"])\n",
    "\n",
    "dataframes = summary_statistics(dataframes)\n",
    "train = dataframes[\"train\"][0]\n",
    "train_target = dataframes[\"train\"][1]\n",
    "test_rf = dataframes[\"test_rf\"][0]\n",
    "test_rf_target = dataframes[\"test_rf\"][1]\n",
    "test_f = dataframes[\"test_f\"][0]\n",
    "test_f_target = dataframes[\"test_f\"][1]"
   ],
   "id": "14ba43e84981f279",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Loading distances from file...\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T15:34:52.124051Z",
     "start_time": "2025-04-04T15:34:52.107657Z"
    }
   },
   "cell_type": "code",
   "source": "train_target",
   "id": "8480a4e270f84b66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             time  station_id     t2m\n",
       "0      1997-01-02           0  277.75\n",
       "1      1997-01-02           1  279.55\n",
       "2      1997-01-02           2  276.45\n",
       "3      1997-01-02           3  275.75\n",
       "4      1997-01-02           4  279.35\n",
       "...           ...         ...     ...\n",
       "420651 2013-12-31         117  281.35\n",
       "420652 2013-12-31         118  279.35\n",
       "420653 2013-12-31         119  278.25\n",
       "420654 2013-12-31         120  273.15\n",
       "420655 2013-12-31         121  272.65\n",
       "\n",
       "[420656 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>station_id</th>\n",
       "      <th>t2m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>0</td>\n",
       "      <td>277.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>279.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>276.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>275.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>279.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420651</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>117</td>\n",
       "      <td>281.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420652</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>118</td>\n",
       "      <td>279.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420653</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>119</td>\n",
       "      <td>278.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420654</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>120</td>\n",
       "      <td>273.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420655</th>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>121</td>\n",
       "      <td>272.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420656 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:36:52.327194Z",
     "start_time": "2025-04-04T14:36:52.261138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nur ein ensemble mitglied (if we don't use summary statistics)\n",
    "train = dataframes[\"train\"][0][dataframes['train'][0]['number'] == 0]\n",
    "train_target = dataframes[\"train\"][1]\n",
    "test_rf = dataframes[\"test_rf\"][0][dataframes['test_rf'][0]['number'] == 0]\n",
    "test_rf_target = dataframes[\"test_rf\"][1]\n",
    "test_f = dataframes[\"test_f\"][0][dataframes['test_f'][0]['number'] == 0]\n",
    "test_f_target = dataframes[\"test_f\"][1]\n",
    "test_f_target"
   ],
   "id": "68aa6b3bc8930e06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            time  station_id     t2m\n",
       "0     2017-01-01           0  278.65\n",
       "1     2017-01-01           1  275.25\n",
       "2     2017-01-01           2  279.75\n",
       "3     2017-01-01           3  279.15\n",
       "4     2017-01-01           4  275.05\n",
       "...          ...         ...     ...\n",
       "89055 2018-12-31         117  277.95\n",
       "89056 2018-12-31         118  276.85\n",
       "89057 2018-12-31         119  276.35\n",
       "89058 2018-12-31         120  270.65\n",
       "89059 2018-12-31         121  268.85\n",
       "\n",
       "[89060 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>station_id</th>\n",
       "      <th>t2m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>278.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>275.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>2</td>\n",
       "      <td>279.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>279.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4</td>\n",
       "      <td>275.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89055</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>117</td>\n",
       "      <td>277.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89056</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>118</td>\n",
       "      <td>276.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89057</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>119</td>\n",
       "      <td>276.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89058</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>120</td>\n",
       "      <td>270.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89059</th>\n",
       "      <td>2018-12-31</td>\n",
       "      <td>121</td>\n",
       "      <td>268.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89060 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:37:19.951221Z",
     "start_time": "2025-04-04T14:37:01.724879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_dist = 100\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "    training_data=(train, train_target),\n",
    "    valid_test_data=[(test_rf, test_rf_target), (test_f, test_f_target)],\n",
    "    mat=dist,\n",
    "    max_dist=max_dist,\n",
    ")\n",
    "\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "graphs_test = graphs_test_rf"
   ],
   "id": "7ce73b0f1bdef70e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Normalizing features...\n",
      "[INFO] Creating graph data...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:37:19.966675Z",
     "start_time": "2025-04-04T14:37:19.960560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print(graphs_train_rf) #(1342, 36)\n",
    "print(next(iter(graphs_train_rf)))\n",
    "print(len(graphs_train_rf))"
   ],
   "id": "ecc217d99424eea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[122, 65], edge_index=[2, 1420], edge_attr=[1420, 1], y=[122], timestamp=1997-01-02 00:00:00, n_idx=[122])\n",
      "3448\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T02:05:27.970570Z",
     "start_time": "2025-03-31T02:05:27.964290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(graphs_train_rf[0].x.shape)\n",
    "print(graphs_train_rf[0].y.shape)\n",
    "#print(graphs_train_rf[0].x)\n",
    "\n",
    "# drop nans in target? => gradienten können dann nciht berechnet werden\n",
    "\n",
    "# standardize data correctly? (should be standardized using normalize_features_and_create_graphs...\n",
    "# target size: [975] vs. [975, 1]"
   ],
   "id": "7a576f89cbb7a5da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122, 36])\n",
      "torch.Size([122])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:37:20.019550Z",
     "start_time": "2025-04-04T14:37:20.013464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(graphs_train_rf, batch_size=batch_size, shuffle=True)\n",
    "test_f_loader = DataLoader(graphs_test_f, batch_size=batch_size, shuffle=False)"
   ],
   "id": "83ac1b4dc6fb3728",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Simple GNN with Conv",
   "id": "32c64ad81ed9be78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T15:49:05.615192Z",
     "start_time": "2025-04-04T15:49:05.593425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DeepSetAggregator(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(DeepSetAggregator, self).__init__()\n",
    "\n",
    "        self.input = torch.nn.Linear(in_channels, hidden_channels)\n",
    "        self.hidden1 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.hidden2 = torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.output = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x, index):\n",
    "        x = self.input(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Input: {x.shape}\")\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Hidden1: {x.shape}\")\n",
    "        x = scatter(x, index, dim=0, reduce=\"mean\")\n",
    "        print(f\"scatter: {x.shape}\")\n",
    "        print(f\"index: {index}\")\n",
    "        self.hidden2(x)\n",
    "        x = self.relu(x)\n",
    "        print(f\"Hidden2: {x.shape}\")\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "class SimpleGCN(L.LightningModule):\n",
    "    def __init__(self, in_features, h_features, out_features, optimizer_class, optimizer_params):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_features, h_features)\n",
    "        self.out = Linear(h_features, out_features)\n",
    "        self.loss = NormalCRPS()\n",
    "        self.aggr = DeepSetAggregator(in_channels=out_features, hidden_channels=h_features, out_channels=2)\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch_id, node_idx = data.x, data.edge_index, data.edge_attr, data.batch, data.n_idx\n",
    "        node_idx = node_idx + batch_id * 122\n",
    "        h = self.conv1(x, edge_index).relu()\n",
    "        z = self.out(h) #.squeeze()\n",
    "        print(f\"z: {z.shape}\")\n",
    "        print(f\"Output from model - Min: {z.min()}, Max: {z.max()}, Mean: {z.mean()}\") #wieso mehrere Werte\n",
    "        z = self.aggr(z, node_idx) # => aggregate to mean\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        #print(f\"y_hat: {y_hat}\")\n",
    "        #print(f\"y_hat size: {y_hat.shape}\")\n",
    "        #print(f\"Batch y: {batch.y}\")\n",
    "        #print(f\"batch y size: {batch.y.shape}\")\n",
    "        loss = self.loss.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        #print(f\"Loss: {loss}\")\n",
    "        #print(f\"loss size: {loss.shape}\")\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss.crps(mu_sigma=y_hat, y=batch.y)\n",
    "\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def initialize(self, dataloader):\n",
    "        batch = next(iter(dataloader))\n",
    "        print(f\"batch: {batch}\")\n",
    "        self.validation_step(batch, 0)\n",
    "\n",
    "    def predict(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        return y_hat\n",
    "\n"
   ],
   "id": "dde5901d7cba31b5",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T15:49:07.844498Z",
     "start_time": "2025-04-04T15:49:07.809436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "in_channels = graphs_train_rf[0].x.shape[1]\n",
    "\n",
    "# model = SimpleGCN(in_features=in_channels, h_features=100, out_features=1, optimizer_class=AdamW, optimizer_params={\"lr\": 0.001})\n",
    "model = SimpleGCN(in_features=in_channels, h_features=100, out_features=2, optimizer_class=torch.optim.SGD, optimizer_params={\"lr\": 0.001})\n",
    "\n",
    "#print(model)\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "batch2 = next(train_iter)\n",
    "print(batch.y.shape)\n",
    "#print(len(batch))\n",
    "model.forward(batch)\n",
    "# print(model.forward(batch2))\n",
    "print(batch)\n",
    "print(batch2)\n"
   ],
   "id": "18f6595ca02ed0ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([976])\n",
      "z: torch.Size([976, 2])\n",
      "Output from model - Min: -0.6658477783203125, Max: 2.8907713890075684, Mean: 1.0742884874343872\n",
      "Input: torch.Size([976, 100])\n",
      "Hidden1: torch.Size([976, 100])\n",
      "scatter: torch.Size([976, 100])\n",
      "index: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
      "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
      "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
      "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
      "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
      "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
      "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
      "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
      "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
      "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
      "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
      "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
      "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
      "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
      "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
      "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
      "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
      "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
      "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
      "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
      "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
      "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
      "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
      "        476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
      "        490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503,\n",
      "        504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517,\n",
      "        518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531,\n",
      "        532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
      "        546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559,\n",
      "        560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573,\n",
      "        574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "        588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601,\n",
      "        602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615,\n",
      "        616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
      "        630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643,\n",
      "        644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657,\n",
      "        658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671,\n",
      "        672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
      "        686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699,\n",
      "        700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713,\n",
      "        714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727,\n",
      "        728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741,\n",
      "        742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755,\n",
      "        756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769,\n",
      "        770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783,\n",
      "        784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797,\n",
      "        798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
      "        812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825,\n",
      "        826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839,\n",
      "        840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853,\n",
      "        854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867,\n",
      "        868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881,\n",
      "        882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895,\n",
      "        896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909,\n",
      "        910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923,\n",
      "        924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937,\n",
      "        938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951,\n",
      "        952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965,\n",
      "        966, 967, 968, 969, 970, 971, 972, 973, 974, 975])\n",
      "Hidden2: torch.Size([976, 100])\n",
      "DataBatch(x=[976, 65], edge_index=[2, 11360], edge_attr=[11360, 1], y=[976], timestamp=[8], n_idx=[976], batch=[976], ptr=[9])\n",
      "DataBatch(x=[976, 65], edge_index=[2, 11360], edge_attr=[11360, 1], y=[976], timestamp=[8], n_idx=[976], batch=[976], ptr=[9])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T15:48:09.289044Z",
     "start_time": "2025-04-04T15:48:05.663126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with wandb.init(\n",
    "        project=\"gnn_explore\",\n",
    "        id=f\"SimpleConv\",\n",
    "        #config=config,\n",
    "        tags=[\"exploration\"],\n",
    "):\n",
    "    in_channels = graphs_train_rf[0].x.shape[1] #+ emb_dim - 1\n",
    "\n",
    "    model = SimpleGCN(in_features=in_channels, h_features=100, out_features=1, optimizer_class=torch.optim.SGD, optimizer_params={\"lr\": 0.00001}) #0.001\n",
    "    batch = next(iter(train_loader))\n",
    "    wandb_logger = WandbLogger(project=\"gnn_explore\")\n",
    "\n",
    "\n",
    "    trainer = L.Trainer(max_epochs=30,\n",
    "        log_every_n_steps=10,\n",
    "        accelerator=\"gpu\",\n",
    "        enable_progress_bar=True,\n",
    "        enable_model_summary=True,\n",
    "        logger=wandb_logger,)\n",
    "\n",
    "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
    "\n",
    "    final_loss = trainer.logged_metrics[\"train_loss\"]\n",
    "    print(\"Final CRPS Loss:\", final_loss)"
   ],
   "id": "89f83a2dde72e0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/ltchen/gnnpp/wandb/run-20250404_174805-SimpleConv</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/leachen_thesis/gnn_explore/runs/SimpleConv' target=\"_blank\">SimpleConv</a></strong> to <a href='https://wandb.ai/leachen_thesis/gnn_explore' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/leachen_thesis/gnn_explore' target=\"_blank\">https://wandb.ai/leachen_thesis/gnn_explore</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/leachen_thesis/gnn_explore/runs/SimpleConv' target=\"_blank\">https://wandb.ai/leachen_thesis/gnn_explore/runs/SimpleConv</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory ./gnn_explore/SimpleConv/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | conv1 | GCNConv    | 6.6 K  | train\n",
      "1 | out   | Linear     | 101    | train\n",
      "2 | loss  | NormalCRPS | 0      | train\n",
      "---------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/431 [00:00<?, ?it/s] z: torch.Size([976, 1])\n",
      "Output from model - Min: -0.7958552241325378, Max: 1.5779939889907837, Mean: 0.05167920142412186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1055092/2216048651.py\", line 21, in <module>\n",
      "    trainer.fit(model=model, train_dataloaders=train_loader)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 140, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 250, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 190, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 268, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 167, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/core/module.py\", line 1306, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py\", line 153, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 238, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 122, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 484, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 89, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/optim/sgd.py\", line 112, in step\n",
      "    loss = closure()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py\", line 108, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 144, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 129, in closure\n",
      "    step_output = self._step_fn()\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py\", line 317, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 319, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py\", line 390, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_1055092/367680379.py\", line 55, in training_step\n",
      "    loss = self.loss.crps(mu_sigma=y_hat, y=batch.y)\n",
      "  File \"/tmp/pycharm_project_408/models/loss.py\", line 82, in crps\n",
      "    mu, sigma = torch.split(mu_sigma, 1, dim=1)\n",
      "ValueError: not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SimpleConv</strong> at: <a href='https://wandb.ai/leachen_thesis/gnn_explore/runs/SimpleConv' target=\"_blank\">https://wandb.ai/leachen_thesis/gnn_explore/runs/SimpleConv</a><br> View project at: <a href='https://wandb.ai/leachen_thesis/gnn_explore' target=\"_blank\">https://wandb.ai/leachen_thesis/gnn_explore</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250404_174805-SimpleConv/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/431 [1:10:32<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/431 [1:02:38<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/431 [1:01:43<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2304: UserWarning: Run (SimpleConv) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 21\u001B[0m\n\u001B[1;32m     11\u001B[0m wandb_logger \u001B[38;5;241m=\u001B[39m WandbLogger(project\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgnn_explore\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m trainer \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m30\u001B[39m,\n\u001B[1;32m     15\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     16\u001B[0m     accelerator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgpu\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     17\u001B[0m     enable_progress_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     18\u001B[0m     enable_model_summary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     19\u001B[0m     logger\u001B[38;5;241m=\u001B[39mwandb_logger,)\n\u001B[0;32m---> 21\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m final_loss \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mlogged_metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinal CRPS Loss:\u001B[39m\u001B[38;5;124m\"\u001B[39m, final_loss)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    536\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 538\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     50\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    568\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    570\u001B[0m     ckpt_path,\n\u001B[1;32m    571\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    572\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m )\n\u001B[0;32m--> 574\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    976\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    978\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 981\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    986\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_sanity_check()\n\u001B[1;32m   1024\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1025\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1026\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnexpected state \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 205\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    361\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_epoch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 363\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 140\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_restarting \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_training_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mlightning_module\u001B[38;5;241m.\u001B[39mautomatic_optimization:\n\u001B[1;32m    249\u001B[0m         \u001B[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001B[39;00m\n\u001B[0;32m--> 250\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautomatic_optimization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    252\u001B[0m         batch_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_optimization\u001B[38;5;241m.\u001B[39mrun(kwargs)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:190\u001B[0m, in \u001B[0;36m_AutomaticOptimization.run\u001B[0;34m(self, optimizer, batch_idx, kwargs)\u001B[0m\n\u001B[1;32m    183\u001B[0m         closure()\n\u001B[1;32m    185\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;66;03m# BACKWARD PASS\u001B[39;00m\n\u001B[1;32m    187\u001B[0m \u001B[38;5;66;03m# ------------------------------\u001B[39;00m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;66;03m# gradient update with accumulated gradients\u001B[39;00m\n\u001B[1;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 190\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m result \u001B[38;5;241m=\u001B[39m closure\u001B[38;5;241m.\u001B[39mconsume_result()\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mloss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:268\u001B[0m, in \u001B[0;36m_AutomaticOptimization._optimizer_step\u001B[0;34m(self, batch_idx, train_step_and_backward_closure)\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_ready()\n\u001B[1;32m    267\u001B[0m \u001B[38;5;66;03m# model hook\u001B[39;00m\n\u001B[0;32m--> 268\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_lightning_module_hook\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moptimizer_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_step_and_backward_closure\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    275\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m should_accumulate:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim_progress\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep\u001B[38;5;241m.\u001B[39mincrement_completed()\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:167\u001B[0m, in \u001B[0;36m_call_lightning_module_hook\u001B[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m hook_name\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LightningModule]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpl_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 167\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    170\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1306\u001B[0m, in \u001B[0;36mLightningModule.optimizer_step\u001B[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001B[0m\n\u001B[1;32m   1275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21moptimizer_step\u001B[39m(\n\u001B[1;32m   1276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1277\u001B[0m     epoch: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     optimizer_closure: Optional[Callable[[], Any]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1281\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1282\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001B[39;00m\n\u001B[1;32m   1283\u001B[0m \u001B[38;5;124;03m    the optimizer.\u001B[39;00m\n\u001B[1;32m   1284\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1304\u001B[0m \n\u001B[1;32m   1305\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1306\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizer_closure\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:153\u001B[0m, in \u001B[0;36mLightningOptimizer.step\u001B[0;34m(self, closure, **kwargs)\u001B[0m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MisconfigurationException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_strategy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_strategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_on_after_step()\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m step_output\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:238\u001B[0m, in \u001B[0;36mStrategy.optimizer_step\u001B[0;34m(self, optimizer, closure, model, **kwargs)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001B[39;00m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(model, pl\u001B[38;5;241m.\u001B[39mLightningModule)\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprecision_plugin\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimizer_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:122\u001B[0m, in \u001B[0;36mPrecision.optimizer_step\u001B[0;34m(self, optimizer, model, closure, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001B[39;00m\n\u001B[1;32m    121\u001B[0m closure \u001B[38;5;241m=\u001B[39m partial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wrap_closure, model, optimizer, closure)\n\u001B[0;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclosure\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/optim/optimizer.py:484\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    480\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    481\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    482\u001B[0m             )\n\u001B[0;32m--> 484\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    487\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/optim/optimizer.py:89\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     87\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m     88\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 89\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     91\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/optim/sgd.py:112\u001B[0m, in \u001B[0;36mSGD.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m closure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39menable_grad():\n\u001B[0;32m--> 112\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m group \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups:\n\u001B[1;32m    115\u001B[0m     params: List[Tensor] \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py:108\u001B[0m, in \u001B[0;36mPrecision._wrap_closure\u001B[0;34m(self, model, optimizer, closure)\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_wrap_closure\u001B[39m(\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     97\u001B[0m     model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpl.LightningModule\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     98\u001B[0m     optimizer: Steppable,\n\u001B[1;32m     99\u001B[0m     closure: Callable[[], Any],\n\u001B[1;32m    100\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    101\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    hook is called.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    106\u001B[0m \n\u001B[1;32m    107\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 108\u001B[0m     closure_result \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_after_closure(model, optimizer)\n\u001B[1;32m    110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m closure_result\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:144\u001B[0m, in \u001B[0;36mClosure.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Optional[Tensor]:\n\u001B[0;32m--> 144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mloss\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:129\u001B[0m, in \u001B[0;36mClosure.closure\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39menable_grad()\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mclosure\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ClosureResult:\n\u001B[0;32m--> 129\u001B[0m     step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_step_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m step_output\u001B[38;5;241m.\u001B[39mclosure_loss \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarning_cache\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py:317\u001B[0m, in \u001B[0;36m_AutomaticOptimization._training_step\u001B[0;34m(self, kwargs)\u001B[0m\n\u001B[1;32m    306\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001B[39;00m\n\u001B[1;32m    307\u001B[0m \n\u001B[1;32m    308\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    313\u001B[0m \n\u001B[1;32m    314\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    315\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\n\u001B[0;32m--> 317\u001B[0m training_step_output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtraining_step\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mpost_training_step()  \u001B[38;5;66;03m# unused hook - call anyway for backward compatibility\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training_step_output \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mworld_size \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:319\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 319\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    322\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001B[0m, in \u001B[0;36mStrategy.training_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 390\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[40], line 55\u001B[0m, in \u001B[0;36mSimpleGCN.training_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m     50\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(batch)\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m#print(f\"y_hat: {y_hat}\")\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m#print(f\"y_hat size: {y_hat.shape}\")\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m#print(f\"Batch y: {batch.y}\")\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m#print(f\"batch y size: {batch.y.shape}\")\u001B[39;00m\n\u001B[0;32m---> 55\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmu_sigma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_hat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m#print(f\"Loss: {loss}\")\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m#print(f\"loss size: {loss.shape}\")\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m'\u001B[39m, loss, on_epoch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/tmp/pycharm_project_408/models/loss.py:82\u001B[0m, in \u001B[0;36mNormalCRPS.crps\u001B[0;34m(self, mu_sigma, y)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Calculates the Continuous Ranked Probability Score (CRPS) assuming normally distributed df\u001B[39;00m\n\u001B[1;32m     74\u001B[0m \n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03m:param torch.Tensor mu_sigma: tensor of mean and standard deviation\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;03m:rtype torch.Tensor\u001B[39;00m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     81\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m~\u001B[39mtorch\u001B[38;5;241m.\u001B[39misnan(y)\n\u001B[0;32m---> 82\u001B[0m mu, sigma \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msplit(mu_sigma, \u001B[38;5;241m1\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     83\u001B[0m y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     85\u001B[0m mu \u001B[38;5;241m=\u001B[39m mu[mask]\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T02:08:52.747803Z",
     "start_time": "2025-03-31T02:06:52.729720Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ef5ede19ff944a9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type    | Params | Mode \n",
      "------------------------------------------\n",
      "0 | conv1 | GCNConv | 3.7 K  | train\n",
      "1 | out   | Linear  | 101    | train\n",
      "2 | loss  | MSELoss | 0      | train\n",
      "------------------------------------------\n",
      "3.8 K     Trainable params\n",
      "0         Non-trainable params\n",
      "3.8 K     Total params\n",
      "0.015     Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 13/431 [00:00<00:22, 18.49it/s, v_num=82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([976])) that is different to the input size (torch.Size([976, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 431/431 [00:03<00:00, 109.48it/s, v_num=82]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 431/431 [00:03<00:00, 109.31it/s, v_num=82]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "preds_list = []\n",
    "\n",
    "\n",
    "targets = test_f[1] # R2F\n",
    "preds = trainer.predict(model=model, dataloaders=test_f_loader) #R2F\n",
    "print(\"test_rf[1]:\")\n",
    "print(targets)\n",
    "\n",
    "preds = torch.cat(preds, dim=0)\n",
    "# Reverse transform of the y_scaler (only on the mean)\n",
    "# preds[:, 0] = torch.Tensor(y_scaler.inverse_transform(preds[:, 0].view(-1, 1))).flatten()\n",
    "preds_list.append(preds)\n",
    "\n",
    "targets = torch.Tensor(targets.t2m.values)\n",
    "print(\"t2m values:\")\n",
    "print(targets)\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "print(\"final_preds\")\n",
    "print(final_preds)\n",
    "\n",
    "res = model.loss(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final mse: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")"
   ],
   "id": "ae02e2c38664bc42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Simple GNN only with GATv2Conv\n",
    "- GATv2Conv\n",
    "- mean aggregation"
   ],
   "id": "41f91a047fcae8a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# GAT => was macht das aus? GATv2Conv, get_attentions?\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, h_features, out_features, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        # layers\n",
    "        self.conv1 = GATv2Conv(in_features, h_features, heads=num_heads, edge_dim=1, add_self_loop=True, fill_value=0.01)\n",
    "        self.out = Linear(h_features * num_heads, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.loss = ??\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = x.float() # why?\n",
    "        edge_attr = edge_attr.float()\n",
    "        h = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        z = self.out(h).squeeze()\n",
    "        return z\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "        attention_list = []\n",
    "\n",
    "        edge_index_attention, attention_weights = None, None\n",
    "        x, (edge_index_attention, attention_weights) = self.conv1(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "        attention_list.append(attention_weights)\n",
    "        x = self.relu(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x)\n",
    "        return x, edge_index_attention, attention_weights, attention_list\n"
   ],
   "id": "5f152ac0917e16f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LGAT(L.LightningModule):\n",
    "    def __init__(self, in_features, h_features, out_features, num_heads, optimizer_class, optimizer_params):\n",
    "        super(LGAT, self).__init__()\n",
    "        self.conv = GAT(in_features, h_features, out_features, num_heads)\n",
    "        self.postprocess = MakePositive()\n",
    "        self.loss_fn = NormalCRPS()\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch_id, node_idx = data.x, data.edge_index, data.edge_attr, data.batch, data.n_idx\n",
    "        node_idx = node_idx + batch_id * 122  # add batch_id to node_idx to get unique node indices\n",
    "        x = self.encoder(x)\n",
    "        x = self.conv(x, edge_index, edge_attr)\n",
    "        x = self.aggr(x, node_idx)\n",
    "        x = self.postprocess(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\n",
    "            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=1\n",
    "        )  # The batch size is not actually 1 but the loss is already averaged over the batch\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer_class(self.parameters(), **self.optimizer_params)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        y_hat = self.forward(batch)\n",
    "        loss = self.loss_fn.crps(mu_sigma=y_hat, y=batch.y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=1)\n",
    "        return loss\n",
    "\n",
    "    def initialize(self, dataloader):\n",
    "        batch = next(iter(dataloader))\n",
    "        self.validation_step(batch, 0)"
   ],
   "id": "78f3c167e4de5547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7be4b16c76533b03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Moritz ResGNN as reference\n",
    "class ResGnn(torch.nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, num_layers: int, hidden_channels: int, heads: int):\n",
    "        super(ResGnn, self).__init__()\n",
    "        assert num_layers > 0, \"num_layers must be > 0.\"\n",
    "\n",
    "        # Create Layers\n",
    "        self.convolutions = ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convolutions.append(\n",
    "                GATv2Conv(-1, hidden_channels, heads=heads, edge_dim=1, add_self_loops=True, fill_value=0.01)\n",
    "            )\n",
    "        self.lin = Linear(hidden_channels * heads, out_channels)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "        for i, conv in enumerate(self.convolutions):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x = conv(x, edge_index, edge_attr)\n",
    "                x = self.relu(x)\n",
    "            else:\n",
    "                x = x + self.relu(conv(x, edge_index, edge_attr))  # Residual Layers\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_attention(\n",
    "        self, x: torch.Tensor, edge_index: torch.Tensor, edge_attr: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Runs a forward Pass for the given graph only though the ResGNN layer.\n",
    "        NOTE: the data that is given to this method must first pass through the layers before this layer in the Graph\n",
    "\n",
    "        :param torch.Tensor x: Tensor of Node Features (NxD)\n",
    "        :param torch.Tensor edge_index: Tensor of Edges (2xE)\n",
    "        :param torch.Tensor edge_attr: Edge Attributes (ExNum_Attr)\n",
    "        :return x, edge_index_attention, attention_weights: Tensor of Node Features (NxD), Tensor of Edges with\n",
    "        self loops (2xE), Tensor of Attention per edge (ExNum_Heads)\n",
    "        \"\"\"\n",
    "        x = x.float()\n",
    "        edge_attr = edge_attr.float()\n",
    "\n",
    "        # Pass Data though Layer to get the Attention\n",
    "        attention_list = []\n",
    "        # Note: edge_index_attention has to be added since we have self loops now\n",
    "        edge_index_attention, attention_weights = None, None\n",
    "\n",
    "        for i, conv in enumerate(\n",
    "            self.convolutions,\n",
    "        ):\n",
    "            if i == 0:\n",
    "                # First Layer\n",
    "                x, (edge_index_attention, attention_weights) = conv(\n",
    "                    x, edge_index, edge_attr, return_attention_weights=True\n",
    "                )\n",
    "                attention_list.append(attention_weights)\n",
    "                x = self.relu(x)\n",
    "                x = self.norm(x)\n",
    "            else:\n",
    "                x_conv, (edge_index_attention, attention_weights) = conv(\n",
    "                    x, edge_index, edge_attr, return_attention_weights=True\n",
    "                )\n",
    "                attention_list.append(attention_weights)\n",
    "                x = x + self.relu(x_conv)  # Residual Layers\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Attention weights of first layer\n",
    "        attention_weights = attention_weights.mean(dim=1)\n",
    "\n",
    "        return x, edge_index_attention, attention_weights, attention_list"
   ],
   "id": "efa6e3053d299011"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GNN with DeepSetAggregator (without GAT)\n",
   "id": "67874560870e56d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27b583188476f3ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## GNN with GAT and DeepSetAggregator",
   "id": "32a27dda06effbd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##",
   "id": "58711b8715212415"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
