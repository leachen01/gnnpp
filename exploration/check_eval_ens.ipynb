{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-01T02:06:07.791225Z",
     "start_time": "2025-05-01T02:06:07.779836Z"
    }
   },
   "source": [
    "%cd /home/ltchen/gnnpp\n",
    "import argparse\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# import lightning as L\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch_geometric\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from models.graphensemble.multigraph import Multigraph\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from utils.data import (\n",
    "    load_dataframes_old,\n",
    "    load_distances,\n",
    "    normalize_features_and_create_graphs,\n",
    "    split_graph,\n",
    "    rm_edges,\n",
    "    summary_statistics,\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ltchen/gnnpp\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T02:06:45.567806Z",
     "start_time": "2025-05-01T02:06:18.112726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# args = argparse.ArgumentParser()\n",
    "# args.add_argument(\"data\", type=str, default=\"rf\", help='Data to use for testing, can be \"rf\" or \"f\"')\n",
    "# args.add_argument(\n",
    "#     \"leadtime\", type=str, default=\"24h\", help='Leadtime to use for testing, can be \"24h\", \"72h\" or \"120h\"'\n",
    "# )\n",
    "# args.add_argument(\"folder\", type=str, default=\"trained_models/best_24h\", help=\"Folder to load the models from\")\n",
    "#\n",
    "# args = args.parse_args()\n",
    "args = {\n",
    "    \"data\": \"rf\",\n",
    "\"leadtime\": \"24h\",\n",
    "\"folder\": \"trained_models/no_ensemble_24h\"}\n",
    "print(\"#################################################\")\n",
    "print(f\"[INFO] Starting evaluation with data: {args['data']} and leadtime: {args['leadtime']}\")\n",
    "print(\"#################################################\")\n",
    "\n",
    "CHECKPOINT_FOLDER = args['folder']\n",
    "JSONPATH = os.path.join(CHECKPOINT_FOLDER, \"params.json\")\n",
    "\n",
    "# Load the JSON file\n",
    "with open(JSONPATH, \"r\") as f:\n",
    "    print(f\"[INFO] Loading {JSONPATH}\")\n",
    "    args_dict = json.load(f)\n",
    "\n",
    "@dataclass\n",
    "class DummyConfig:\n",
    "    pass\n",
    "\n",
    "for key, value in args_dict.items():\n",
    "    setattr(DummyConfig, key, value)\n",
    "\n",
    "config = DummyConfig()\n",
    "print(\"[INFO] Starting eval with config: \", args_dict)\n",
    "\n",
    "# Load Data ######################################################################\n",
    "dataframes = load_dataframes_old(mode=\"eval\", leadtime=args['leadtime'])\n",
    "# Only Summary ###################################################################\n",
    "only_summary = False\n",
    "if hasattr(config, \"only_summary\"):\n",
    "    if config.only_summary is True or config.only_summary == \"True\":\n",
    "        print(\"[INFO] Only using summary statistics...\")\n",
    "        dataframes = summary_statistics(dataframes)\n",
    "        only_summary = True\n",
    "\n",
    "dist = load_distances(dataframes[\"stations\"])\n",
    "graphs_train_rf, tests = normalize_features_and_create_graphs(\n",
    "    training_data=dataframes[\"train\"],\n",
    "    valid_test_data=[dataframes[\"test_rf\"], dataframes[\"test_f\"]],\n",
    "    mat=dist,\n",
    "    max_dist=config.max_dist,\n",
    ")\n",
    "graphs_test_rf, graphs_test_f = tests\n",
    "\n",
    "graphs_test = graphs_test_rf if args['data'] == \"rf\" else graphs_test_f\n",
    "\n",
    "if args['data'] == \"f\" and not only_summary:\n",
    "    print(\"[INFO] Splitting graphs for f data...\")\n",
    "    graphs_split = [split_graph(g) for g in graphs_test]\n",
    "    graphs_test = [g for sublist in graphs_split for g in sublist]\n",
    "\n",
    "# Remove Edges ##################################################################\n",
    "if hasattr(config, \"remove_edges\"):\n",
    "    if config.remove_edges == \"True\" or config.remove_edges is True:\n",
    "        print(\"[INFO] Removing edges...\")\n",
    "        rm_edges(graphs_train_rf)\n",
    "        rm_edges(graphs_test)\n",
    "\n",
    "# Create Data Loaders ###########################################################\n",
    "print(\"[INFO] Creating data loaders...\")\n",
    "train_loader = DataLoader(graphs_train_rf, batch_size=config.batch_size, shuffle=True)\n",
    "# test_loader_rf = DataLoader(graphs_test_rf, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(graphs_test, batch_size=1 if args['data'] == \"rf\" else 5, shuffle=False)\n",
    "\n",
    "# Create Model ##################################################################\n",
    "print(\"[INFO] Creating ensemble...\")\n",
    "\n",
    "emb_dim = 20\n",
    "in_channels = 55  # graphs_train_rf[0].x.shape[1] + emb_dim - 1\n",
    "\n",
    "FOLDER = os.path.join(CHECKPOINT_FOLDER, \"models\")\n",
    "preds_list = []\n",
    "for path in os.listdir(FOLDER):\n",
    "    if path.endswith(\".ckpt\"):\n",
    "        print(f\"[INFO] Loading model from {path}\")\n",
    "        # Load Model from chekcpoint\n",
    "        checkpoint = torch.load(os.path.join(FOLDER, path))\n",
    "\n",
    "        multigraph = Multigraph(\n",
    "            num_nodes=graphs_test_f[0].num_nodes,\n",
    "            edge_dim=1,\n",
    "            embedding_dim=emb_dim,\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels_gnn=config.gnn_hidden,\n",
    "            out_channels_gnn=config.gnn_hidden,\n",
    "            num_layers_gnn=config.gnn_layers,\n",
    "            heads=config.heads,\n",
    "            hidden_channels_deepset=config.gnn_hidden,\n",
    "            optimizer_class=AdamW,\n",
    "            optimizer_params=dict(lr=config.lr),\n",
    "        )\n",
    "        # torch_geometric.compile(multigraph)\n",
    "\n",
    "        # run a dummy forward pass to initialize the model\n",
    "        batch = next(iter(train_loader))\n",
    "        batch = batch  # .to(\"cuda\")\n",
    "        #multigraph  # .to(\"cuda\")\n",
    "        multigraph.forward(batch)\n",
    "        print(type(multigraph))\n",
    "        print(isinstance(multigraph, LightningModule))\n",
    "\n",
    "        multigraph.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "        trainer = L.Trainer(log_every_n_steps=1, accelerator=\"gpu\", devices=[1], enable_progress_bar=True)\n",
    "\n",
    "        preds = trainer.predict(model=multigraph, dataloaders=[test_loader])\n",
    "\n",
    "        if args['data'] == \"f\" and not only_summary:\n",
    "            preds = [\n",
    "                prediction.reshape(5, 122, 2).mean(axis=0) for prediction in preds\n",
    "            ]  # Average over the batch dimension\n",
    "\n",
    "        preds = torch.cat(preds, dim=0)\n",
    "        preds_list.append(preds)\n",
    "\n"
   ],
   "id": "5322e1b3b030ca43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################\n",
      "[INFO] Starting evaluation with data: rf and leadtime: 24h\n",
      "#################################################\n",
      "[INFO] Loading trained_models/no_ensemble_24h/params.json\n",
      "[INFO] Starting eval with config:  {'batch_size': 8, 'gnn_hidden': 256, 'gnn_layers': 1, 'heads': 8, 'lr': 0.0001, 'max_dist': 50, 'max_epochs': 23, 'remove_edges': 'False', 'only_summary': 'True'}\n",
      "[INFO] Dataframes exist. Will load pandas dataframes.\n",
      "[INFO] Only using summary statistics...\n",
      "[INFO] Calculating summary statistics for train\n",
      "[INFO] Calculating summary statistics for test_rf\n",
      "[INFO] Calculating summary statistics for test_f\n",
      "[INFO] Loading distances from file...\n",
      "[INFO] Normalizing features...\n",
      "[INFO] Creating graph data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_482610/1397428512.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(os.path.join(FOLDER, path))\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/ltchen/.conda/envs/gnn_env4/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Creating data loaders...\n",
      "[INFO] Creating ensemble...\n",
      "[INFO] Loading model from run_0.ckpt\n",
      "<class 'models.graphensemble.multigraph.Multigraph'>\n",
      "True\n",
      "Predicting DataLoader 0: 100%|██████████| 836/836 [00:04<00:00, 194.69it/s]\n",
      "[INFO] Loading model from run_1.ckpt\n",
      "<class 'models.graphensemble.multigraph.Multigraph'>\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 836/836 [00:04<00:00, 207.19it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ! Hacky wack of getting the targets\n",
    "targets = dataframes[\"test_rf\"][1] if args['data'] == \"rf\" else dataframes[\"test_f\"][1]\n",
    "targets = torch.tensor(targets.t2m.values) - 273.15\n",
    "\n",
    "stacked = torch.stack(preds_list)\n",
    "final_preds = torch.mean(stacked, dim=0)\n",
    "\n",
    "res = multigraph.loss_fn.crps(final_preds, targets)\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "print(f\"final crps: {res.item()}\")\n",
    "print(\"#############################################\")\n",
    "print(\"#############################################\")\n",
    "\n",
    "# Save Results ##################################################################\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(np.concatenate([targets.view(-1, 1), final_preds], axis=1), columns=[\"t2m\", \"mu\", \"sigma\"])\n",
    "df.to_csv(os.path.join(CHECKPOINT_FOLDER, f\"{args['data']}_results.csv\"), index=False)\n",
    "\n",
    "# Create Log File ###############################################################\n",
    "log_file = os.path.join(CHECKPOINT_FOLDER, f\"{args['data']}.txt\")\n",
    "with open(log_file, \"w\") as f:\n",
    "    f.write(f\"Data: {args['data']}\\n\")\n",
    "    f.write(f\"Leadtime: {args['leadtime']}\\n\")\n",
    "    f.write(f\"Final crps: {res.item()}\")"
   ],
   "id": "df9d17444adbe7d5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
